{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T04:36:18.263377Z",
     "start_time": "2025-05-29T04:36:09.414407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "import models\n",
    "import helper_functions\n",
    "import preprocess\n",
    "import settings\n",
    "\n",
    "importlib.reload(settings)\n",
    "# importlib.reload(models)\n",
    "importlib.reload(helper_functions)\n",
    "importlib.reload(preprocess)\n",
    "\n",
    "from models.LSTM import LSTM_HumanFi, CNN_LSTM, CNN_BiLSTM_TemporalAttention, CNN_BiLSTM_ChannelAttention, CNN_BiLSTM_DualAttention, CNN_BiLSTM_Attention\n",
    "from models.RadioNet import RadioNet_NeuralWave\n",
    "from models.ResNet import ECAResNet1D, ECABasicBlock1D, ResNet1D_JARILWWF, OptResNet1D_JARILWWF, OptECAResNet1D_JARILWWF, CustomResNet1D, CustomECAResNet1D\n",
    "from models.TemporalConvNet import TemporalConvNet\n",
    "\n",
    "from helper_functions import grid_search, run_gridsearch_with_seeds\n",
    "from preprocess import DataPreprocessor\n",
    "from settings import folder_path_5ghz_10hz_collected, folder_path_60ghz_collected, folder_path_5ghz_200hz_collected, folder_path_60ghz_external, output_path"
   ],
   "id": "eaf05c930fb61034",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-29T04:36:18.275572Z",
     "start_time": "2025-05-29T04:36:18.270382Z"
    }
   },
   "source": [
    "input_dim = 60\n",
    "num_epochs = 500\n",
    "\n",
    "param_grid = {\n",
    "    'batch_size': [32],\n",
    "    'learning_rate': [0.0007], # 0.001,\n",
    "    'optimizer': ['adam'],\n",
    "    'mixup_alpha': [0.0, 0.4],\n",
    "    'smoothing_prob': [0.0, 0.5],\n",
    "    'model': [],\n",
    "}\n",
    "\n",
    "hidden_dims = [64, 128] # 32, 256\n",
    "num_layers_list = [1, 2]\n",
    "bidirectional_flags = [False, True]\n",
    "dropouts = [0.2, 0.5]\n",
    "\n",
    "for hidden_dim in hidden_dims:\n",
    "    for num_layers in num_layers_list:\n",
    "        for bidirectional in bidirectional_flags:\n",
    "            for dropout in dropouts:\n",
    "                param_grid['model'].append({\n",
    "                    'model_class': LSTM_HumanFi,\n",
    "                    'model_args': {\n",
    "                        'input_dim': input_dim,\n",
    "                        'hidden_dim': hidden_dim,\n",
    "                        'num_layers': num_layers,\n",
    "                        'bidirectional': bidirectional,\n",
    "                        'dropout': dropout\n",
    "                    },\n",
    "                    'num_epochs': num_epochs\n",
    "                })\n",
    "\n",
    "tcn_channel_options = [\n",
    "    # [32, 64],\n",
    "    [64, 128],\n",
    "    [64, 128, 128],\n",
    "    [64, 128, 256],\n",
    "    # [128, 256, 512],\n",
    "]\n",
    "tcn_kernel_sizes = [2, 3] # 5\n",
    "tcn_dropout_rates = [0.2, 0.5]\n",
    "\n",
    "for channels in tcn_channel_options:\n",
    "    for kernel_size in tcn_kernel_sizes:\n",
    "        for dropout in tcn_dropout_rates:\n",
    "            param_grid['model'].append({\n",
    "                'model_class': TemporalConvNet,\n",
    "                'model_args': {\n",
    "                    'num_inputs': input_dim,\n",
    "                    'num_channels': channels,\n",
    "                    'kernel_size': kernel_size,\n",
    "                    'dropout': dropout\n",
    "                },\n",
    "                'num_epochs': num_epochs\n",
    "            })\n",
    "\n",
    "cnn_lstm_hidden_dims = [64, 128] # 256\n",
    "cnn_lstm_layers = [1, 2]\n",
    "\n",
    "for hidden_dim in cnn_lstm_hidden_dims:\n",
    "    for num_layers in cnn_lstm_layers:\n",
    "        param_grid['model'].append({\n",
    "            'model_class': CNN_BiLSTM_Attention,\n",
    "            'model_args': {\n",
    "                'input_dim': input_dim,\n",
    "                'cnn_filters': 64,\n",
    "                'lstm_units': hidden_dim,\n",
    "                'num_layers': num_layers,\n",
    "            },\n",
    "            'num_epochs': num_epochs\n",
    "        })\n",
    "\n",
    "        param_grid['model'].append({\n",
    "            'model_class': CNN_BiLSTM_TemporalAttention,\n",
    "            'model_args': {\n",
    "                'input_dim': input_dim,\n",
    "                'cnn_channels': 64,\n",
    "                'lstm_hidden_dim': hidden_dim,\n",
    "                'lstm_layers': num_layers,\n",
    "            },\n",
    "            'num_epochs': num_epochs\n",
    "        })\n",
    "\n",
    "        # param_grid['model'].append({\n",
    "        #     'model_class': CNN_BiLSTM_ChannelAttention,\n",
    "        #     'model_args': {\n",
    "        #         'input_dim': input_dim,\n",
    "        #         'cnn_channels': 64,\n",
    "        #         'lstm_hidden_dim': hidden_dim,\n",
    "        #         'lstm_layers': num_layers,\n",
    "        #     },\n",
    "        #     'num_epochs': num_epochs\n",
    "        # })\n",
    "        #\n",
    "        # param_grid['model'].append({\n",
    "        #     'model_class': CNN_BiLSTM_DualAttention,\n",
    "        #     'model_args': {\n",
    "        #         'input_dim': input_dim,\n",
    "        #         'cnn_channels': 64,\n",
    "        #         'lstm_hidden_dim': hidden_dim,\n",
    "        #         'lstm_layers': num_layers,\n",
    "        #     },\n",
    "        #     'num_epochs': num_epochs\n",
    "        # })\n",
    "\n",
    "\n",
    "param_grid['model'] += [\n",
    "    # {\n",
    "    #     'model_class': RadioNet_NeuralWave,\n",
    "    #     'model_args': {'input_dim': input_dim},\n",
    "    #     'num_epochs': num_epochs,\n",
    "    #     'data_preprocessor': DataPreprocessor(target_dim=354)\n",
    "    # },\n",
    "    \n",
    "    # the ones commented below are not optimal (proven by early tests)\n",
    "    # {\n",
    "    #     'model_class': ECAResNet1D,\n",
    "    #     'model_args': {'input_channels': input_dim, 'block': ECABasicBlock1D, 'layers': (1, 1, 1, 1)},\n",
    "    #     'num_epochs': num_epochs\n",
    "    # },\n",
    "    # {\n",
    "    #     'model_class': ECAResNet1D,\n",
    "    #     'model_args': {'input_channels': input_dim, 'block': ECABasicBlock1D, 'layers': (2, 2, 2, 2)},\n",
    "    #     'num_epochs': num_epochs\n",
    "    # },\n",
    "    # {\n",
    "    #     'model_class': ResNet1D_JARILWWF,\n",
    "    #     'model_args': {'input_channels': input_dim, 'layers': [1,1,1,1]},\n",
    "    #     'num_epochs': num_epochs,\n",
    "    # },\n",
    "    # {\n",
    "    #     'model_class': ResNet1D_JARILWWF,\n",
    "    #     'model_args': {'input_channels': input_dim, 'layers': [2,2,2,2]},\n",
    "    #     'num_epochs': num_epochs,\n",
    "    # },\n",
    "    \n",
    "    \n",
    "    {\n",
    "        'model_class': OptResNet1D_JARILWWF,\n",
    "        'model_args': {'input_channels': input_dim, 'layers': [1,1,1,1]},\n",
    "        'num_epochs': num_epochs,\n",
    "    },\n",
    "    # {\n",
    "    #     'model_class': OptResNet1D_JARILWWF,\n",
    "    #     'model_args': {'input_channels': input_dim, 'layers': [2,2,2,2]},\n",
    "    #     'num_epochs': num_epochs,\n",
    "    # },\n",
    "    {\n",
    "        'model_class': OptECAResNet1D_JARILWWF,\n",
    "        'model_args': {'input_channels': input_dim, 'layers': [1,1,1,1]},\n",
    "        'num_epochs': num_epochs,\n",
    "    },\n",
    "    # {\n",
    "    #     'model_class': OptECAResNet1D_JARILWWF,\n",
    "    #     'model_args': {'input_channels': input_dim, 'layers': [2,2,2,2]},\n",
    "    #     'num_epochs': num_epochs,\n",
    "    # },\n",
    "    {\n",
    "        'model_class': CustomResNet1D,\n",
    "        'model_args': {'input_channels': input_dim},\n",
    "        'num_epochs': num_epochs,\n",
    "    },\n",
    "    {\n",
    "        'model_class': CustomECAResNet1D,\n",
    "        'model_args': {'input_channels': input_dim},\n",
    "        'num_epochs': num_epochs,\n",
    "    },\n",
    "] "
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T04:15:20.864371Z",
     "start_time": "2025-05-28T03:27:41.617289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gridsearch_results_df = run_gridsearch_with_seeds(\n",
    "    gridsearch_func=grid_search,\n",
    "    gridsearch_args={\n",
    "        'param_grid': param_grid,\n",
    "        'folder_path': folder_path_60ghz_collected,\n",
    "        'background_subtraction': False,\n",
    "        'seconds_per_sample': 5,\n",
    "        'rows_per_second': 10,\n",
    "    },\n",
    "    n_seeds=3,\n",
    "    output_dir=output_path,\n",
    "    filename=\"gridsearch_60ghz_coll\"\n",
    ")"
   ],
   "id": "5a5de08b7ecfded3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running seed 42 ---\n",
      "Running 160 configurations...\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.091676.\n",
      "\n",
      "Testing Configuration 0: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 442 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.332615\n",
      "\n",
      "Testing Configuration 1: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 372 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.135230\n",
      "\n",
      "Testing Configuration 2: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 454 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.855821\n",
      "\n",
      "Testing Configuration 3: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:14.431741\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 4: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:14.853597\n",
      "\n",
      "Testing Configuration 5: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:15.111233\n",
      "\n",
      "Testing Configuration 6: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 300 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.420997\n",
      "\n",
      "Testing Configuration 7: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:14.300502\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 8: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 367 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.151103\n",
      "\n",
      "Testing Configuration 9: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 494 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.642996\n",
      "\n",
      "Testing Configuration 10: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:17.451509\n",
      "\n",
      "Testing Configuration 11: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 496 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.097162\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 12: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:14.964726\n",
      "\n",
      "Testing Configuration 13: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:14.731375\n",
      "\n",
      "Testing Configuration 14: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:15.915317\n",
      "\n",
      "Testing Configuration 15: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:16.238124\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012500.\n",
      "\n",
      "Testing Configuration 16: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 285 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.172282\n",
      "\n",
      "Testing Configuration 17: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 327 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.325708\n",
      "\n",
      "Testing Configuration 18: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 322 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.295010\n",
      "\n",
      "Testing Configuration 19: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:15.971467\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 20: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:13.441148\n",
      "\n",
      "Testing Configuration 21: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 319 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.919708\n",
      "\n",
      "Testing Configuration 22: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:15.207758\n",
      "\n",
      "Testing Configuration 23: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:15.960377\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010505.\n",
      "\n",
      "Testing Configuration 24: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 211 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.053317\n",
      "\n",
      "Testing Configuration 25: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 407 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.022978\n",
      "\n",
      "Testing Configuration 26: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 489 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.445488\n",
      "\n",
      "Testing Configuration 27: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 475 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.128491\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.016000.\n",
      "\n",
      "Testing Configuration 28: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 224 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.179000\n",
      "\n",
      "Testing Configuration 29: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 414 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.974262\n",
      "\n",
      "Testing Configuration 30: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 430 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.738888\n",
      "\n",
      "Testing Configuration 31: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 393 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.588866\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.098472.\n",
      "\n",
      "Testing Configuration 32: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 361 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.939820\n",
      "\n",
      "Testing Configuration 33: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 455 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.831630\n",
      "\n",
      "Testing Configuration 34: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:16.406521\n",
      "\n",
      "Testing Configuration 35: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 356 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.006833\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 36: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 320 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.620998\n",
      "\n",
      "Testing Configuration 37: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:14.460175\n",
      "\n",
      "Testing Configuration 38: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 302 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.988661\n",
      "\n",
      "Testing Configuration 39: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 454 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.907279\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 40: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 326 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.393517\n",
      "\n",
      "Testing Configuration 41: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 330 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.842857\n",
      "\n",
      "Testing Configuration 42: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:15.647415\n",
      "\n",
      "Testing Configuration 43: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 499 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.032356\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.013000.\n",
      "\n",
      "Testing Configuration 44: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 375 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.497466\n",
      "\n",
      "Testing Configuration 45: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:16.165770\n",
      "\n",
      "Testing Configuration 46: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 407 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.000014\n",
      "\n",
      "Testing Configuration 47: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:16.657433\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 48: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 339 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.508680\n",
      "\n",
      "Testing Configuration 49: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 325 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.426460\n",
      "\n",
      "Testing Configuration 50: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 414 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.905785\n",
      "\n",
      "Testing Configuration 51: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 455 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.633642\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010001.\n",
      "\n",
      "Testing Configuration 52: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 198 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.413195\n",
      "\n",
      "Testing Configuration 53: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 497 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.012480\n",
      "\n",
      "Testing Configuration 54: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 400 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.107326\n",
      "\n",
      "Testing Configuration 55: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 403 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.433109\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 56: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 377 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.962368\n",
      "\n",
      "Testing Configuration 57: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 137 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.313005\n",
      "\n",
      "Testing Configuration 58: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 276 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.051492\n",
      "\n",
      "Testing Configuration 59: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 393 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.787151\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010999.\n",
      "\n",
      "Testing Configuration 60: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 254 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.361205\n",
      "\n",
      "Testing Configuration 61: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 335 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.633896\n",
      "\n",
      "Testing Configuration 62: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 425 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.086230\n",
      "\n",
      "Testing Configuration 63: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 249 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.171027\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 64: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 170 (no improvement in 50 epochs)\n",
      "Training time:  0:02:28.992482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 65: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 218 (no improvement in 50 epochs)\n",
      "Training time:  0:03:07.656256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 66: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 466 (no improvement in 50 epochs)\n",
      "Training time:  0:06:44.996948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 67: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 443 (no improvement in 50 epochs)\n",
      "Training time:  0:06:25.318228\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 68: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 206 (no improvement in 50 epochs)\n",
      "Training time:  0:02:59.606470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 69: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 238 (no improvement in 50 epochs)\n",
      "Training time:  0:03:26.146876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 70: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 283 (no improvement in 50 epochs)\n",
      "Training time:  0:04:06.092542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 71: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 326 (no improvement in 50 epochs)\n",
      "Training time:  0:04:44.461353\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 72: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 214 (no improvement in 50 epochs)\n",
      "Training time:  0:03:06.202443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 73: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 149 (no improvement in 50 epochs)\n",
      "Training time:  0:02:10.072115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 74: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 390 (no improvement in 50 epochs)\n",
      "Training time:  0:05:40.910522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 75: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 238 (no improvement in 50 epochs)\n",
      "Training time:  0:03:28.227621\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 76: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 259 (no improvement in 50 epochs)\n",
      "Training time:  0:03:46.496548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 77: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 216 (no improvement in 50 epochs)\n",
      "Training time:  0:03:08.557425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 78: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 374 (no improvement in 50 epochs)\n",
      "Training time:  0:05:21.674125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 79: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 363 (no improvement in 50 epochs)\n",
      "Training time:  0:05:04.504439\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 80: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 237 (no improvement in 50 epochs)\n",
      "Training time:  0:06:27.590280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 81: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 206 (no improvement in 50 epochs)\n",
      "Training time:  0:05:36.349607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 82: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 226 (no improvement in 50 epochs)\n",
      "Training time:  0:06:10.576286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 83: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 356 (no improvement in 50 epochs)\n",
      "Training time:  0:09:42.461706\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 84: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 170 (no improvement in 50 epochs)\n",
      "Training time:  0:04:38.827227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 85: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 303 (no improvement in 50 epochs)\n",
      "Training time:  0:08:14.644963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 86: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 455 (no improvement in 50 epochs)\n",
      "Training time:  0:12:24.935876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 87: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 227 (no improvement in 50 epochs)\n",
      "Training time:  0:06:11.228898\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010000.\n",
      "\n",
      "Testing Configuration 88: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 193 (no improvement in 50 epochs)\n",
      "Training time:  0:05:16.919643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 89: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 130 (no improvement in 50 epochs)\n",
      "Training time:  0:03:34.333649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 90: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 303 (no improvement in 50 epochs)\n",
      "Training time:  0:08:17.651186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 91: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 309 (no improvement in 50 epochs)\n",
      "Training time:  0:08:29.527530\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 92: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 291 (no improvement in 50 epochs)\n",
      "Training time:  0:07:57.762444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 93: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 331 (no improvement in 50 epochs)\n",
      "Training time:  0:09:03.510395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 94: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 397 (no improvement in 50 epochs)\n",
      "Training time:  0:10:52.351796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 95: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 297 (no improvement in 50 epochs)\n",
      "Training time:  0:08:08.243824\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 96: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 194 (no improvement in 50 epochs)\n",
      "Training time:  0:07:42.470849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 97: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 220 (no improvement in 50 epochs)\n",
      "Training time:  0:08:44.449284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 98: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 243 (no improvement in 50 epochs)\n",
      "Training time:  0:09:39.407271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 99: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 269 (no improvement in 50 epochs)\n",
      "Training time:  0:10:40.101001\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 100: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 152 (no improvement in 50 epochs)\n",
      "Training time:  0:06:02.548395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 101: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 341 (no improvement in 50 epochs)\n",
      "Training time:  0:13:34.166541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 102: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 201 (no improvement in 50 epochs)\n",
      "Training time:  0:07:58.381236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 103: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 416 (no improvement in 50 epochs)\n",
      "Training time:  0:16:31.483567\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 104: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 200 (no improvement in 50 epochs)\n",
      "Training time:  0:07:57.120575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 105: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 177 (no improvement in 50 epochs)\n",
      "Training time:  0:07:03.660604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 106: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 323 (no improvement in 50 epochs)\n",
      "Training time:  0:12:53.524776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 107: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 251 (no improvement in 50 epochs)\n",
      "Training time:  0:10:00.375579\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010499.\n",
      "\n",
      "Testing Configuration 108: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:05:18.186970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 109: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 313 (no improvement in 50 epochs)\n",
      "Training time:  0:12:28.811252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 110: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 349 (no improvement in 50 epochs)\n",
      "Training time:  0:13:54.805088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 111: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 359 (no improvement in 50 epochs)\n",
      "Training time:  0:14:19.298653\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.077500.\n",
      "\n",
      "Testing Configuration 112: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 311 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.772738\n",
      "\n",
      "Testing Configuration 113: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 237 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.366204\n",
      "\n",
      "Testing Configuration 114: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 341 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.366179\n",
      "\n",
      "Testing Configuration 115: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 286 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.786311\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 116: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 123 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.120332\n",
      "\n",
      "Testing Configuration 117: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 125 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.993293\n",
      "\n",
      "Testing Configuration 118: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.947474\n",
      "\n",
      "Testing Configuration 119: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 159 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.701756\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 120: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 238 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.116033\n",
      "\n",
      "Testing Configuration 121: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 364 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.574529\n",
      "\n",
      "Testing Configuration 122: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 354 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.787971\n",
      "\n",
      "Testing Configuration 123: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 300 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.443871\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 124: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 157 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.928678\n",
      "\n",
      "Testing Configuration 125: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 213 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.180648\n",
      "\n",
      "Testing Configuration 126: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 232 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.766957\n",
      "\n",
      "Testing Configuration 127: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 219 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.718559\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.013500.\n",
      "\n",
      "Testing Configuration 128: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 190 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.848219\n",
      "\n",
      "Testing Configuration 129: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 260 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.204136\n",
      "\n",
      "Testing Configuration 130: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 247 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.553246\n",
      "\n",
      "Testing Configuration 131: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 362 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.077939\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 132: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 213 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.357824\n",
      "\n",
      "Testing Configuration 133: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 87 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.773050\n",
      "\n",
      "Testing Configuration 134: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.794571\n",
      "\n",
      "Testing Configuration 135: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 139 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.958966\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.015001.\n",
      "\n",
      "Testing Configuration 136: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 244 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.281317\n",
      "\n",
      "Testing Configuration 137: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 171 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.050855\n",
      "\n",
      "Testing Configuration 138: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 291 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.481246\n",
      "\n",
      "Testing Configuration 139: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 297 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.717469\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010000.\n",
      "\n",
      "Testing Configuration 140: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 105 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.757052\n",
      "\n",
      "Testing Configuration 141: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 128 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.595972\n",
      "\n",
      "Testing Configuration 142: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.602576\n",
      "\n",
      "Testing Configuration 143: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.311376\n",
      "\n",
      "Processing Model: OptResNet1D_JARILWWF with params {'input_channels': 60, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 144: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 72 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.397050\n",
      "\n",
      "Testing Configuration 145: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 115 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.609134\n",
      "\n",
      "Testing Configuration 146: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.134468\n",
      "\n",
      "Testing Configuration 147: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 171 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.992186\n",
      "\n",
      "Processing Model: OptECAResNet1D_JARILWWF with params {'input_channels': 60, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010999.\n",
      "\n",
      "Testing Configuration 148: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 101 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.421811\n",
      "\n",
      "Testing Configuration 149: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.626028\n",
      "\n",
      "Testing Configuration 150: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 60 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.032000\n",
      "\n",
      "Testing Configuration 151: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 61 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.228629\n",
      "\n",
      "Processing Model: CustomResNet1D with params {'input_channels': 60}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 152: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 134 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.514063\n",
      "\n",
      "Testing Configuration 153: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 105 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.775881\n",
      "\n",
      "Testing Configuration 154: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.264463\n",
      "\n",
      "Testing Configuration 155: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.085256\n",
      "\n",
      "Processing Model: CustomECAResNet1D with params {'input_channels': 60}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011001.\n",
      "\n",
      "Testing Configuration 156: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.893603\n",
      "\n",
      "Testing Configuration 157: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.490792\n",
      "\n",
      "Testing Configuration 158: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 110 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.524954\n",
      "\n",
      "Testing Configuration 159: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 144 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.199681\n",
      "\n",
      "--- Running seed 420 ---\n",
      "Running 160 configurations...\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 0: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 441 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.206201\n",
      "\n",
      "Testing Configuration 1: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 493 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.061060\n",
      "\n",
      "Testing Configuration 2: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:15.825473\n",
      "\n",
      "Testing Configuration 3: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 486 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.205032\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 4: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:14.799211\n",
      "\n",
      "Testing Configuration 5: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:15.512851\n",
      "\n",
      "Testing Configuration 6: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:14.620237\n",
      "\n",
      "Testing Configuration 7: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 446 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.503082\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 8: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:14.366749\n",
      "\n",
      "Testing Configuration 9: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 486 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.388750\n",
      "\n",
      "Testing Configuration 10: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 255 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.701527\n",
      "\n",
      "Testing Configuration 11: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 443 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.452617\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012503.\n",
      "\n",
      "Testing Configuration 12: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:15.274925\n",
      "\n",
      "Testing Configuration 13: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 456 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.692096\n",
      "\n",
      "Testing Configuration 14: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:16.000487\n",
      "\n",
      "Testing Configuration 15: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:16.462160\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010503.\n",
      "\n",
      "Testing Configuration 16: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 209 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.764111\n",
      "\n",
      "Testing Configuration 17: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 451 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.812279\n",
      "\n",
      "Testing Configuration 18: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 379 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.195554\n",
      "\n",
      "Testing Configuration 19: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 346 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.560396\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 20: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 202 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.514641\n",
      "\n",
      "Testing Configuration 21: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 101 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.643894\n",
      "\n",
      "Testing Configuration 22: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:17.540387\n",
      "\n",
      "Testing Configuration 23: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 382 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.559772\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010499.\n",
      "\n",
      "Testing Configuration 24: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 401 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.105587\n",
      "\n",
      "Testing Configuration 25: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 325 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.006252\n",
      "\n",
      "Testing Configuration 26: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 204 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.099062\n",
      "\n",
      "Testing Configuration 27: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 481 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.586223\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 28: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 402 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.612184\n",
      "\n",
      "Testing Configuration 29: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 293 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.951710\n",
      "\n",
      "Testing Configuration 30: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 464 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.261599\n",
      "\n",
      "Testing Configuration 31: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 386 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.376034\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.013999.\n",
      "\n",
      "Testing Configuration 32: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 376 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.808111\n",
      "\n",
      "Testing Configuration 33: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 361 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.011533\n",
      "\n",
      "Testing Configuration 34: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:15.131111\n",
      "\n",
      "Testing Configuration 35: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:15.556052\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 36: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:12.993519\n",
      "\n",
      "Testing Configuration 37: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 479 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.774935\n",
      "\n",
      "Testing Configuration 38: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 308 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.322068\n",
      "\n",
      "Testing Configuration 39: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 414 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.641893\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010999.\n",
      "\n",
      "Testing Configuration 40: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 310 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.252525\n",
      "\n",
      "Testing Configuration 41: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 346 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.612953\n",
      "\n",
      "Testing Configuration 42: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 480 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.732355\n",
      "\n",
      "Testing Configuration 43: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 333 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.341725\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 44: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 313 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.969784\n",
      "\n",
      "Testing Configuration 45: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 378 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.373106\n",
      "\n",
      "Testing Configuration 46: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 490 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.812962\n",
      "\n",
      "Testing Configuration 47: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 396 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.934195\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010000.\n",
      "\n",
      "Testing Configuration 48: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 112 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.656248\n",
      "\n",
      "Testing Configuration 49: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 191 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.664430\n",
      "\n",
      "Testing Configuration 50: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:17.191267\n",
      "\n",
      "Testing Configuration 51: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 398 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.240957\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 52: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 297 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.299345\n",
      "\n",
      "Testing Configuration 53: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 287 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.829540\n",
      "\n",
      "Testing Configuration 54: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:17.978114\n",
      "\n",
      "Testing Configuration 55: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:16.819035\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 56: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 315 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.944680\n",
      "\n",
      "Testing Configuration 57: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.262622\n",
      "\n",
      "Testing Configuration 58: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 336 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.972581\n",
      "\n",
      "Testing Configuration 59: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 342 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.874213\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010499.\n",
      "\n",
      "Testing Configuration 60: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 137 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.534900\n",
      "\n",
      "Testing Configuration 61: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 213 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.019707\n",
      "\n",
      "Testing Configuration 62: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 341 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.823259\n",
      "\n",
      "Testing Configuration 63: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 358 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.247049\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 64: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 178 (no improvement in 50 epochs)\n",
      "Training time:  0:02:28.596520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 65: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 281 (no improvement in 50 epochs)\n",
      "Training time:  0:03:54.102203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 66: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 284 (no improvement in 50 epochs)\n",
      "Training time:  0:03:58.325980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 67: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 237 (no improvement in 50 epochs)\n",
      "Training time:  0:03:18.420303\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 68: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 178 (no improvement in 50 epochs)\n",
      "Training time:  0:02:29.209990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 69: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 214 (no improvement in 50 epochs)\n",
      "Training time:  0:02:59.435223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 70: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 355 (no improvement in 50 epochs)\n",
      "Training time:  0:04:56.359438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 71: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 283 (no improvement in 50 epochs)\n",
      "Training time:  0:03:56.536731\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 72: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:01:45.920933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 73: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 159 (no improvement in 50 epochs)\n",
      "Training time:  0:02:13.858545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 74: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 295 (no improvement in 50 epochs)\n",
      "Training time:  0:04:07.295341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 75: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 250 (no improvement in 50 epochs)\n",
      "Training time:  0:03:30.106303\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.015000.\n",
      "\n",
      "Testing Configuration 76: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 214 (no improvement in 50 epochs)\n",
      "Training time:  0:03:00.979794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 77: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 255 (no improvement in 50 epochs)\n",
      "Training time:  0:03:35.097738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 78: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 298 (no improvement in 50 epochs)\n",
      "Training time:  0:04:10.568621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 79: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 422 (no improvement in 50 epochs)\n",
      "Training time:  0:05:54.670297\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.013501.\n",
      "\n",
      "Testing Configuration 80: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 185 (no improvement in 50 epochs)\n",
      "Training time:  0:05:04.358604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 81: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 213 (no improvement in 50 epochs)\n",
      "Training time:  0:05:50.754721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 82: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 265 (no improvement in 50 epochs)\n",
      "Training time:  0:07:15.070543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 83: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 446 (no improvement in 50 epochs)\n",
      "Training time:  0:12:12.503280\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 84: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 157 (no improvement in 50 epochs)\n",
      "Training time:  0:04:18.813182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 85: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 323 (no improvement in 50 epochs)\n",
      "Training time:  0:08:50.837370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 86: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 426 (no improvement in 50 epochs)\n",
      "Training time:  0:11:40.639206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 87: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Training time:  0:13:39.868977\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.015499.\n",
      "\n",
      "Testing Configuration 88: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 229 (no improvement in 50 epochs)\n",
      "Training time:  0:06:17.344082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 89: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 158 (no improvement in 50 epochs)\n",
      "Training time:  0:04:21.072622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 90: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 160 (no improvement in 50 epochs)\n",
      "Training time:  0:04:25.426531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 91: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 347 (no improvement in 50 epochs)\n",
      "Training time:  0:09:33.545324\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.016000.\n",
      "\n",
      "Testing Configuration 92: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 250 (no improvement in 50 epochs)\n",
      "Training time:  0:06:53.064165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 93: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 245 (no improvement in 50 epochs)\n",
      "Training time:  0:06:45.231980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 94: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 307 (no improvement in 50 epochs)\n",
      "Training time:  0:08:26.966516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 95: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 345 (no improvement in 50 epochs)\n",
      "Training time:  0:09:32.229118\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010999.\n",
      "\n",
      "Testing Configuration 96: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 186 (no improvement in 50 epochs)\n",
      "Training time:  0:07:25.878942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 97: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 214 (no improvement in 50 epochs)\n",
      "Training time:  0:08:34.112321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 98: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 373 (no improvement in 50 epochs)\n",
      "Training time:  0:14:55.960200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 99: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 205 (no improvement in 50 epochs)\n",
      "Training time:  0:08:11.716779\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 100: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 234 (no improvement in 50 epochs)\n",
      "Training time:  0:09:22.237857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 101: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 192 (no improvement in 50 epochs)\n",
      "Training time:  0:07:40.289985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 102: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 291 (no improvement in 50 epochs)\n",
      "Training time:  0:11:40.702268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 103: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 313 (no improvement in 50 epochs)\n",
      "Training time:  0:12:33.293685\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 104: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 217 (no improvement in 50 epochs)\n",
      "Training time:  0:08:42.605952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 105: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 305 (no improvement in 50 epochs)\n",
      "Training time:  0:12:13.623142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 106: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 353 (no improvement in 50 epochs)\n",
      "Training time:  0:14:08.198004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 107: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 319 (no improvement in 50 epochs)\n",
      "Training time:  0:12:48.803640\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.014500.\n",
      "\n",
      "Testing Configuration 108: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 242 (no improvement in 50 epochs)\n",
      "Training time:  0:09:42.877949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 109: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 316 (no improvement in 50 epochs)\n",
      "Training time:  0:12:41.205204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 110: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 383 (no improvement in 50 epochs)\n",
      "Training time:  0:15:21.127478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 111: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 249 (no improvement in 50 epochs)\n",
      "Training time:  0:10:01.044356\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012501.\n",
      "\n",
      "Testing Configuration 112: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 360 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.980994\n",
      "\n",
      "Testing Configuration 113: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 323 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.953609\n",
      "\n",
      "Testing Configuration 114: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 475 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.545913\n",
      "\n",
      "Testing Configuration 115: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 500 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.098972\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 116: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 156 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.120799\n",
      "\n",
      "Testing Configuration 117: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.359053\n",
      "\n",
      "Testing Configuration 118: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 184 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.098134\n",
      "\n",
      "Testing Configuration 119: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 256 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.482369\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010502.\n",
      "\n",
      "Testing Configuration 120: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 291 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.804702\n",
      "\n",
      "Testing Configuration 121: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 227 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.510840\n",
      "\n",
      "Testing Configuration 122: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 371 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.973758\n",
      "\n",
      "Testing Configuration 123: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 410 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.082327\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010499.\n",
      "\n",
      "Testing Configuration 124: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 174 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.896505\n",
      "\n",
      "Testing Configuration 125: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.657953\n",
      "\n",
      "Testing Configuration 126: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.094357\n",
      "\n",
      "Testing Configuration 127: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 171 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.373321\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010000.\n",
      "\n",
      "Testing Configuration 128: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 197 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.398563\n",
      "\n",
      "Testing Configuration 129: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 291 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.306274\n",
      "\n",
      "Testing Configuration 130: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 379 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.301563\n",
      "\n",
      "Testing Configuration 131: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 253 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.128020\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 132: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 134 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.757434\n",
      "\n",
      "Testing Configuration 133: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 131 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.751496\n",
      "\n",
      "Testing Configuration 134: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 175 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.009365\n",
      "\n",
      "Testing Configuration 135: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 135 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.305996\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 136: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 265 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.570420\n",
      "\n",
      "Testing Configuration 137: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 240 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.551868\n",
      "\n",
      "Testing Configuration 138: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 279 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.039518\n",
      "\n",
      "Testing Configuration 139: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 306 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.720506\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 140: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 121 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.880689\n",
      "\n",
      "Testing Configuration 141: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 105 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.204524\n",
      "\n",
      "Testing Configuration 142: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 105 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.624675\n",
      "\n",
      "Testing Configuration 143: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 141 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.590872\n",
      "\n",
      "Processing Model: OptResNet1D_JARILWWF with params {'input_channels': 60, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 144: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 79 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.822038\n",
      "\n",
      "Testing Configuration 145: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.731292\n",
      "\n",
      "Testing Configuration 146: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 134 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.339376\n",
      "\n",
      "Testing Configuration 147: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 182 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.032029\n",
      "\n",
      "Processing Model: OptECAResNet1D_JARILWWF with params {'input_channels': 60, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 148: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 146 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.708101\n",
      "\n",
      "Testing Configuration 149: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 168 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.851639\n",
      "\n",
      "Testing Configuration 150: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 97 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.938777\n",
      "\n",
      "Testing Configuration 151: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 148 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.697655\n",
      "\n",
      "Processing Model: CustomResNet1D with params {'input_channels': 60}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 152: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 146 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.431817\n",
      "\n",
      "Testing Configuration 153: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 168 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.121637\n",
      "\n",
      "Testing Configuration 154: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 128 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.286517\n",
      "\n",
      "Testing Configuration 155: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 96 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.779460\n",
      "\n",
      "Processing Model: CustomECAResNet1D with params {'input_channels': 60}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010499.\n",
      "\n",
      "Testing Configuration 156: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 179 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.241534\n",
      "\n",
      "Testing Configuration 157: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.782926\n",
      "\n",
      "Testing Configuration 158: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.420466\n",
      "\n",
      "Testing Configuration 159: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 149 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.891114\n",
      "\n",
      "--- Running seed 101010 ---\n",
      "Running 160 configurations...\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.013500.\n",
      "\n",
      "Testing Configuration 0: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:12.418990\n",
      "\n",
      "Testing Configuration 1: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:13.372037\n",
      "\n",
      "Testing Configuration 2: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:16.176353\n",
      "\n",
      "Testing Configuration 3: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:16.530178\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 4: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:14.998231\n",
      "\n",
      "Testing Configuration 5: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:14.056187\n",
      "\n",
      "Testing Configuration 6: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:14.287270\n",
      "\n",
      "Testing Configuration 7: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:14.852840\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 8: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:14.472852\n",
      "\n",
      "Testing Configuration 9: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 343 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.426756\n",
      "\n",
      "Testing Configuration 10: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:15.976242\n",
      "\n",
      "Testing Configuration 11: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:17.175131\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 12: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:16.570075\n",
      "\n",
      "Testing Configuration 13: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:15.997459\n",
      "\n",
      "Testing Configuration 14: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:16.487034\n",
      "\n",
      "Testing Configuration 15: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:18.788747\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 16: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 402 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.824177\n",
      "\n",
      "Testing Configuration 17: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:14.537847\n",
      "\n",
      "Testing Configuration 18: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:15.089575\n",
      "\n",
      "Testing Configuration 19: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 318 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.949646\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 20: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 380 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.735877\n",
      "\n",
      "Testing Configuration 21: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:16.255794\n",
      "\n",
      "Testing Configuration 22: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 419 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.639290\n",
      "\n",
      "Testing Configuration 23: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.996124\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 24: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 240 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.029148\n",
      "\n",
      "Testing Configuration 25: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 460 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.323446\n",
      "\n",
      "Testing Configuration 26: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 462 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.340400\n",
      "\n",
      "Testing Configuration 27: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:19.202583\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 28: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 269 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.730962\n",
      "\n",
      "Testing Configuration 29: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 384 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.001924\n",
      "\n",
      "Testing Configuration 30: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:19.411915\n",
      "\n",
      "Testing Configuration 31: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 481 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.520818\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010501.\n",
      "\n",
      "Testing Configuration 32: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 340 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.657817\n",
      "\n",
      "Testing Configuration 33: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 357 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.538307\n",
      "\n",
      "Testing Configuration 34: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:14.654629\n",
      "\n",
      "Testing Configuration 35: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:15.342214\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010000.\n",
      "\n",
      "Testing Configuration 36: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:12.508494\n",
      "\n",
      "Testing Configuration 37: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 478 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.063859\n",
      "\n",
      "Testing Configuration 38: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:16.834090\n",
      "\n",
      "Testing Configuration 39: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:15.221362\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 40: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 386 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.039490\n",
      "\n",
      "Testing Configuration 41: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 326 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.706888\n",
      "\n",
      "Testing Configuration 42: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 382 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.962932\n",
      "\n",
      "Testing Configuration 43: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 446 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.637801\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 44: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 494 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.193442\n",
      "\n",
      "Testing Configuration 45: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 349 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.476386\n",
      "\n",
      "Testing Configuration 46: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 367 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.687741\n",
      "\n",
      "Testing Configuration 47: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 419 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.084596\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010000.\n",
      "\n",
      "Testing Configuration 48: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 278 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.512276\n",
      "\n",
      "Testing Configuration 49: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 285 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.216342\n",
      "\n",
      "Testing Configuration 50: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 335 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.006552\n",
      "\n",
      "Testing Configuration 51: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 440 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.595226\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 52: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 277 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.774913\n",
      "\n",
      "Testing Configuration 53: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 473 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.675743\n",
      "\n",
      "Testing Configuration 54: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 394 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.222390\n",
      "\n",
      "Testing Configuration 55: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 329 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.558761\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 56: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 97 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.938113\n",
      "\n",
      "Testing Configuration 57: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 169 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.974834\n",
      "\n",
      "Testing Configuration 58: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 334 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.164250\n",
      "\n",
      "Testing Configuration 59: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 373 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.806407\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010500.\n",
      "\n",
      "Testing Configuration 60: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 422 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.788986\n",
      "\n",
      "Testing Configuration 61: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 383 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.867773\n",
      "\n",
      "Testing Configuration 62: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 326 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.365590\n",
      "\n",
      "Testing Configuration 63: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 439 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.631752\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010000.\n",
      "\n",
      "Testing Configuration 64: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 250 (no improvement in 50 epochs)\n",
      "Training time:  0:03:39.936670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 65: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 191 (no improvement in 50 epochs)\n",
      "Training time:  0:02:55.276502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 66: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 346 (no improvement in 50 epochs)\n",
      "Training time:  0:05:22.941490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 67: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 284 (no improvement in 50 epochs)\n",
      "Training time:  0:04:21.317109\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.013019.\n",
      "\n",
      "Testing Configuration 68: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 189 (no improvement in 50 epochs)\n",
      "Training time:  0:02:49.305302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 69: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 175 (no improvement in 50 epochs)\n",
      "Training time:  0:02:37.003743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 70: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 322 (no improvement in 50 epochs)\n",
      "Training time:  0:04:50.731822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 71: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 256 (no improvement in 50 epochs)\n",
      "Training time:  0:03:53.155445\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011506.\n",
      "\n",
      "Testing Configuration 72: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 125 (no improvement in 50 epochs)\n",
      "Training time:  0:01:57.654846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 73: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 199 (no improvement in 50 epochs)\n",
      "Training time:  0:03:14.782944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 74: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 231 (no improvement in 50 epochs)\n",
      "Training time:  0:03:33.176292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 75: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 454 (no improvement in 50 epochs)\n",
      "Training time:  0:06:56.831634\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011511.\n",
      "\n",
      "Testing Configuration 76: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 156 (no improvement in 50 epochs)\n",
      "Training time:  0:02:29.006279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 77: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 225 (no improvement in 50 epochs)\n",
      "Training time:  0:03:31.452414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 78: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 465 (no improvement in 50 epochs)\n",
      "Training time:  0:07:06.902574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 79: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 239 (no improvement in 50 epochs)\n",
      "Training time:  0:03:43.006680\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012508.\n",
      "\n",
      "Testing Configuration 80: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:04:03.210120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 81: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 194 (no improvement in 50 epochs)\n",
      "Training time:  0:10:22.515299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 82: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 275 (no improvement in 50 epochs)\n",
      "Training time:  0:34:21.847036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 83: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 187 (no improvement in 50 epochs)\n",
      "Training time:  0:22:16.748827\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.098022.\n",
      "\n",
      "Testing Configuration 84: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:18:02.007418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 85: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 264 (no improvement in 50 epochs)\n",
      "Training time:  0:17:45.984323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 86: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Training time:  1:01:11.465994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 87: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 356 (no improvement in 50 epochs)\n",
      "Training time:  0:29:14.381834\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011507.\n",
      "\n",
      "Testing Configuration 88: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 134 (no improvement in 50 epochs)\n",
      "Training time:  0:03:55.424420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 89: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 236 (no improvement in 50 epochs)\n",
      "Training time:  0:06:56.192397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 90: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 343 (no improvement in 50 epochs)\n",
      "Training time:  0:10:10.469990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 91: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 278 (no improvement in 50 epochs)\n",
      "Training time:  0:08:40.044865\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011507.\n",
      "\n",
      "Testing Configuration 92: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 180 (no improvement in 50 epochs)\n",
      "Training time:  0:05:35.799057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 93: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 332 (no improvement in 50 epochs)\n",
      "Training time:  0:10:19.702187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 94: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 475 (no improvement in 50 epochs)\n",
      "Training time:  0:14:38.502483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 95: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 310 (no improvement in 50 epochs)\n",
      "Training time:  0:09:22.208979\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012506.\n",
      "\n",
      "Testing Configuration 96: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 221 (no improvement in 50 epochs)\n",
      "Training time:  0:10:18.541501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 97: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 274 (no improvement in 50 epochs)\n",
      "Training time:  0:12:46.490807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 98: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 257 (no improvement in 50 epochs)\n",
      "Training time:  0:46:23.421832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 99: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 287 (no improvement in 50 epochs)\n",
      "Training time:  0:53:24.488226\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.045060.\n",
      "\n",
      "Testing Configuration 100: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 177 (no improvement in 50 epochs)\n",
      "Training time:  0:33:27.821007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 101: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 198 (no improvement in 50 epochs)\n",
      "Training time:  0:37:51.531043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 102: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 276 (no improvement in 50 epochs)\n",
      "Training time:  0:48:26.440388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 103: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Training time:  0:58:05.718607\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012500.\n",
      "\n",
      "Testing Configuration 104: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 187 (no improvement in 50 epochs)\n",
      "Training time:  0:07:21.010219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 105: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 154 (no improvement in 50 epochs)\n",
      "Training time:  0:06:03.482167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 106: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 255 (no improvement in 50 epochs)\n",
      "Training time:  0:10:01.063879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 107: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 381 (no improvement in 50 epochs)\n",
      "Training time:  0:14:58.276481\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 108: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 209 (no improvement in 50 epochs)\n",
      "Training time:  0:08:11.178163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 109: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 188 (no improvement in 50 epochs)\n",
      "Training time:  0:07:23.040152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 110: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 383 (no improvement in 50 epochs)\n",
      "Training time:  0:14:59.623346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 111: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 312 (no improvement in 50 epochs)\n",
      "Training time:  0:12:16.738456\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.073501.\n",
      "\n",
      "Testing Configuration 112: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 398 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.747410\n",
      "\n",
      "Testing Configuration 113: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 286 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.365617\n",
      "\n",
      "Testing Configuration 114: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 294 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.265164\n",
      "\n",
      "Testing Configuration 115: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 450 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.620873\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 116: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 170 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.965252\n",
      "\n",
      "Testing Configuration 117: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 180 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.782077\n",
      "\n",
      "Testing Configuration 118: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 227 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.805264\n",
      "\n",
      "Testing Configuration 119: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 190 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.886696\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 120: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 326 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.255294\n",
      "\n",
      "Testing Configuration 121: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 277 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.928956\n",
      "\n",
      "Testing Configuration 122: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 324 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.648919\n",
      "\n",
      "Testing Configuration 123: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 386 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.628995\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011505.\n",
      "\n",
      "Testing Configuration 124: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.681331\n",
      "\n",
      "Testing Configuration 125: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.550512\n",
      "\n",
      "Testing Configuration 126: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 158 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.992293\n",
      "\n",
      "Testing Configuration 127: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 95 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.706568\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011009.\n",
      "\n",
      "Testing Configuration 128: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 192 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.883218\n",
      "\n",
      "Testing Configuration 129: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.709263\n",
      "\n",
      "Testing Configuration 130: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 322 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.830255\n",
      "\n",
      "Testing Configuration 131: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 347 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.577908\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011512.\n",
      "\n",
      "Testing Configuration 132: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 155 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.926342\n",
      "\n",
      "Testing Configuration 133: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 120 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.458827\n",
      "\n",
      "Testing Configuration 134: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 118 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.657054\n",
      "\n",
      "Testing Configuration 135: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 131 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.407022\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 136: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 277 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.365983\n",
      "\n",
      "Testing Configuration 137: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 267 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.806402\n",
      "\n",
      "Testing Configuration 138: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 280 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.982903\n",
      "\n",
      "Testing Configuration 139: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 242 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.957311\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011505.\n",
      "\n",
      "Testing Configuration 140: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 140 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.455457\n",
      "\n",
      "Testing Configuration 141: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 142 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.413488\n",
      "\n",
      "Testing Configuration 142: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.672074\n",
      "\n",
      "Testing Configuration 143: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 178 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.935547\n",
      "\n",
      "Processing Model: OptResNet1D_JARILWWF with params {'input_channels': 60, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011067.\n",
      "\n",
      "Testing Configuration 144: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.823934\n",
      "\n",
      "Testing Configuration 145: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 210 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.394652\n",
      "\n",
      "Testing Configuration 146: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 71 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.579521\n",
      "\n",
      "Testing Configuration 147: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 159 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.914215\n",
      "\n",
      "Processing Model: OptECAResNet1D_JARILWWF with params {'input_channels': 60, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011507.\n",
      "\n",
      "Testing Configuration 148: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.950423\n",
      "\n",
      "Testing Configuration 149: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 135 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.005625\n",
      "\n",
      "Testing Configuration 150: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 198 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.780816\n",
      "\n",
      "Testing Configuration 151: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.658002\n",
      "\n",
      "Processing Model: CustomResNet1D with params {'input_channels': 60}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011006.\n",
      "\n",
      "Testing Configuration 152: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 117 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.556028\n",
      "\n",
      "Testing Configuration 153: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 132 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.931355\n",
      "\n",
      "Testing Configuration 154: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 56 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.261472\n",
      "\n",
      "Testing Configuration 155: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 115 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.866378\n",
      "\n",
      "Processing Model: CustomECAResNet1D with params {'input_channels': 60}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011005.\n",
      "\n",
      "Testing Configuration 156: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 71 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.846815\n",
      "\n",
      "Testing Configuration 157: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 178 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.538790\n",
      "\n",
      "Testing Configuration 158: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 134 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.539867\n",
      "\n",
      "Testing Configuration 159: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 87 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.686130\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T15:00:51.845910Z",
     "start_time": "2025-05-29T04:36:27.850693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gridsearch_results_df_bgsub = run_gridsearch_with_seeds(\n",
    "    gridsearch_func=grid_search,\n",
    "    gridsearch_args={\n",
    "        'param_grid': param_grid,\n",
    "        'folder_path': folder_path_60ghz_collected,\n",
    "        'background_subtraction': True,\n",
    "        'seconds_per_sample': 5,\n",
    "        'rows_per_second': 10,\n",
    "    },\n",
    "    n_seeds=3,\n",
    "    output_dir=output_path,\n",
    "    filename=\"gridsearch_60ghz_coll\"\n",
    ")"
   ],
   "id": "b22eb10fc55a47b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running seed 42 ---\n",
      "Running 160 configurations...\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.093973.\n",
      "\n",
      "Testing Configuration 0: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 459 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.604882\n",
      "\n",
      "Testing Configuration 1: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 248 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.042000\n",
      "\n",
      "Testing Configuration 2: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 214 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.710490\n",
      "\n",
      "Testing Configuration 3: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 228 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.266418\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.013500.\n",
      "\n",
      "Testing Configuration 4: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 214 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.994140\n",
      "\n",
      "Testing Configuration 5: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 262 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.546639\n",
      "\n",
      "Testing Configuration 6: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 421 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.846980\n",
      "\n",
      "Testing Configuration 7: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 427 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.516912\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 8: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 364 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.841403\n",
      "\n",
      "Testing Configuration 9: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 389 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.313330\n",
      "\n",
      "Testing Configuration 10: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 184 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.520597\n",
      "\n",
      "Testing Configuration 11: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 242 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.585677\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 12: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 332 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.914551\n",
      "\n",
      "Testing Configuration 13: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 195 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.757280\n",
      "\n",
      "Testing Configuration 14: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 234 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.194000\n",
      "\n",
      "Testing Configuration 15: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 142 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.900722\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011999.\n",
      "\n",
      "Testing Configuration 16: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 240 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.529685\n",
      "\n",
      "Testing Configuration 17: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.721747\n",
      "\n",
      "Testing Configuration 18: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 168 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.405354\n",
      "\n",
      "Testing Configuration 19: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 152 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.717112\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012500.\n",
      "\n",
      "Testing Configuration 20: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 153 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.119810\n",
      "\n",
      "Testing Configuration 21: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 189 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.953937\n",
      "\n",
      "Testing Configuration 22: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 269 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.928265\n",
      "\n",
      "Testing Configuration 23: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 214 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.805986\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012500.\n",
      "\n",
      "Testing Configuration 24: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 129 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.209024\n",
      "\n",
      "Testing Configuration 25: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 146 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.225568\n",
      "\n",
      "Testing Configuration 26: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 119 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.399809\n",
      "\n",
      "Testing Configuration 27: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 235 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.427465\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 28: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 104 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.114831\n",
      "\n",
      "Testing Configuration 29: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 113 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.481458\n",
      "\n",
      "Testing Configuration 30: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 438 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.707365\n",
      "\n",
      "Testing Configuration 31: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 264 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.917673\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.013000.\n",
      "\n",
      "Testing Configuration 32: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 272 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.769119\n",
      "\n",
      "Testing Configuration 33: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 268 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.007999\n",
      "\n",
      "Testing Configuration 34: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 152 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.660090\n",
      "\n",
      "Testing Configuration 35: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 208 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.256010\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 36: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 330 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.546245\n",
      "\n",
      "Testing Configuration 37: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 187 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.882408\n",
      "\n",
      "Testing Configuration 38: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 200 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.180828\n",
      "\n",
      "Testing Configuration 39: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 300 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.801325\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.016000.\n",
      "\n",
      "Testing Configuration 40: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 263 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.718749\n",
      "\n",
      "Testing Configuration 41: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 439 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.152542\n",
      "\n",
      "Testing Configuration 42: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 183 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.212118\n",
      "\n",
      "Testing Configuration 43: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 112 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.160763\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 44: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 286 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.357150\n",
      "\n",
      "Testing Configuration 45: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 182 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.094127\n",
      "\n",
      "Testing Configuration 46: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 215 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.616388\n",
      "\n",
      "Testing Configuration 47: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 200 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.573861\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.013500.\n",
      "\n",
      "Testing Configuration 48: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.011292\n",
      "\n",
      "Testing Configuration 49: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.482204\n",
      "\n",
      "Testing Configuration 50: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 200 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.319537\n",
      "\n",
      "Testing Configuration 51: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 200 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.784733\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 52: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 79 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.203671\n",
      "\n",
      "Testing Configuration 53: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.915550\n",
      "\n",
      "Testing Configuration 54: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 263 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.405214\n",
      "\n",
      "Testing Configuration 55: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 162 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.193092\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012020.\n",
      "\n",
      "Testing Configuration 56: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 102 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.812361\n",
      "\n",
      "Testing Configuration 57: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 85 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.240673\n",
      "\n",
      "Testing Configuration 58: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.727645\n",
      "\n",
      "Testing Configuration 59: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 255 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.398070\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012500.\n",
      "\n",
      "Testing Configuration 60: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 90 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.627716\n",
      "\n",
      "Testing Configuration 61: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.196276\n",
      "\n",
      "Testing Configuration 62: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 251 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.277816\n",
      "\n",
      "Testing Configuration 63: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 165 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.299373\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 64: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 110 (no improvement in 50 epochs)\n",
      "Training time:  0:01:32.405313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 65: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 104 (no improvement in 50 epochs)\n",
      "Training time:  0:01:26.479996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 66: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 191 (no improvement in 50 epochs)\n",
      "Training time:  0:02:38.184125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 67: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 272 (no improvement in 50 epochs)\n",
      "Training time:  0:03:44.081024\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.071002.\n",
      "\n",
      "Testing Configuration 68: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 89 (no improvement in 50 epochs)\n",
      "Training time:  0:01:13.741017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 69: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 105 (no improvement in 50 epochs)\n",
      "Training time:  0:01:26.417423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 70: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 165 (no improvement in 50 epochs)\n",
      "Training time:  0:02:15.710520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 71: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 208 (no improvement in 50 epochs)\n",
      "Training time:  0:02:51.046528\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.014500.\n",
      "\n",
      "Testing Configuration 72: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 170 (no improvement in 50 epochs)\n",
      "Training time:  0:02:18.949086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 73: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:01:17.054770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 74: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 297 (no improvement in 50 epochs)\n",
      "Training time:  0:04:04.068482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 75: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 147 (no improvement in 50 epochs)\n",
      "Training time:  0:02:00.446444\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.015000.\n",
      "\n",
      "Testing Configuration 76: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 130 (no improvement in 50 epochs)\n",
      "Training time:  0:01:48.032334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 77: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 129 (no improvement in 50 epochs)\n",
      "Training time:  0:01:46.981891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 78: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 180 (no improvement in 50 epochs)\n",
      "Training time:  0:02:27.255365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 79: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 156 (no improvement in 50 epochs)\n",
      "Training time:  0:02:08.957675\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 80: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 85 (no improvement in 50 epochs)\n",
      "Training time:  0:02:15.667549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 81: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 102 (no improvement in 50 epochs)\n",
      "Training time:  0:02:42.478513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 82: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 165 (no improvement in 50 epochs)\n",
      "Training time:  0:04:23.529394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 83: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 266 (no improvement in 50 epochs)\n",
      "Training time:  0:07:03.689987\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.016000.\n",
      "\n",
      "Testing Configuration 84: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 230 (no improvement in 50 epochs)\n",
      "Training time:  0:06:06.783226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 85: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 137 (no improvement in 50 epochs)\n",
      "Training time:  0:03:39.182134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 86: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 196 (no improvement in 50 epochs)\n",
      "Training time:  0:05:12.417227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 87: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 242 (no improvement in 50 epochs)\n",
      "Training time:  0:06:25.171625\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012499.\n",
      "\n",
      "Testing Configuration 88: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 168 (no improvement in 50 epochs)\n",
      "Training time:  0:04:28.779758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 89: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 87 (no improvement in 50 epochs)\n",
      "Training time:  0:02:19.037462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 90: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 180 (no improvement in 50 epochs)\n",
      "Training time:  0:04:48.667244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 91: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 124 (no improvement in 50 epochs)\n",
      "Training time:  0:03:18.400062\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.014998.\n",
      "\n",
      "Testing Configuration 92: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 172 (no improvement in 50 epochs)\n",
      "Training time:  0:04:35.668024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 93: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 175 (no improvement in 50 epochs)\n",
      "Training time:  0:04:38.542334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 94: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 173 (no improvement in 50 epochs)\n",
      "Training time:  0:04:37.673961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 95: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:03:22.648021\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011999.\n",
      "\n",
      "Testing Configuration 96: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 68 (no improvement in 50 epochs)\n",
      "Training time:  0:02:38.265471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 97: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:03:23.365499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 98: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 162 (no improvement in 50 epochs)\n",
      "Training time:  0:06:17.442441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 99: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 236 (no improvement in 50 epochs)\n",
      "Training time:  0:09:07.606973\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 100: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 105 (no improvement in 50 epochs)\n",
      "Training time:  0:04:04.005061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 101: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:03:12.836262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 102: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 238 (no improvement in 50 epochs)\n",
      "Training time:  0:09:10.741138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 103: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 130 (no improvement in 50 epochs)\n",
      "Training time:  0:05:04.163825\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012500.\n",
      "\n",
      "Testing Configuration 104: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 105 (no improvement in 50 epochs)\n",
      "Training time:  0:04:03.467330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 105: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 238 (no improvement in 50 epochs)\n",
      "Training time:  0:09:13.283407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 106: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 119 (no improvement in 50 epochs)\n",
      "Training time:  0:04:37.166956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 107: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 140 (no improvement in 50 epochs)\n",
      "Training time:  0:05:27.426528\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011998.\n",
      "\n",
      "Testing Configuration 108: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 101 (no improvement in 50 epochs)\n",
      "Training time:  0:03:55.285857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 109: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 131 (no improvement in 50 epochs)\n",
      "Training time:  0:05:04.734928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 110: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 172 (no improvement in 50 epochs)\n",
      "Training time:  0:06:40.412040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 111: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 145 (no improvement in 50 epochs)\n",
      "Training time:  0:05:39.109238\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 112: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 192 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.449508\n",
      "\n",
      "Testing Configuration 113: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 251 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.021286\n",
      "\n",
      "Testing Configuration 114: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 353 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.163386\n",
      "\n",
      "Testing Configuration 115: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.796100\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 116: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 131 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.152064\n",
      "\n",
      "Testing Configuration 117: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.203488\n",
      "\n",
      "Testing Configuration 118: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 156 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.713300\n",
      "\n",
      "Testing Configuration 119: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 188 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.150922\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012500.\n",
      "\n",
      "Testing Configuration 120: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 145 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.425945\n",
      "\n",
      "Testing Configuration 121: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.714994\n",
      "\n",
      "Testing Configuration 122: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 192 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.620640\n",
      "\n",
      "Testing Configuration 123: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 244 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.356432\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 124: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 205 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.241228\n",
      "\n",
      "Testing Configuration 125: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 132 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.215067\n",
      "\n",
      "Testing Configuration 126: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 146 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.257796\n",
      "\n",
      "Testing Configuration 127: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 212 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.388071\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 128: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 173 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.150392\n",
      "\n",
      "Testing Configuration 129: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 129 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.547278\n",
      "\n",
      "Testing Configuration 130: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 197 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.681682\n",
      "\n",
      "Testing Configuration 131: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 194 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.601572\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011998.\n",
      "\n",
      "Testing Configuration 132: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.388544\n",
      "\n",
      "Testing Configuration 133: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 118 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.546058\n",
      "\n",
      "Testing Configuration 134: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 155 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.942779\n",
      "\n",
      "Testing Configuration 135: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 154 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.991973\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 136: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 145 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.791870\n",
      "\n",
      "Testing Configuration 137: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 115 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.281827\n",
      "\n",
      "Testing Configuration 138: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 284 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.912420\n",
      "\n",
      "Testing Configuration 139: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 218 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.713873\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 140: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 104 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.474636\n",
      "\n",
      "Testing Configuration 141: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 181 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.187698\n",
      "\n",
      "Testing Configuration 142: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 155 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.238242\n",
      "\n",
      "Testing Configuration 143: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 185 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.066230\n",
      "\n",
      "Processing Model: OptResNet1D_JARILWWF with params {'input_channels': 60, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 144: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 74 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.121703\n",
      "\n",
      "Testing Configuration 145: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 82 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.664860\n",
      "\n",
      "Testing Configuration 146: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 70 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.361533\n",
      "\n",
      "Testing Configuration 147: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 78 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.619776\n",
      "\n",
      "Processing Model: OptECAResNet1D_JARILWWF with params {'input_channels': 60, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.015500.\n",
      "\n",
      "Testing Configuration 148: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.166972\n",
      "\n",
      "Testing Configuration 149: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.491541\n",
      "\n",
      "Testing Configuration 150: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 57 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.927110\n",
      "\n",
      "Testing Configuration 151: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 59 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.386128\n",
      "\n",
      "Processing Model: CustomResNet1D with params {'input_channels': 60}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012999.\n",
      "\n",
      "Testing Configuration 152: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 72 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.211258\n",
      "\n",
      "Testing Configuration 153: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 73 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.099807\n",
      "\n",
      "Testing Configuration 154: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.341781\n",
      "\n",
      "Testing Configuration 155: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.209807\n",
      "\n",
      "Processing Model: CustomECAResNet1D with params {'input_channels': 60}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012500.\n",
      "\n",
      "Testing Configuration 156: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 67 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.438429\n",
      "\n",
      "Testing Configuration 157: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 70 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.678567\n",
      "\n",
      "Testing Configuration 158: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 145 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.247479\n",
      "\n",
      "Testing Configuration 159: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.977120\n",
      "\n",
      "--- Running seed 420 ---\n",
      "Running 160 configurations...\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011497.\n",
      "\n",
      "Testing Configuration 0: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 267 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.332239\n",
      "\n",
      "Testing Configuration 1: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 171 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.396526\n",
      "\n",
      "Testing Configuration 2: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 182 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.069378\n",
      "\n",
      "Testing Configuration 3: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 243 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.823076\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 4: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 215 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.238581\n",
      "\n",
      "Testing Configuration 5: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 238 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.359098\n",
      "\n",
      "Testing Configuration 6: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 500 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.417200\n",
      "\n",
      "Testing Configuration 7: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 286 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.220871\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012166.\n",
      "\n",
      "Testing Configuration 8: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 293 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.371020\n",
      "\n",
      "Testing Configuration 9: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 320 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.528012\n",
      "\n",
      "Testing Configuration 10: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 225 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.157519\n",
      "\n",
      "Testing Configuration 11: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 152 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.689050\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011001.\n",
      "\n",
      "Testing Configuration 12: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 259 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.468954\n",
      "\n",
      "Testing Configuration 13: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 283 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.824031\n",
      "\n",
      "Testing Configuration 14: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 282 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.269141\n",
      "\n",
      "Testing Configuration 15: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 392 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.726375\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.013000.\n",
      "\n",
      "Testing Configuration 16: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 158 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.685045\n",
      "\n",
      "Testing Configuration 17: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 131 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.708978\n",
      "\n",
      "Testing Configuration 18: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 157 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.871061\n",
      "\n",
      "Testing Configuration 19: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 258 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.501816\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.014499.\n",
      "\n",
      "Testing Configuration 20: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 270 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.403000\n",
      "\n",
      "Testing Configuration 21: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 152 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.510978\n",
      "\n",
      "Testing Configuration 22: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 258 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.885037\n",
      "\n",
      "Testing Configuration 23: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 282 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.249756\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012500.\n",
      "\n",
      "Testing Configuration 24: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.087568\n",
      "\n",
      "Testing Configuration 25: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 121 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.326040\n",
      "\n",
      "Testing Configuration 26: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 233 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.750556\n",
      "\n",
      "Testing Configuration 27: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 146 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.140513\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 28: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 124 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.717545\n",
      "\n",
      "Testing Configuration 29: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 174 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.052907\n",
      "\n",
      "Testing Configuration 30: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 230 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.988868\n",
      "\n",
      "Testing Configuration 31: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 156 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.996734\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.014999.\n",
      "\n",
      "Testing Configuration 32: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 234 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.236792\n",
      "\n",
      "Testing Configuration 33: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 215 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.652005\n",
      "\n",
      "Testing Configuration 34: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 143 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.112664\n",
      "\n",
      "Testing Configuration 35: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 288 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.920342\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 36: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 184 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.620832\n",
      "\n",
      "Testing Configuration 37: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 235 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.450199\n",
      "\n",
      "Testing Configuration 38: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 142 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.077494\n",
      "\n",
      "Testing Configuration 39: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 314 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.562664\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 40: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 223 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.253688\n",
      "\n",
      "Testing Configuration 41: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 298 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.033625\n",
      "\n",
      "Testing Configuration 42: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.464437\n",
      "\n",
      "Testing Configuration 43: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 158 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.529115\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 44: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 231 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.550381\n",
      "\n",
      "Testing Configuration 45: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 294 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.360178\n",
      "\n",
      "Testing Configuration 46: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 227 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.781467\n",
      "\n",
      "Testing Configuration 47: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 144 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.053960\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 48: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.387986\n",
      "\n",
      "Testing Configuration 49: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.363601\n",
      "\n",
      "Testing Configuration 50: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 192 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.198464\n",
      "\n",
      "Testing Configuration 51: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.469739\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 52: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 123 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.369420\n",
      "\n",
      "Testing Configuration 53: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 108 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.104580\n",
      "\n",
      "Testing Configuration 54: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 230 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.079705\n",
      "\n",
      "Testing Configuration 55: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 173 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.767235\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 56: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 62 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.644042\n",
      "\n",
      "Testing Configuration 57: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 228 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.619921\n",
      "\n",
      "Testing Configuration 58: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 87 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.808344\n",
      "\n",
      "Testing Configuration 59: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 137 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.994457\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 60: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 75 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.770148\n",
      "\n",
      "Testing Configuration 61: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.804157\n",
      "\n",
      "Testing Configuration 62: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 131 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.439468\n",
      "\n",
      "Testing Configuration 63: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 131 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.335959\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 64: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 78 (no improvement in 50 epochs)\n",
      "Training time:  0:01:04.047999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 65: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 121 (no improvement in 50 epochs)\n",
      "Training time:  0:01:39.643696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 66: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 215 (no improvement in 50 epochs)\n",
      "Training time:  0:02:57.152006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 67: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 299 (no improvement in 50 epochs)\n",
      "Training time:  0:04:06.570204\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 68: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 129 (no improvement in 50 epochs)\n",
      "Training time:  0:01:46.599639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 69: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:01:33.851796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 70: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 208 (no improvement in 50 epochs)\n",
      "Training time:  0:02:50.659319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 71: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:01:22.734810\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 72: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 113 (no improvement in 50 epochs)\n",
      "Training time:  0:01:33.127642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 73: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 78 (no improvement in 50 epochs)\n",
      "Training time:  0:01:05.052953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 74: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 241 (no improvement in 50 epochs)\n",
      "Training time:  0:03:19.230992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 75: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 170 (no improvement in 50 epochs)\n",
      "Training time:  0:02:20.306060\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.013501.\n",
      "\n",
      "Testing Configuration 76: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 99 (no improvement in 50 epochs)\n",
      "Training time:  0:01:22.348544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 77: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 101 (no improvement in 50 epochs)\n",
      "Training time:  0:01:23.735542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 78: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 149 (no improvement in 50 epochs)\n",
      "Training time:  0:02:03.849290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 79: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 277 (no improvement in 50 epochs)\n",
      "Training time:  0:03:49.688127\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012002.\n",
      "\n",
      "Testing Configuration 80: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:02:21.534032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 81: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 89 (no improvement in 50 epochs)\n",
      "Training time:  0:02:24.822448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 82: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 220 (no improvement in 50 epochs)\n",
      "Training time:  0:05:55.785332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 83: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:04:02.353369\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.068500.\n",
      "\n",
      "Testing Configuration 84: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 143 (no improvement in 50 epochs)\n",
      "Training time:  0:03:50.686080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 85: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:02:04.385651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 86: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 287 (no improvement in 50 epochs)\n",
      "Training time:  0:07:42.922434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 87: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 160 (no improvement in 50 epochs)\n",
      "Training time:  0:04:18.447711\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.016500.\n",
      "\n",
      "Testing Configuration 88: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:02:30.406796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 89: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 90 (no improvement in 50 epochs)\n",
      "Training time:  0:02:25.761627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 90: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 143 (no improvement in 50 epochs)\n",
      "Training time:  0:03:51.649956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 91: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 205 (no improvement in 50 epochs)\n",
      "Training time:  0:05:30.984928\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.016001.\n",
      "\n",
      "Testing Configuration 92: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 99 (no improvement in 50 epochs)\n",
      "Training time:  0:02:40.096655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 93: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 120 (no improvement in 50 epochs)\n",
      "Training time:  0:03:14.763094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 94: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:04:03.462679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 95: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 123 (no improvement in 50 epochs)\n",
      "Training time:  0:03:19.387434\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.016000.\n",
      "\n",
      "Testing Configuration 96: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 119 (no improvement in 50 epochs)\n",
      "Training time:  0:04:40.492636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 97: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:03:54.899048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 98: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 140 (no improvement in 50 epochs)\n",
      "Training time:  0:05:30.424402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 99: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 190 (no improvement in 50 epochs)\n",
      "Training time:  0:07:28.462989\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.016000.\n",
      "\n",
      "Testing Configuration 100: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 124 (no improvement in 50 epochs)\n",
      "Training time:  0:04:51.118987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 101: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 149 (no improvement in 50 epochs)\n",
      "Training time:  0:05:51.331636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 102: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 167 (no improvement in 50 epochs)\n",
      "Training time:  0:06:34.919175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 103: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 180 (no improvement in 50 epochs)\n",
      "Training time:  0:07:04.834606\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.015500.\n",
      "\n",
      "Testing Configuration 104: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 63 (no improvement in 50 epochs)\n",
      "Training time:  0:02:29.014239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 105: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:05:13.828462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 106: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 169 (no improvement in 50 epochs)\n",
      "Training time:  0:06:38.510173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 107: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 260 (no improvement in 50 epochs)\n",
      "Training time:  0:10:12.821524\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.015500.\n",
      "\n",
      "Testing Configuration 108: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 143 (no improvement in 50 epochs)\n",
      "Training time:  0:05:36.774506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 109: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 118 (no improvement in 50 epochs)\n",
      "Training time:  0:04:38.595152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 110: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 191 (no improvement in 50 epochs)\n",
      "Training time:  0:07:31.059626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 111: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 144 (no improvement in 50 epochs)\n",
      "Training time:  0:05:42.309334\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 112: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 206 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.965136\n",
      "\n",
      "Testing Configuration 113: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 184 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.297926\n",
      "\n",
      "Testing Configuration 114: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 254 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.568999\n",
      "\n",
      "Testing Configuration 115: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 266 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.196806\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 116: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.220158\n",
      "\n",
      "Testing Configuration 117: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 197 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.913253\n",
      "\n",
      "Testing Configuration 118: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 118 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.789583\n",
      "\n",
      "Testing Configuration 119: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 141 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.540475\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011999.\n",
      "\n",
      "Testing Configuration 120: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 141 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.609442\n",
      "\n",
      "Testing Configuration 121: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 274 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.348958\n",
      "\n",
      "Testing Configuration 122: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 205 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.752244\n",
      "\n",
      "Testing Configuration 123: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 220 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.633299\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 124: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 149 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.864383\n",
      "\n",
      "Testing Configuration 125: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 104 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.245581\n",
      "\n",
      "Testing Configuration 126: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 248 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.842892\n",
      "\n",
      "Testing Configuration 127: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.356625\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 128: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 134 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.792675\n",
      "\n",
      "Testing Configuration 129: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 141 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.947261\n",
      "\n",
      "Testing Configuration 130: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 197 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.431329\n",
      "\n",
      "Testing Configuration 131: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 219 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.836915\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.015001.\n",
      "\n",
      "Testing Configuration 132: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.228858\n",
      "\n",
      "Testing Configuration 133: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 95 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.077593\n",
      "\n",
      "Testing Configuration 134: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 102 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.646507\n",
      "\n",
      "Testing Configuration 135: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 125 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.690507\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 136: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 125 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.207499\n",
      "\n",
      "Testing Configuration 137: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 137 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.849958\n",
      "\n",
      "Testing Configuration 138: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 195 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.930843\n",
      "\n",
      "Testing Configuration 139: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 174 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.904504\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.013500.\n",
      "\n",
      "Testing Configuration 140: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 90 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.287889\n",
      "\n",
      "Testing Configuration 141: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 161 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.660875\n",
      "\n",
      "Testing Configuration 142: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 132 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.680760\n",
      "\n",
      "Testing Configuration 143: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 119 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.109061\n",
      "\n",
      "Processing Model: OptResNet1D_JARILWWF with params {'input_channels': 60, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 144: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 128 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.292890\n",
      "\n",
      "Testing Configuration 145: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 87 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.372718\n",
      "\n",
      "Testing Configuration 146: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.506226\n",
      "\n",
      "Testing Configuration 147: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 74 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.704671\n",
      "\n",
      "Processing Model: OptECAResNet1D_JARILWWF with params {'input_channels': 60, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 148: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 65 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.509976\n",
      "\n",
      "Testing Configuration 149: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 153 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.825096\n",
      "\n",
      "Testing Configuration 150: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 57 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.051639\n",
      "\n",
      "Testing Configuration 151: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 69 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.052132\n",
      "\n",
      "Processing Model: CustomResNet1D with params {'input_channels': 60}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 152: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 105 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.934477\n",
      "\n",
      "Testing Configuration 153: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 81 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.533244\n",
      "\n",
      "Testing Configuration 154: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.582718\n",
      "\n",
      "Testing Configuration 155: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.331954\n",
      "\n",
      "Processing Model: CustomECAResNet1D with params {'input_channels': 60}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 156: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 65 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.297789\n",
      "\n",
      "Testing Configuration 157: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 81 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.715962\n",
      "\n",
      "Testing Configuration 158: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 134 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.518493\n",
      "\n",
      "Testing Configuration 159: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.181620\n",
      "\n",
      "--- Running seed 101010 ---\n",
      "Running 160 configurations...\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 0: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 282 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.523479\n",
      "\n",
      "Testing Configuration 1: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 255 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.632446\n",
      "\n",
      "Testing Configuration 2: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 199 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.707321\n",
      "\n",
      "Testing Configuration 3: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 263 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.998078\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012500.\n",
      "\n",
      "Testing Configuration 4: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 286 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.079013\n",
      "\n",
      "Testing Configuration 5: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 316 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.492676\n",
      "\n",
      "Testing Configuration 6: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 318 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.083902\n",
      "\n",
      "Testing Configuration 7: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 331 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.624442\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 8: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 301 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.686094\n",
      "\n",
      "Testing Configuration 9: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 329 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.615496\n",
      "\n",
      "Testing Configuration 10: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 225 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.171953\n",
      "\n",
      "Testing Configuration 11: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 316 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.500051\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.015000.\n",
      "\n",
      "Testing Configuration 12: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 265 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.614710\n",
      "\n",
      "Testing Configuration 13: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 307 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.625988\n",
      "\n",
      "Testing Configuration 14: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 272 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.575871\n",
      "\n",
      "Testing Configuration 15: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 302 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.004438\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012002.\n",
      "\n",
      "Testing Configuration 16: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.603516\n",
      "\n",
      "Testing Configuration 17: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 190 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.352514\n",
      "\n",
      "Testing Configuration 18: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 256 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.766706\n",
      "\n",
      "Testing Configuration 19: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 365 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.734292\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 20: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 129 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.250688\n",
      "\n",
      "Testing Configuration 21: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 261 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.668827\n",
      "\n",
      "Testing Configuration 22: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 260 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.502404\n",
      "\n",
      "Testing Configuration 23: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 276 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.239497\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 24: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.828662\n",
      "\n",
      "Testing Configuration 25: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 112 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.940500\n",
      "\n",
      "Testing Configuration 26: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 314 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.870634\n",
      "\n",
      "Testing Configuration 27: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 179 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.547190\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012501.\n",
      "\n",
      "Testing Configuration 28: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 158 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.189930\n",
      "\n",
      "Testing Configuration 29: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.600197\n",
      "\n",
      "Testing Configuration 30: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 166 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.359870\n",
      "\n",
      "Testing Configuration 31: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 169 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.651897\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.015000.\n",
      "\n",
      "Testing Configuration 32: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 321 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.315145\n",
      "\n",
      "Testing Configuration 33: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 378 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.479397\n",
      "\n",
      "Testing Configuration 34: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 209 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.835160\n",
      "\n",
      "Testing Configuration 35: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 253 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.851432\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 36: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 170 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.296674\n",
      "\n",
      "Testing Configuration 37: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 154 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.121912\n",
      "\n",
      "Testing Configuration 38: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 317 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.416692\n",
      "\n",
      "Testing Configuration 39: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 248 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.386993\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011501.\n",
      "\n",
      "Testing Configuration 40: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 305 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.856490\n",
      "\n",
      "Testing Configuration 41: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 394 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.817389\n",
      "\n",
      "Testing Configuration 42: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 202 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.427970\n",
      "\n",
      "Testing Configuration 43: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 149 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.447911\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 44: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 477 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.819328\n",
      "\n",
      "Testing Configuration 45: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 200 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.356608\n",
      "\n",
      "Testing Configuration 46: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 259 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.602589\n",
      "\n",
      "Testing Configuration 47: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 172 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.626196\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.014001.\n",
      "\n",
      "Testing Configuration 48: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.155212\n",
      "\n",
      "Testing Configuration 49: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 142 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.131102\n",
      "\n",
      "Testing Configuration 50: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 107 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.656277\n",
      "\n",
      "Testing Configuration 51: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 129 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.174120\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011001.\n",
      "\n",
      "Testing Configuration 52: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.641043\n",
      "\n",
      "Testing Configuration 53: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 125 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.636737\n",
      "\n",
      "Testing Configuration 54: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 258 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.209859\n",
      "\n",
      "Testing Configuration 55: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 216 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.934583\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011500.\n",
      "\n",
      "Testing Configuration 56: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 73 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.964091\n",
      "\n",
      "Testing Configuration 57: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 131 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.018253\n",
      "\n",
      "Testing Configuration 58: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 190 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.671906\n",
      "\n",
      "Testing Configuration 59: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 91 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.053679\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 60, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.010999.\n",
      "\n",
      "Testing Configuration 60: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.075681\n",
      "\n",
      "Testing Configuration 61: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.935286\n",
      "\n",
      "Testing Configuration 62: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 132 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.497893\n",
      "\n",
      "Testing Configuration 63: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.581003\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012002.\n",
      "\n",
      "Testing Configuration 64: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:01:31.735750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 65: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 75 (no improvement in 50 epochs)\n",
      "Training time:  0:01:02.185036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 66: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 142 (no improvement in 50 epochs)\n",
      "Training time:  0:01:57.697889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 67: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 99 (no improvement in 50 epochs)\n",
      "Training time:  0:01:22.416933\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.013500.\n",
      "\n",
      "Testing Configuration 68: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:01:12.836363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 69: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 95 (no improvement in 50 epochs)\n",
      "Training time:  0:01:18.670068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 70: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 395 (no improvement in 50 epochs)\n",
      "Training time:  0:05:26.372974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 71: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 195 (no improvement in 50 epochs)\n",
      "Training time:  0:02:40.879054\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.015500.\n",
      "\n",
      "Testing Configuration 72: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 70 (no improvement in 50 epochs)\n",
      "Training time:  0:00:58.244179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 73: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 91 (no improvement in 50 epochs)\n",
      "Training time:  0:01:15.720091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 74: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 143 (no improvement in 50 epochs)\n",
      "Training time:  0:01:58.332351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 75: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 290 (no improvement in 50 epochs)\n",
      "Training time:  0:03:59.648447\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 76: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 156 (no improvement in 50 epochs)\n",
      "Training time:  0:02:09.094919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 77: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 125 (no improvement in 50 epochs)\n",
      "Training time:  0:01:44.366019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 78: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 206 (no improvement in 50 epochs)\n",
      "Training time:  0:02:50.650288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 79: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 113 (no improvement in 50 epochs)\n",
      "Training time:  0:01:33.309002\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012001.\n",
      "\n",
      "Testing Configuration 80: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 96 (no improvement in 50 epochs)\n",
      "Training time:  0:02:35.298291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 81: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 82 (no improvement in 50 epochs)\n",
      "Training time:  0:02:12.693243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 82: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 182 (no improvement in 50 epochs)\n",
      "Training time:  0:04:55.260251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 83: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 230 (no improvement in 50 epochs)\n",
      "Training time:  0:06:10.963985\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.015000.\n",
      "\n",
      "Testing Configuration 84: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:02:32.062236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 85: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 140 (no improvement in 50 epochs)\n",
      "Training time:  0:03:44.474153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 86: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 189 (no improvement in 50 epochs)\n",
      "Training time:  0:05:05.605255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 87: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 181 (no improvement in 50 epochs)\n",
      "Training time:  0:04:51.427635\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011000.\n",
      "\n",
      "Testing Configuration 88: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 156 (no improvement in 50 epochs)\n",
      "Training time:  0:04:12.561103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 89: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 120 (no improvement in 50 epochs)\n",
      "Training time:  0:03:14.468983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 90: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 213 (no improvement in 50 epochs)\n",
      "Training time:  0:05:45.011215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 91: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 183 (no improvement in 50 epochs)\n",
      "Training time:  0:04:57.139531\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.015000.\n",
      "\n",
      "Testing Configuration 92: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 115 (no improvement in 50 epochs)\n",
      "Training time:  0:03:06.408033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 93: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 110 (no improvement in 50 epochs)\n",
      "Training time:  0:02:58.925046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 94: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 198 (no improvement in 50 epochs)\n",
      "Training time:  0:05:22.236691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 95: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 169 (no improvement in 50 epochs)\n",
      "Training time:  0:04:36.110182\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.017500.\n",
      "\n",
      "Testing Configuration 96: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:05:11.193429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 97: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 129 (no improvement in 50 epochs)\n",
      "Training time:  0:05:04.296931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 98: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 142 (no improvement in 50 epochs)\n",
      "Training time:  0:05:35.607889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 99: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 293 (no improvement in 50 epochs)\n",
      "Training time:  0:11:29.190661\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.016000.\n",
      "\n",
      "Testing Configuration 100: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 119 (no improvement in 50 epochs)\n",
      "Training time:  0:04:39.548435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 101: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 101 (no improvement in 50 epochs)\n",
      "Training time:  0:03:56.908345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 102: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 197 (no improvement in 50 epochs)\n",
      "Training time:  0:07:46.048028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 103: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 191 (no improvement in 50 epochs)\n",
      "Training time:  0:07:31.342438\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 104: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 82 (no improvement in 50 epochs)\n",
      "Training time:  0:03:12.729794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 105: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 110 (no improvement in 50 epochs)\n",
      "Training time:  0:04:19.963770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 106: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 245 (no improvement in 50 epochs)\n",
      "Training time:  0:09:35.952420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 107: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 140 (no improvement in 50 epochs)\n",
      "Training time:  0:05:31.351952\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 60, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012000.\n",
      "\n",
      "Testing Configuration 108: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 168 (no improvement in 50 epochs)\n",
      "Training time:  0:06:33.443382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 109: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 144 (no improvement in 50 epochs)\n",
      "Training time:  0:05:40.110850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 110: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 212 (no improvement in 50 epochs)\n",
      "Training time:  0:08:17.521914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 111: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 235 (no improvement in 50 epochs)\n",
      "Training time:  0:09:11.694839\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.013003.\n",
      "\n",
      "Testing Configuration 112: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 171 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.525735\n",
      "\n",
      "Testing Configuration 113: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 184 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.620346\n",
      "\n",
      "Testing Configuration 114: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 227 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.370662\n",
      "\n",
      "Testing Configuration 115: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 237 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.401966\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011503.\n",
      "\n",
      "Testing Configuration 116: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 188 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.433768\n",
      "\n",
      "Testing Configuration 117: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 243 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.992813\n",
      "\n",
      "Testing Configuration 118: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 173 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.793809\n",
      "\n",
      "Testing Configuration 119: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 224 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.325409\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011503.\n",
      "\n",
      "Testing Configuration 120: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 134 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.387344\n",
      "\n",
      "Testing Configuration 121: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 156 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.687069\n",
      "\n",
      "Testing Configuration 122: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 226 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.709119\n",
      "\n",
      "Testing Configuration 123: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 420 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.400320\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011003.\n",
      "\n",
      "Testing Configuration 124: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.064503\n",
      "\n",
      "Testing Configuration 125: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 141 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.856058\n",
      "\n",
      "Testing Configuration 126: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 108 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.430995\n",
      "\n",
      "Testing Configuration 127: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 155 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.091147\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012002.\n",
      "\n",
      "Testing Configuration 128: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.897588\n",
      "\n",
      "Testing Configuration 129: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 192 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.903499\n",
      "\n",
      "Testing Configuration 130: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 249 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.098106\n",
      "\n",
      "Testing Configuration 131: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 192 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.508139\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011503.\n",
      "\n",
      "Testing Configuration 132: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.898950\n",
      "\n",
      "Testing Configuration 133: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 89 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.515428\n",
      "\n",
      "Testing Configuration 134: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 138 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.019574\n",
      "\n",
      "Testing Configuration 135: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 131 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.738787\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 60, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011002.\n",
      "\n",
      "Testing Configuration 136: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 180 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.704606\n",
      "\n",
      "Testing Configuration 137: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.073644\n",
      "\n",
      "Testing Configuration 138: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 127 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.370596\n",
      "\n",
      "Testing Configuration 139: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 185 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.242035\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 60, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.011503.\n",
      "\n",
      "Testing Configuration 140: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 162 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.557233\n",
      "\n",
      "Testing Configuration 141: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 128 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.798492\n",
      "\n",
      "Testing Configuration 142: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 145 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.721710\n",
      "\n",
      "Testing Configuration 143: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 180 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.827015\n",
      "\n",
      "Processing Model: OptResNet1D_JARILWWF with params {'input_channels': 60, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012003.\n",
      "\n",
      "Testing Configuration 144: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 85 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.855819\n",
      "\n",
      "Testing Configuration 145: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.107715\n",
      "\n",
      "Testing Configuration 146: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 74 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.603457\n",
      "\n",
      "Testing Configuration 147: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.031411\n",
      "\n",
      "Processing Model: OptECAResNet1D_JARILWWF with params {'input_channels': 60, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012002.\n",
      "\n",
      "Testing Configuration 148: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.698108\n",
      "\n",
      "Testing Configuration 149: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.111639\n",
      "\n",
      "Testing Configuration 150: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 59 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.187357\n",
      "\n",
      "Testing Configuration 151: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 95 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.922832\n",
      "\n",
      "Processing Model: CustomResNet1D with params {'input_channels': 60}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012503.\n",
      "\n",
      "Testing Configuration 152: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 66 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.886119\n",
      "\n",
      "Testing Configuration 153: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 71 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.949327\n",
      "\n",
      "Testing Configuration 154: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 95 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.693172\n",
      "\n",
      "Testing Configuration 155: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 171 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.482039\n",
      "\n",
      "Processing Model: CustomECAResNet1D with params {'input_channels': 60}\n",
      "Started loading data...\n",
      "Train size: 384, Validation size: 68, Test size: 68\n",
      "Train labels distribution: Counter({10: 21, 0: 20, 2: 20, 3: 20, 15: 20, 16: 20, 17: 20, 18: 20, 1: 19, 4: 19, 5: 19, 7: 19, 8: 19, 9: 19, 12: 19, 13: 19, 14: 19, 6: 18, 11: 17, 19: 17})\n",
      "Validation labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Test labels distribution: Counter({0: 4, 1: 4, 4: 4, 8: 4, 10: 4, 12: 4, 13: 4, 14: 4, 2: 3, 3: 3, 5: 3, 6: 3, 7: 3, 9: 3, 11: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.012002.\n",
      "\n",
      "Testing Configuration 156: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 65 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.381673\n",
      "\n",
      "Testing Configuration 157: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 63 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.555288\n",
      "\n",
      "Testing Configuration 158: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 76 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.190233\n",
      "\n",
      "Testing Configuration 159: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 71 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.811772\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
