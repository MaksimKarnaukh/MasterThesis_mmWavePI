{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T14:33:07.399546Z",
     "start_time": "2025-05-30T14:32:59.061297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "import models\n",
    "import helper_funtions\n",
    "import preprocess\n",
    "import settings\n",
    "\n",
    "importlib.reload(settings)\n",
    "# importlib.reload(models)\n",
    "importlib.reload(helper_funtions)\n",
    "importlib.reload(preprocess)\n",
    "\n",
    "from models.LSTM import LSTM_HumanFi, CNN_LSTM, CNN_BiLSTM_TemporalAttention, CNN_BiLSTM_ChannelAttention, CNN_BiLSTM_DualAttention, CNN_BiLSTM_Attention\n",
    "from models.RadioNet import RadioNet_NeuralWave\n",
    "from models.ResNet import ECAResNet1D, ECABasicBlock1D, ResNet1D_JARILWWF, OptResNet1D_JARILWWF, OptECAResNet1D_JARILWWF, CustomResNet1D, CustomECAResNet1D\n",
    "from models.TemporalConvNet import TemporalConvNet\n",
    "\n",
    "from helper_funtions import grid_search, run_gridsearch_with_seeds\n",
    "from preprocess import DataPreprocessor\n",
    "from settings import folder_path_5ghz_10hz_collected, folder_path_60ghz_collected, folder_path_5ghz_200hz_collected, folder_path_60ghz_external, output_path"
   ],
   "id": "eaf05c930fb61034",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-30T14:33:07.413235Z",
     "start_time": "2025-05-30T14:33:07.407885Z"
    }
   },
   "source": [
    "input_dim = 52\n",
    "num_epochs = 500\n",
    "\n",
    "param_grid = {\n",
    "    'batch_size': [32],\n",
    "    'learning_rate': [0.0007], # 0.001,\n",
    "    'optimizer': ['adam'],\n",
    "    'mixup_alpha': [0.0, 0.4],\n",
    "    'smoothing_prob': [0.0, 0.5],\n",
    "    'model': [],\n",
    "}\n",
    "\n",
    "hidden_dims = [64, 128] # 32, 256\n",
    "num_layers_list = [1, 2]\n",
    "bidirectional_flags = [False, True]\n",
    "dropouts = [0.2, 0.5]\n",
    "\n",
    "for hidden_dim in hidden_dims:\n",
    "    for num_layers in num_layers_list:\n",
    "        for bidirectional in bidirectional_flags:\n",
    "            for dropout in dropouts:\n",
    "                param_grid['model'].append({\n",
    "                    'model_class': LSTM_HumanFi,\n",
    "                    'model_args': {\n",
    "                        'input_dim': input_dim,\n",
    "                        'hidden_dim': hidden_dim,\n",
    "                        'num_layers': num_layers,\n",
    "                        'bidirectional': bidirectional,\n",
    "                        'dropout': dropout\n",
    "                    },\n",
    "                    'num_epochs': num_epochs\n",
    "                })\n",
    "\n",
    "tcn_channel_options = [\n",
    "    # [32, 64],\n",
    "    [64, 128],\n",
    "    [64, 128, 128],\n",
    "    [64, 128, 256],\n",
    "    # [128, 256, 512],\n",
    "]\n",
    "tcn_kernel_sizes = [2, 3] # 5\n",
    "tcn_dropout_rates = [0.2, 0.5]\n",
    "\n",
    "for channels in tcn_channel_options:\n",
    "    for kernel_size in tcn_kernel_sizes:\n",
    "        for dropout in tcn_dropout_rates:\n",
    "            param_grid['model'].append({\n",
    "                'model_class': TemporalConvNet,\n",
    "                'model_args': {\n",
    "                    'num_inputs': input_dim,\n",
    "                    'num_channels': channels,\n",
    "                    'kernel_size': kernel_size,\n",
    "                    'dropout': dropout\n",
    "                },\n",
    "                'num_epochs': num_epochs\n",
    "            })\n",
    "\n",
    "cnn_lstm_hidden_dims = [64, 128] # 256\n",
    "cnn_lstm_layers = [1, 2]\n",
    "\n",
    "for hidden_dim in cnn_lstm_hidden_dims:\n",
    "    for num_layers in cnn_lstm_layers:\n",
    "        param_grid['model'].append({\n",
    "            'model_class': CNN_BiLSTM_Attention,\n",
    "            'model_args': {\n",
    "                'input_dim': input_dim,\n",
    "                'cnn_filters': 64,\n",
    "                'lstm_units': hidden_dim,\n",
    "                'num_layers': num_layers,\n",
    "            },\n",
    "            'num_epochs': num_epochs\n",
    "        })\n",
    "\n",
    "        param_grid['model'].append({\n",
    "            'model_class': CNN_BiLSTM_TemporalAttention,\n",
    "            'model_args': {\n",
    "                'input_dim': input_dim,\n",
    "                'cnn_channels': 64,\n",
    "                'lstm_hidden_dim': hidden_dim,\n",
    "                'lstm_layers': num_layers,\n",
    "            },\n",
    "            'num_epochs': num_epochs\n",
    "        })\n",
    "\n",
    "        # param_grid['model'].append({\n",
    "        #     'model_class': CNN_BiLSTM_ChannelAttention,\n",
    "        #     'model_args': {\n",
    "        #         'input_dim': input_dim,\n",
    "        #         'cnn_channels': 64,\n",
    "        #         'lstm_hidden_dim': hidden_dim,\n",
    "        #         'lstm_layers': num_layers,\n",
    "        #     },\n",
    "        #     'num_epochs': num_epochs\n",
    "        # })\n",
    "        #\n",
    "        # param_grid['model'].append({\n",
    "        #     'model_class': CNN_BiLSTM_DualAttention,\n",
    "        #     'model_args': {\n",
    "        #         'input_dim': input_dim,\n",
    "        #         'cnn_channels': 64,\n",
    "        #         'lstm_hidden_dim': hidden_dim,\n",
    "        #         'lstm_layers': num_layers,\n",
    "        #     },\n",
    "        #     'num_epochs': num_epochs\n",
    "        # })\n",
    "\n",
    "\n",
    "param_grid['model'] += [\n",
    "    # {\n",
    "    #     'model_class': RadioNet_NeuralWave,\n",
    "    #     'model_args': {'input_dim': input_dim},\n",
    "    #     'num_epochs': num_epochs,\n",
    "    #     'data_preprocessor': DataPreprocessor(target_dim=354)\n",
    "    # },\n",
    "    \n",
    "    # the ones commented below are not optimal (proven by early tests)\n",
    "    # {\n",
    "    #     'model_class': ECAResNet1D,\n",
    "    #     'model_args': {'input_channels': input_dim, 'block': ECABasicBlock1D, 'layers': (1, 1, 1, 1)},\n",
    "    #     'num_epochs': num_epochs\n",
    "    # },\n",
    "    # {\n",
    "    #     'model_class': ECAResNet1D,\n",
    "    #     'model_args': {'input_channels': input_dim, 'block': ECABasicBlock1D, 'layers': (2, 2, 2, 2)},\n",
    "    #     'num_epochs': num_epochs\n",
    "    # },\n",
    "    # {\n",
    "    #     'model_class': ResNet1D_JARILWWF,\n",
    "    #     'model_args': {'input_channels': input_dim, 'layers': [1,1,1,1]},\n",
    "    #     'num_epochs': num_epochs,\n",
    "    # },\n",
    "    # {\n",
    "    #     'model_class': ResNet1D_JARILWWF,\n",
    "    #     'model_args': {'input_channels': input_dim, 'layers': [2,2,2,2]},\n",
    "    #     'num_epochs': num_epochs,\n",
    "    # },\n",
    "    \n",
    "    \n",
    "    {\n",
    "        'model_class': OptResNet1D_JARILWWF,\n",
    "        'model_args': {'input_channels': input_dim, 'layers': [1,1,1,1]},\n",
    "        'num_epochs': num_epochs,\n",
    "    },\n",
    "    # {\n",
    "    #     'model_class': OptResNet1D_JARILWWF,\n",
    "    #     'model_args': {'input_channels': input_dim, 'layers': [2,2,2,2]},\n",
    "    #     'num_epochs': num_epochs,\n",
    "    # },\n",
    "    {\n",
    "        'model_class': OptECAResNet1D_JARILWWF,\n",
    "        'model_args': {'input_channels': input_dim, 'layers': [1,1,1,1]},\n",
    "        'num_epochs': num_epochs,\n",
    "    },\n",
    "    # {\n",
    "    #     'model_class': OptECAResNet1D_JARILWWF,\n",
    "    #     'model_args': {'input_channels': input_dim, 'layers': [2,2,2,2]},\n",
    "    #     'num_epochs': num_epochs,\n",
    "    # },\n",
    "    {\n",
    "        'model_class': CustomResNet1D,\n",
    "        'model_args': {'input_channels': input_dim},\n",
    "        'num_epochs': num_epochs,\n",
    "    },\n",
    "    {\n",
    "        'model_class': CustomECAResNet1D,\n",
    "        'model_args': {'input_channels': input_dim},\n",
    "        'num_epochs': num_epochs,\n",
    "    },\n",
    "] "
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T14:19:17.385946Z",
     "start_time": "2025-05-29T17:03:25.028243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gridsearch_results_df = run_gridsearch_with_seeds(\n",
    "    gridsearch_func=grid_search,\n",
    "    gridsearch_args={\n",
    "        'param_grid': param_grid,\n",
    "        'folder_path': folder_path_5ghz_200hz_collected,\n",
    "        'background_subtraction': False,\n",
    "        'seconds_per_sample': 5,\n",
    "        'rows_per_second': 200,\n",
    "    },\n",
    "    n_seeds=3,\n",
    "    output_dir=output_path,\n",
    "    filename=\"gridsearch_5ghz_coll_200hz\"\n",
    ")"
   ],
   "id": "5a5de08b7ecfded3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running seed 42 ---\n",
      "Running 160 configurations...\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.230075.\n",
      "\n",
      "Testing Configuration 0: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 216 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.320964\n",
      "\n",
      "Testing Configuration 1: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 170 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.361414\n",
      "\n",
      "Testing Configuration 2: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 205 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.643028\n",
      "\n",
      "Testing Configuration 3: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 219 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.096468\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.111289.\n",
      "\n",
      "Testing Configuration 4: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.335119\n",
      "\n",
      "Testing Configuration 5: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 124 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.632879\n",
      "\n",
      "Testing Configuration 6: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 285 (no improvement in 50 epochs)\n",
      "Training time:  0:00:27.139881\n",
      "\n",
      "Testing Configuration 7: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 271 (no improvement in 50 epochs)\n",
      "Training time:  0:00:25.968293\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.114655.\n",
      "\n",
      "Testing Configuration 8: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 187 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.330751\n",
      "\n",
      "Testing Configuration 9: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 109 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.548813\n",
      "\n",
      "Testing Configuration 10: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 125 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.280186\n",
      "\n",
      "Testing Configuration 11: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 203 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.375622\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110582.\n",
      "\n",
      "Testing Configuration 12: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 153 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.694621\n",
      "\n",
      "Testing Configuration 13: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 123 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.119613\n",
      "\n",
      "Testing Configuration 14: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 267 (no improvement in 50 epochs)\n",
      "Training time:  0:00:27.378017\n",
      "\n",
      "Testing Configuration 15: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 246 (no improvement in 50 epochs)\n",
      "Training time:  0:00:59.203553\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.278317.\n",
      "\n",
      "Testing Configuration 16: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 91 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.771790\n",
      "\n",
      "Testing Configuration 17: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:00:24.446736\n",
      "\n",
      "Testing Configuration 18: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 185 (no improvement in 50 epochs)\n",
      "Training time:  0:00:46.183015\n",
      "\n",
      "Testing Configuration 19: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:00:38.030431\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.285800.\n",
      "\n",
      "Testing Configuration 20: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 153 (no improvement in 50 epochs)\n",
      "Training time:  0:00:35.967914\n",
      "\n",
      "Testing Configuration 21: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.428115\n",
      "\n",
      "Testing Configuration 22: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 173 (no improvement in 50 epochs)\n",
      "Training time:  0:00:42.817768\n",
      "\n",
      "Testing Configuration 23: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 231 (no improvement in 50 epochs)\n",
      "Training time:  0:00:58.426691\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.286401.\n",
      "\n",
      "Testing Configuration 24: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 80 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.910541\n",
      "\n",
      "Testing Configuration 25: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 104 (no improvement in 50 epochs)\n",
      "Training time:  0:00:27.580904\n",
      "\n",
      "Testing Configuration 26: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 132 (no improvement in 50 epochs)\n",
      "Training time:  0:00:36.394908\n",
      "\n",
      "Testing Configuration 27: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 108 (no improvement in 50 epochs)\n",
      "Training time:  0:00:30.176347\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.277333.\n",
      "\n",
      "Testing Configuration 28: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:00:25.336610\n",
      "\n",
      "Testing Configuration 29: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 89 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.397338\n",
      "\n",
      "Testing Configuration 30: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:00:33.618405\n",
      "\n",
      "Testing Configuration 31: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:00:26.031645\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.289201.\n",
      "\n",
      "Testing Configuration 32: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 130 (no improvement in 50 epochs)\n",
      "Training time:  0:00:32.506572\n",
      "\n",
      "Testing Configuration 33: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 157 (no improvement in 50 epochs)\n",
      "Training time:  0:00:40.412527\n",
      "\n",
      "Testing Configuration 34: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 388 (no improvement in 50 epochs)\n",
      "Training time:  0:01:40.924040\n",
      "\n",
      "Testing Configuration 35: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 188 (no improvement in 50 epochs)\n",
      "Training time:  0:00:50.230063\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.292626.\n",
      "\n",
      "Testing Configuration 36: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 206 (no improvement in 50 epochs)\n",
      "Training time:  0:00:51.764440\n",
      "\n",
      "Testing Configuration 37: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:00:25.978936\n",
      "\n",
      "Testing Configuration 38: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 125 (no improvement in 50 epochs)\n",
      "Training time:  0:00:33.114422\n",
      "\n",
      "Testing Configuration 39: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 162 (no improvement in 50 epochs)\n",
      "Training time:  0:00:38.255398\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.111780.\n",
      "\n",
      "Testing Configuration 40: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 71 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.948152\n",
      "\n",
      "Testing Configuration 41: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 71 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.130189\n",
      "\n",
      "Testing Configuration 42: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 201 (no improvement in 50 epochs)\n",
      "Training time:  0:00:35.547237\n",
      "\n",
      "Testing Configuration 43: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 236 (no improvement in 50 epochs)\n",
      "Training time:  0:01:09.383484\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.282382.\n",
      "\n",
      "Testing Configuration 44: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:00:42.696994\n",
      "\n",
      "Testing Configuration 45: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 125 (no improvement in 50 epochs)\n",
      "Training time:  0:00:35.921571\n",
      "\n",
      "Testing Configuration 46: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 194 (no improvement in 50 epochs)\n",
      "Training time:  0:00:57.045230\n",
      "\n",
      "Testing Configuration 47: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 209 (no improvement in 50 epochs)\n",
      "Training time:  0:01:02.402807\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.299656.\n",
      "\n",
      "Testing Configuration 48: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 106 (no improvement in 50 epochs)\n",
      "Training time:  0:00:30.803546\n",
      "\n",
      "Testing Configuration 49: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:00:30.432289\n",
      "\n",
      "Testing Configuration 50: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 245 (no improvement in 50 epochs)\n",
      "Training time:  0:01:13.969871\n",
      "\n",
      "Testing Configuration 51: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:00:26.364854\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.273187.\n",
      "\n",
      "Testing Configuration 52: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.824929\n",
      "\n",
      "Testing Configuration 53: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 84 (no improvement in 50 epochs)\n",
      "Training time:  0:00:24.174469\n",
      "\n",
      "Testing Configuration 54: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 228 (no improvement in 50 epochs)\n",
      "Training time:  0:01:07.797856\n",
      "\n",
      "Testing Configuration 55: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:00:46.165326\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.265859.\n",
      "\n",
      "Testing Configuration 56: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 65 (no improvement in 50 epochs)\n",
      "Training time:  0:00:27.152253\n",
      "\n",
      "Testing Configuration 57: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 67 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.084457\n",
      "\n",
      "Testing Configuration 58: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 146 (no improvement in 50 epochs)\n",
      "Training time:  0:01:02.261892\n",
      "\n",
      "Testing Configuration 59: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 118 (no improvement in 50 epochs)\n",
      "Training time:  0:00:50.082380\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.280836.\n",
      "\n",
      "Testing Configuration 60: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 69 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.113308\n",
      "\n",
      "Testing Configuration 61: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 68 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.368598\n",
      "\n",
      "Testing Configuration 62: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 90 (no improvement in 50 epochs)\n",
      "Training time:  0:00:38.134261\n",
      "\n",
      "Testing Configuration 63: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 74 (no improvement in 50 epochs)\n",
      "Training time:  0:00:31.516574\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.269697.\n",
      "\n",
      "Testing Configuration 64: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 181 (no improvement in 50 epochs)\n",
      "Training time:  0:08:21.254241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 65: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 109 (no improvement in 50 epochs)\n",
      "Training time:  0:06:52.293251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 66: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 229 (no improvement in 50 epochs)\n",
      "Training time:  0:14:05.131977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 67: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 329 (no improvement in 50 epochs)\n",
      "Training time:  0:07:46.837199\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.112166.\n",
      "\n",
      "Testing Configuration 68: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 175 (no improvement in 50 epochs)\n",
      "Training time:  0:02:24.355779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 69: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 144 (no improvement in 50 epochs)\n",
      "Training time:  0:02:01.208846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 70: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 269 (no improvement in 50 epochs)\n",
      "Training time:  0:03:46.335532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 71: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 349 (no improvement in 50 epochs)\n",
      "Training time:  0:04:58.049993\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.117281.\n",
      "\n",
      "Testing Configuration 72: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 107 (no improvement in 50 epochs)\n",
      "Training time:  0:01:33.645419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 73: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 112 (no improvement in 50 epochs)\n",
      "Training time:  0:01:34.459449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 74: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 212 (no improvement in 50 epochs)\n",
      "Training time:  0:03:01.783541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 75: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 302 (no improvement in 50 epochs)\n",
      "Training time:  0:04:27.769116\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.138308.\n",
      "\n",
      "Testing Configuration 76: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 138 (no improvement in 50 epochs)\n",
      "Training time:  0:01:58.303908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 77: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 104 (no improvement in 50 epochs)\n",
      "Training time:  0:01:30.962194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 78: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 260 (no improvement in 50 epochs)\n",
      "Training time:  0:03:44.510306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 79: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 350 (no improvement in 50 epochs)\n",
      "Training time:  0:04:57.881971\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.120251.\n",
      "\n",
      "Testing Configuration 80: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 128 (no improvement in 50 epochs)\n",
      "Training time:  0:04:49.552634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 81: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 144 (no improvement in 50 epochs)\n",
      "Training time:  0:17:06.168355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 82: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 309 (no improvement in 50 epochs)\n",
      "Training time:  0:36:26.213721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 83: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 272 (no improvement in 50 epochs)\n",
      "Training time:  0:31:35.135604\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.308844.\n",
      "\n",
      "Testing Configuration 84: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 172 (no improvement in 50 epochs)\n",
      "Training time:  0:20:47.989756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 85: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 139 (no improvement in 50 epochs)\n",
      "Training time:  0:15:58.293140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 86: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 390 (no improvement in 50 epochs)\n",
      "Training time:  0:42:56.068244\n",
      "\n",
      "Testing Configuration 87: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 266 (no improvement in 50 epochs)\n",
      "Training time:  0:31:56.519516\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.287553.\n",
      "\n",
      "Testing Configuration 88: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 82 (no improvement in 50 epochs)\n",
      "Training time:  0:10:10.296006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 89: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 158 (no improvement in 50 epochs)\n",
      "Training time:  0:19:25.200990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 90: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 236 (no improvement in 50 epochs)\n",
      "Training time:  0:28:55.330598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 91: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 282 (no improvement in 50 epochs)\n",
      "Training time:  0:33:11.084790\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.292497.\n",
      "\n",
      "Testing Configuration 92: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 124 (no improvement in 50 epochs)\n",
      "Training time:  0:15:15.988990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 93: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:11:57.919359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 94: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 334 (no improvement in 50 epochs)\n",
      "Training time:  0:40:04.895536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 95: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 320 (no improvement in 50 epochs)\n",
      "Training time:  0:36:34.172217\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.289811.\n",
      "\n",
      "Testing Configuration 96: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 106 (no improvement in 50 epochs)\n",
      "Training time:  0:18:17.044824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 97: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 197 (no improvement in 50 epochs)\n",
      "Training time:  0:35:33.459167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 98: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 331 (no improvement in 50 epochs)\n",
      "Training time:  0:30:23.164138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 99: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 189 (no improvement in 50 epochs)\n",
      "Training time:  0:06:50.734746\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.108537.\n",
      "\n",
      "Testing Configuration 100: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 119 (no improvement in 50 epochs)\n",
      "Training time:  0:04:18.288830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 101: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 91 (no improvement in 50 epochs)\n",
      "Training time:  0:03:16.868775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 102: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 208 (no improvement in 50 epochs)\n",
      "Training time:  0:07:31.894858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 103: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 219 (no improvement in 50 epochs)\n",
      "Training time:  0:07:57.412566\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110301.\n",
      "\n",
      "Testing Configuration 104: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:04:55.416855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 105: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 85 (no improvement in 50 epochs)\n",
      "Training time:  0:03:04.481644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 106: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 329 (no improvement in 50 epochs)\n",
      "Training time:  0:11:57.998423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 107: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 228 (no improvement in 50 epochs)\n",
      "Training time:  0:08:18.195294\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.109282.\n",
      "\n",
      "Testing Configuration 108: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 95 (no improvement in 50 epochs)\n",
      "Training time:  0:03:26.582227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 109: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:05:27.022770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 110: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 205 (no improvement in 50 epochs)\n",
      "Training time:  0:07:28.851312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 111: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:05:29.937106\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.106923.\n",
      "\n",
      "Testing Configuration 112: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 231 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.678438\n",
      "\n",
      "Testing Configuration 113: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 183 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.183169\n",
      "\n",
      "Testing Configuration 114: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 192 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.510723\n",
      "\n",
      "Testing Configuration 115: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 284 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.939006\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.108949.\n",
      "\n",
      "Testing Configuration 116: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 145 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.363976\n",
      "\n",
      "Testing Configuration 117: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.938359\n",
      "\n",
      "Testing Configuration 118: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 287 (no improvement in 50 epochs)\n",
      "Training time:  0:00:27.077703\n",
      "\n",
      "Testing Configuration 119: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 178 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.759337\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.111000.\n",
      "\n",
      "Testing Configuration 120: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.437023\n",
      "\n",
      "Testing Configuration 121: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.516773\n",
      "\n",
      "Testing Configuration 122: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 382 (no improvement in 50 epochs)\n",
      "Training time:  0:00:49.826513\n",
      "\n",
      "Testing Configuration 123: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.867329\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.108786.\n",
      "\n",
      "Testing Configuration 124: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 123 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.802411\n",
      "\n",
      "Testing Configuration 125: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 155 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.699875\n",
      "\n",
      "Testing Configuration 126: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 140 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.151949\n",
      "\n",
      "Testing Configuration 127: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 171 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.116887\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.107508.\n",
      "\n",
      "Testing Configuration 128: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 185 (no improvement in 50 epochs)\n",
      "Training time:  0:00:34.702441\n",
      "\n",
      "Testing Configuration 129: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 144 (no improvement in 50 epochs)\n",
      "Training time:  0:00:27.282256\n",
      "\n",
      "Testing Configuration 130: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 484 (no improvement in 50 epochs)\n",
      "Training time:  0:01:32.666640\n",
      "\n",
      "Testing Configuration 131: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 472 (no improvement in 50 epochs)\n",
      "Training time:  0:01:30.393282\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.108613.\n",
      "\n",
      "Testing Configuration 132: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:00:25.697376\n",
      "\n",
      "Testing Configuration 133: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:00:26.204490\n",
      "\n",
      "Testing Configuration 134: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 146 (no improvement in 50 epochs)\n",
      "Training time:  0:00:25.578347\n",
      "\n",
      "Testing Configuration 135: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 169 (no improvement in 50 epochs)\n",
      "Training time:  0:00:29.792507\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110706.\n",
      "\n",
      "Testing Configuration 136: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 125 (no improvement in 50 epochs)\n",
      "Training time:  0:00:38.084855\n",
      "\n",
      "Testing Configuration 137: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:00:40.502765\n",
      "\n",
      "Testing Configuration 138: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:00:35.041607\n",
      "\n",
      "Testing Configuration 139: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 318 (no improvement in 50 epochs)\n",
      "Training time:  0:01:38.262686\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.107686.\n",
      "\n",
      "Testing Configuration 140: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.761903\n",
      "\n",
      "Testing Configuration 141: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 182 (no improvement in 50 epochs)\n",
      "Training time:  0:00:52.479039\n",
      "\n",
      "Testing Configuration 142: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 125 (no improvement in 50 epochs)\n",
      "Training time:  0:00:36.558845\n",
      "\n",
      "Testing Configuration 143: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 110 (no improvement in 50 epochs)\n",
      "Training time:  0:00:32.483881\n",
      "\n",
      "Processing Model: OptResNet1D_JARILWWF with params {'input_channels': 52, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.112131.\n",
      "\n",
      "Testing Configuration 144: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 101 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.386067\n",
      "\n",
      "Testing Configuration 145: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 105 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.046861\n",
      "\n",
      "Testing Configuration 146: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 181 (no improvement in 50 epochs)\n",
      "Training time:  0:00:26.213955\n",
      "\n",
      "Testing Configuration 147: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 121 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.750501\n",
      "\n",
      "Processing Model: OptECAResNet1D_JARILWWF with params {'input_channels': 52, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.107508.\n",
      "\n",
      "Testing Configuration 148: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 120 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.219651\n",
      "\n",
      "Testing Configuration 149: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.408266\n",
      "\n",
      "Testing Configuration 150: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.421233\n",
      "\n",
      "Testing Configuration 151: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 167 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.961919\n",
      "\n",
      "Processing Model: CustomResNet1D with params {'input_channels': 52}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.106874.\n",
      "\n",
      "Testing Configuration 152: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.982640\n",
      "\n",
      "Testing Configuration 153: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 158 (no improvement in 50 epochs)\n",
      "Training time:  0:00:31.527568\n",
      "\n",
      "Testing Configuration 154: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 250 (no improvement in 50 epochs)\n",
      "Training time:  0:00:50.014203\n",
      "\n",
      "Testing Configuration 155: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 125 (no improvement in 50 epochs)\n",
      "Training time:  0:00:25.299057\n",
      "\n",
      "Processing Model: CustomECAResNet1D with params {'input_channels': 52}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.108214.\n",
      "\n",
      "Testing Configuration 156: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.629453\n",
      "\n",
      "Testing Configuration 157: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 190 (no improvement in 50 epochs)\n",
      "Training time:  0:00:41.495684\n",
      "\n",
      "Testing Configuration 158: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 154 (no improvement in 50 epochs)\n",
      "Training time:  0:00:33.882094\n",
      "\n",
      "Testing Configuration 159: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.328522\n",
      "\n",
      "--- Running seed 420 ---\n",
      "Running 160 configurations...\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110000.\n",
      "\n",
      "Testing Configuration 0: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 249 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.724217\n",
      "\n",
      "Testing Configuration 1: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.217746\n",
      "\n",
      "Testing Configuration 2: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 196 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.883349\n",
      "\n",
      "Testing Configuration 3: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 213 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.161822\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.106500.\n",
      "\n",
      "Testing Configuration 4: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 165 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.600760\n",
      "\n",
      "Testing Configuration 5: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 119 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.238147\n",
      "\n",
      "Testing Configuration 6: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 332 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.396485\n",
      "\n",
      "Testing Configuration 7: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 310 (no improvement in 50 epochs)\n",
      "Training time:  0:00:27.120289\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110996.\n",
      "\n",
      "Testing Configuration 8: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.422125\n",
      "\n",
      "Testing Configuration 9: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 96 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.300648\n",
      "\n",
      "Testing Configuration 10: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 159 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.960940\n",
      "\n",
      "Testing Configuration 11: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 135 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.000114\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.109581.\n",
      "\n",
      "Testing Configuration 12: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 199 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.904047\n",
      "\n",
      "Testing Configuration 13: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 201 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.386616\n",
      "\n",
      "Testing Configuration 14: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 146 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.148530\n",
      "\n",
      "Testing Configuration 15: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 157 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.150275\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.107953.\n",
      "\n",
      "Testing Configuration 16: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 113 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.432461\n",
      "\n",
      "Testing Configuration 17: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 79 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.434979\n",
      "\n",
      "Testing Configuration 18: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.680503\n",
      "\n",
      "Testing Configuration 19: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 135 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.008695\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110000.\n",
      "\n",
      "Testing Configuration 20: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.220458\n",
      "\n",
      "Testing Configuration 21: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 161 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.167700\n",
      "\n",
      "Testing Configuration 22: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 109 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.384279\n",
      "\n",
      "Testing Configuration 23: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.029308\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.111500.\n",
      "\n",
      "Testing Configuration 24: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 120 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.829833\n",
      "\n",
      "Testing Configuration 25: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 104 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.064233\n",
      "\n",
      "Testing Configuration 26: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 203 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.752304\n",
      "\n",
      "Testing Configuration 27: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 128 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.130704\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.113451.\n",
      "\n",
      "Testing Configuration 28: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 99 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.380697\n",
      "\n",
      "Testing Configuration 29: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 78 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.934458\n",
      "\n",
      "Testing Configuration 30: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 118 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.866440\n",
      "\n",
      "Testing Configuration 31: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 95 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.336309\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.107395.\n",
      "\n",
      "Testing Configuration 32: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 117 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.810548\n",
      "\n",
      "Testing Configuration 33: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.254460\n",
      "\n",
      "Testing Configuration 34: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 156 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.563516\n",
      "\n",
      "Testing Configuration 35: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 110 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.478011\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.114000.\n",
      "\n",
      "Testing Configuration 36: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 158 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.619254\n",
      "\n",
      "Testing Configuration 37: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 152 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.896460\n",
      "\n",
      "Testing Configuration 38: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 252 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.362592\n",
      "\n",
      "Testing Configuration 39: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 253 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.632367\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.109301.\n",
      "\n",
      "Testing Configuration 40: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.969558\n",
      "\n",
      "Testing Configuration 41: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 73 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.156582\n",
      "\n",
      "Testing Configuration 42: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 199 (no improvement in 50 epochs)\n",
      "Training time:  0:00:33.069355\n",
      "\n",
      "Testing Configuration 43: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.919830\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.108621.\n",
      "\n",
      "Testing Configuration 44: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 209 (no improvement in 50 epochs)\n",
      "Training time:  0:00:34.048835\n",
      "\n",
      "Testing Configuration 45: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.515046\n",
      "\n",
      "Testing Configuration 46: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:00:25.230490\n",
      "\n",
      "Testing Configuration 47: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 108 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.297371\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110862.\n",
      "\n",
      "Testing Configuration 48: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 68 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.692796\n",
      "\n",
      "Testing Configuration 49: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 106 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.411098\n",
      "\n",
      "Testing Configuration 50: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.586702\n",
      "\n",
      "Testing Configuration 51: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 218 (no improvement in 50 epochs)\n",
      "Training time:  0:00:38.505049\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110999.\n",
      "\n",
      "Testing Configuration 52: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 87 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.906118\n",
      "\n",
      "Testing Configuration 53: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.202248\n",
      "\n",
      "Testing Configuration 54: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.798023\n",
      "\n",
      "Testing Configuration 55: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 138 (no improvement in 50 epochs)\n",
      "Training time:  0:00:24.286729\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.109139.\n",
      "\n",
      "Testing Configuration 56: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 78 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.914317\n",
      "\n",
      "Testing Configuration 57: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 64 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.218228\n",
      "\n",
      "Testing Configuration 58: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 87 (no improvement in 50 epochs)\n",
      "Training time:  0:00:25.038223\n",
      "\n",
      "Testing Configuration 59: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 93 (no improvement in 50 epochs)\n",
      "Training time:  0:00:27.033000\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.113431.\n",
      "\n",
      "Testing Configuration 60: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 71 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.992539\n",
      "\n",
      "Testing Configuration 61: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:24.474186\n",
      "\n",
      "Testing Configuration 62: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 105 (no improvement in 50 epochs)\n",
      "Training time:  0:00:29.953213\n",
      "\n",
      "Testing Configuration 63: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 106 (no improvement in 50 epochs)\n",
      "Training time:  0:00:30.473946\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110224.\n",
      "\n",
      "Testing Configuration 64: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 139 (no improvement in 50 epochs)\n",
      "Training time:  0:01:50.690745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 65: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 113 (no improvement in 50 epochs)\n",
      "Training time:  0:01:29.897429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 66: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 396 (no improvement in 50 epochs)\n",
      "Training time:  0:05:16.069678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 67: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 370 (no improvement in 50 epochs)\n",
      "Training time:  0:04:56.260162\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110001.\n",
      "\n",
      "Testing Configuration 68: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 137 (no improvement in 50 epochs)\n",
      "Training time:  0:01:48.372358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 69: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:02:00.397248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 70: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 291 (no improvement in 50 epochs)\n",
      "Training time:  0:03:52.905856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 71: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 255 (no improvement in 50 epochs)\n",
      "Training time:  0:03:24.337553\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.108683.\n",
      "\n",
      "Testing Configuration 72: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:01:36.850203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 73: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 145 (no improvement in 50 epochs)\n",
      "Training time:  0:01:55.053073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 74: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 212 (no improvement in 50 epochs)\n",
      "Training time:  0:02:48.996030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 75: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 292 (no improvement in 50 epochs)\n",
      "Training time:  0:03:52.811018\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.107965.\n",
      "\n",
      "Testing Configuration 76: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 127 (no improvement in 50 epochs)\n",
      "Training time:  0:01:40.695058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 77: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 125 (no improvement in 50 epochs)\n",
      "Training time:  0:01:39.799874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 78: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 305 (no improvement in 50 epochs)\n",
      "Training time:  0:04:03.268696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 79: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 279 (no improvement in 50 epochs)\n",
      "Training time:  0:03:41.963495\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.112258.\n",
      "\n",
      "Testing Configuration 80: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:03:09.425601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 81: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 128 (no improvement in 50 epochs)\n",
      "Training time:  0:03:12.973645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 82: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 261 (no improvement in 50 epochs)\n",
      "Training time:  0:06:32.763041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 83: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 219 (no improvement in 50 epochs)\n",
      "Training time:  0:05:29.780705\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.106160.\n",
      "\n",
      "Testing Configuration 84: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 115 (no improvement in 50 epochs)\n",
      "Training time:  0:02:52.862199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 85: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 251 (no improvement in 50 epochs)\n",
      "Training time:  0:06:18.737169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 86: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 253 (no improvement in 50 epochs)\n",
      "Training time:  0:06:23.184767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 87: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 172 (no improvement in 50 epochs)\n",
      "Training time:  0:04:21.079540\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.161502.\n",
      "\n",
      "Testing Configuration 88: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:02:19.194722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 89: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:03:46.302105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 90: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 205 (no improvement in 50 epochs)\n",
      "Training time:  0:05:09.582298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 91: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 253 (no improvement in 50 epochs)\n",
      "Training time:  0:06:21.511657\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.108057.\n",
      "\n",
      "Testing Configuration 92: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 129 (no improvement in 50 epochs)\n",
      "Training time:  0:03:14.246876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 93: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 99 (no improvement in 50 epochs)\n",
      "Training time:  0:02:29.633776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 94: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 220 (no improvement in 50 epochs)\n",
      "Training time:  0:05:32.271266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 95: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 221 (no improvement in 50 epochs)\n",
      "Training time:  0:05:34.637323\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110585.\n",
      "\n",
      "Testing Configuration 96: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 117 (no improvement in 50 epochs)\n",
      "Training time:  0:04:14.322567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 97: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 164 (no improvement in 50 epochs)\n",
      "Training time:  0:05:56.287297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 98: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 254 (no improvement in 50 epochs)\n",
      "Training time:  0:09:12.002013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 99: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 197 (no improvement in 50 epochs)\n",
      "Training time:  0:07:10.382287\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.105563.\n",
      "\n",
      "Testing Configuration 100: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 113 (no improvement in 50 epochs)\n",
      "Training time:  0:04:05.479696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 101: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 129 (no improvement in 50 epochs)\n",
      "Training time:  0:04:39.546333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 102: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 380 (no improvement in 50 epochs)\n",
      "Training time:  0:13:44.307904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 103: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 262 (no improvement in 50 epochs)\n",
      "Training time:  0:09:30.554075\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110000.\n",
      "\n",
      "Testing Configuration 104: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 129 (no improvement in 50 epochs)\n",
      "Training time:  0:04:40.247315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 105: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 118 (no improvement in 50 epochs)\n",
      "Training time:  0:04:15.156959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 106: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 246 (no improvement in 50 epochs)\n",
      "Training time:  0:08:55.644743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 107: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 276 (no improvement in 50 epochs)\n",
      "Training time:  0:10:01.417496\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.107579.\n",
      "\n",
      "Testing Configuration 108: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 148 (no improvement in 50 epochs)\n",
      "Training time:  0:05:21.446276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 109: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 87 (no improvement in 50 epochs)\n",
      "Training time:  0:03:11.130030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 110: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 242 (no improvement in 50 epochs)\n",
      "Training time:  0:08:47.811956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 111: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 258 (no improvement in 50 epochs)\n",
      "Training time:  0:09:24.997828\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.174000.\n",
      "\n",
      "Testing Configuration 112: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 219 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.624854\n",
      "\n",
      "Testing Configuration 113: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 237 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.707155\n",
      "\n",
      "Testing Configuration 114: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Training time:  0:00:50.606227\n",
      "\n",
      "Testing Configuration 115: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 474 (no improvement in 50 epochs)\n",
      "Training time:  0:00:48.461486\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.109713.\n",
      "\n",
      "Testing Configuration 116: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 202 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.342728\n",
      "\n",
      "Testing Configuration 117: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 97 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.812089\n",
      "\n",
      "Testing Configuration 118: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 159 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.030559\n",
      "\n",
      "Testing Configuration 119: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 207 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.663422\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110000.\n",
      "\n",
      "Testing Configuration 120: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 113 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.259691\n",
      "\n",
      "Testing Configuration 121: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 283 (no improvement in 50 epochs)\n",
      "Training time:  0:00:36.163599\n",
      "\n",
      "Testing Configuration 122: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 146 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.168293\n",
      "\n",
      "Testing Configuration 123: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 128 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.095974\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110225.\n",
      "\n",
      "Testing Configuration 124: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 161 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.608680\n",
      "\n",
      "Testing Configuration 125: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 173 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.900038\n",
      "\n",
      "Testing Configuration 126: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 375 (no improvement in 50 epochs)\n",
      "Training time:  0:00:45.787239\n",
      "\n",
      "Testing Configuration 127: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 117 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.387821\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110023.\n",
      "\n",
      "Testing Configuration 128: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 123 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.113628\n",
      "\n",
      "Testing Configuration 129: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:25.670630\n",
      "\n",
      "Testing Configuration 130: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.692065\n",
      "\n",
      "Testing Configuration 131: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 233 (no improvement in 50 epochs)\n",
      "Training time:  0:00:44.834844\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.109635.\n",
      "\n",
      "Testing Configuration 132: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 128 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.063445\n",
      "\n",
      "Testing Configuration 133: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.115097\n",
      "\n",
      "Testing Configuration 134: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 323 (no improvement in 50 epochs)\n",
      "Training time:  0:00:56.565800\n",
      "\n",
      "Testing Configuration 135: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 107 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.892325\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.107480.\n",
      "\n",
      "Testing Configuration 136: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 154 (no improvement in 50 epochs)\n",
      "Training time:  0:00:46.705934\n",
      "\n",
      "Testing Configuration 137: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:00:37.393920\n",
      "\n",
      "Testing Configuration 138: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 386 (no improvement in 50 epochs)\n",
      "Training time:  0:01:58.432764\n",
      "\n",
      "Testing Configuration 139: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 328 (no improvement in 50 epochs)\n",
      "Training time:  0:01:41.099021\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.116166.\n",
      "\n",
      "Testing Configuration 140: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 90 (no improvement in 50 epochs)\n",
      "Training time:  0:00:25.849282\n",
      "\n",
      "Testing Configuration 141: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 112 (no improvement in 50 epochs)\n",
      "Training time:  0:00:32.329970\n",
      "\n",
      "Testing Configuration 142: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 211 (no improvement in 50 epochs)\n",
      "Training time:  0:01:01.653286\n",
      "\n",
      "Testing Configuration 143: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 178 (no improvement in 50 epochs)\n",
      "Training time:  0:00:52.208956\n",
      "\n",
      "Processing Model: OptResNet1D_JARILWWF with params {'input_channels': 52, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.108121.\n",
      "\n",
      "Testing Configuration 144: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 174 (no improvement in 50 epochs)\n",
      "Training time:  0:00:24.700486\n",
      "\n",
      "Testing Configuration 145: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 156 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.361226\n",
      "\n",
      "Testing Configuration 146: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.799100\n",
      "\n",
      "Testing Configuration 147: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 143 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.884223\n",
      "\n",
      "Processing Model: OptECAResNet1D_JARILWWF with params {'input_channels': 52, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.108181.\n",
      "\n",
      "Testing Configuration 148: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 113 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.330841\n",
      "\n",
      "Testing Configuration 149: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 223 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.517693\n",
      "\n",
      "Testing Configuration 150: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 215 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.144074\n",
      "\n",
      "Testing Configuration 151: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 154 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.184563\n",
      "\n",
      "Processing Model: CustomResNet1D with params {'input_channels': 52}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110118.\n",
      "\n",
      "Testing Configuration 152: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.290674\n",
      "\n",
      "Testing Configuration 153: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 113 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.475542\n",
      "\n",
      "Testing Configuration 154: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 188 (no improvement in 50 epochs)\n",
      "Training time:  0:00:37.519965\n",
      "\n",
      "Testing Configuration 155: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 205 (no improvement in 50 epochs)\n",
      "Training time:  0:00:41.523622\n",
      "\n",
      "Processing Model: CustomECAResNet1D with params {'input_channels': 52}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.111893.\n",
      "\n",
      "Testing Configuration 156: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 107 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.224620\n",
      "\n",
      "Testing Configuration 157: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 123 (no improvement in 50 epochs)\n",
      "Training time:  0:00:26.853109\n",
      "\n",
      "Testing Configuration 158: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 157 (no improvement in 50 epochs)\n",
      "Training time:  0:00:34.453639\n",
      "\n",
      "Testing Configuration 159: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:00:27.000562\n",
      "\n",
      "--- Running seed 101010 ---\n",
      "Running 160 configurations...\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.107017.\n",
      "\n",
      "Testing Configuration 0: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.759093\n",
      "\n",
      "Testing Configuration 1: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 165 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.008893\n",
      "\n",
      "Testing Configuration 2: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 164 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.428937\n",
      "\n",
      "Testing Configuration 3: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 272 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.777842\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.108370.\n",
      "\n",
      "Testing Configuration 4: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 228 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.224724\n",
      "\n",
      "Testing Configuration 5: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 316 (no improvement in 50 epochs)\n",
      "Training time:  0:00:26.940602\n",
      "\n",
      "Testing Configuration 6: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 211 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.995837\n",
      "\n",
      "Testing Configuration 7: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 389 (no improvement in 50 epochs)\n",
      "Training time:  0:00:34.209851\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110403.\n",
      "\n",
      "Testing Configuration 8: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 97 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.612925\n",
      "\n",
      "Testing Configuration 9: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 158 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.915473\n",
      "\n",
      "Testing Configuration 10: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 139 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.118665\n",
      "\n",
      "Testing Configuration 11: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 119 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.658084\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.109440.\n",
      "\n",
      "Testing Configuration 12: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 142 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.246601\n",
      "\n",
      "Testing Configuration 13: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 184 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.925231\n",
      "\n",
      "Testing Configuration 14: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 190 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.806744\n",
      "\n",
      "Testing Configuration 15: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 250 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.127743\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.107517.\n",
      "\n",
      "Testing Configuration 16: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 115 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.306361\n",
      "\n",
      "Testing Configuration 17: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 121 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.178848\n",
      "\n",
      "Testing Configuration 18: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 155 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.676991\n",
      "\n",
      "Testing Configuration 19: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 183 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.471270\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.108716.\n",
      "\n",
      "Testing Configuration 20: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 115 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.539207\n",
      "\n",
      "Testing Configuration 21: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 226 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.863360\n",
      "\n",
      "Testing Configuration 22: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 183 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.175566\n",
      "\n",
      "Testing Configuration 23: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 260 (no improvement in 50 epochs)\n",
      "Training time:  0:00:24.899355\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110560.\n",
      "\n",
      "Testing Configuration 24: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 79 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.152366\n",
      "\n",
      "Testing Configuration 25: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 73 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.480076\n",
      "\n",
      "Testing Configuration 26: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 78 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.337477\n",
      "\n",
      "Testing Configuration 27: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 129 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.515531\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.107517.\n",
      "\n",
      "Testing Configuration 28: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 84 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.773261\n",
      "\n",
      "Testing Configuration 29: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 101 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.880515\n",
      "\n",
      "Testing Configuration 30: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 221 (no improvement in 50 epochs)\n",
      "Training time:  0:00:26.350773\n",
      "\n",
      "Testing Configuration 31: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 125 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.940992\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.108250.\n",
      "\n",
      "Testing Configuration 32: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.519124\n",
      "\n",
      "Testing Configuration 33: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 185 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.585750\n",
      "\n",
      "Testing Configuration 34: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 262 (no improvement in 50 epochs)\n",
      "Training time:  0:00:29.312987\n",
      "\n",
      "Testing Configuration 35: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 171 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.389645\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.108666.\n",
      "\n",
      "Testing Configuration 36: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 140 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.278577\n",
      "\n",
      "Testing Configuration 37: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 204 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.459644\n",
      "\n",
      "Testing Configuration 38: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 146 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.451847\n",
      "\n",
      "Testing Configuration 39: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 324 (no improvement in 50 epochs)\n",
      "Training time:  0:00:36.838147\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.107539.\n",
      "\n",
      "Testing Configuration 40: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 74 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.134674\n",
      "\n",
      "Testing Configuration 41: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 170 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.068257\n",
      "\n",
      "Testing Configuration 42: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 181 (no improvement in 50 epochs)\n",
      "Training time:  0:00:30.103328\n",
      "\n",
      "Testing Configuration 43: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 69 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.575275\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.108575.\n",
      "\n",
      "Testing Configuration 44: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 125 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.468511\n",
      "\n",
      "Testing Configuration 45: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.181059\n",
      "\n",
      "Testing Configuration 46: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 185 (no improvement in 50 epochs)\n",
      "Training time:  0:00:30.861439\n",
      "\n",
      "Testing Configuration 47: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 147 (no improvement in 50 epochs)\n",
      "Training time:  0:00:24.736277\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.113014.\n",
      "\n",
      "Testing Configuration 48: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 68 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.633032\n",
      "\n",
      "Testing Configuration 49: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.866921\n",
      "\n",
      "Testing Configuration 50: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 210 (no improvement in 50 epochs)\n",
      "Training time:  0:00:36.607841\n",
      "\n",
      "Testing Configuration 51: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.116817\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.106191.\n",
      "\n",
      "Testing Configuration 52: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 90 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.384180\n",
      "\n",
      "Testing Configuration 53: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 109 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.810678\n",
      "\n",
      "Testing Configuration 54: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.007797\n",
      "\n",
      "Testing Configuration 55: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 154 (no improvement in 50 epochs)\n",
      "Training time:  0:00:27.261538\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.111076.\n",
      "\n",
      "Testing Configuration 56: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 62 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.506304\n",
      "\n",
      "Testing Configuration 57: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 62 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.667677\n",
      "\n",
      "Testing Configuration 58: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:00:25.224317\n",
      "\n",
      "Testing Configuration 59: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 110 (no improvement in 50 epochs)\n",
      "Training time:  0:00:31.658254\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.114092.\n",
      "\n",
      "Testing Configuration 60: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 69 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.502691\n",
      "\n",
      "Testing Configuration 61: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 70 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.858328\n",
      "\n",
      "Testing Configuration 62: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 123 (no improvement in 50 epochs)\n",
      "Training time:  0:00:35.119081\n",
      "\n",
      "Testing Configuration 63: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 197 (no improvement in 50 epochs)\n",
      "Training time:  0:00:56.700992\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.107520.\n",
      "\n",
      "Testing Configuration 64: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 105 (no improvement in 50 epochs)\n",
      "Training time:  0:01:22.859563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 65: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 164 (no improvement in 50 epochs)\n",
      "Training time:  0:02:10.627138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 66: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 244 (no improvement in 50 epochs)\n",
      "Training time:  0:03:16.000091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 67: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 313 (no improvement in 50 epochs)\n",
      "Training time:  0:04:11.496895\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.107510.\n",
      "\n",
      "Testing Configuration 68: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:01:19.932874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 69: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 187 (no improvement in 50 epochs)\n",
      "Training time:  0:02:29.020514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 70: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 235 (no improvement in 50 epochs)\n",
      "Training time:  0:03:08.380138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 71: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 197 (no improvement in 50 epochs)\n",
      "Training time:  0:02:37.854542\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110318.\n",
      "\n",
      "Testing Configuration 72: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:01:28.087956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 73: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 144 (no improvement in 50 epochs)\n",
      "Training time:  0:01:54.779937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 74: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 322 (no improvement in 50 epochs)\n",
      "Training time:  0:04:15.943978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 75: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 314 (no improvement in 50 epochs)\n",
      "Training time:  0:04:10.898825\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.109007.\n",
      "\n",
      "Testing Configuration 76: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 115 (no improvement in 50 epochs)\n",
      "Training time:  0:01:31.248853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 77: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:01:37.126885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 78: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 341 (no improvement in 50 epochs)\n",
      "Training time:  0:04:31.598870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 79: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 304 (no improvement in 50 epochs)\n",
      "Training time:  0:04:03.668437\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.109254.\n",
      "\n",
      "Testing Configuration 80: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 194 (no improvement in 50 epochs)\n",
      "Training time:  0:04:50.742368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 81: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:02:51.654468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 82: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 334 (no improvement in 50 epochs)\n",
      "Training time:  0:08:24.861522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 83: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 384 (no improvement in 50 epochs)\n",
      "Training time:  0:09:40.480019\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.107504.\n",
      "\n",
      "Testing Configuration 84: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:03:48.214800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 85: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:03:48.354999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 86: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 257 (no improvement in 50 epochs)\n",
      "Training time:  0:06:30.490757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 87: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 329 (no improvement in 50 epochs)\n",
      "Training time:  0:08:17.463795\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.109768.\n",
      "\n",
      "Testing Configuration 88: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:02:47.585908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 89: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 153 (no improvement in 50 epochs)\n",
      "Training time:  0:03:51.723270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 90: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 222 (no improvement in 50 epochs)\n",
      "Training time:  0:05:35.517903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 91: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 267 (no improvement in 50 epochs)\n",
      "Training time:  0:06:45.317799\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.112712.\n",
      "\n",
      "Testing Configuration 92: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 120 (no improvement in 50 epochs)\n",
      "Training time:  0:03:01.499725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 93: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 143 (no improvement in 50 epochs)\n",
      "Training time:  0:03:35.667482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 94: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 251 (no improvement in 50 epochs)\n",
      "Training time:  0:06:22.558444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 95: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 245 (no improvement in 50 epochs)\n",
      "Training time:  0:06:11.751775\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110915.\n",
      "\n",
      "Testing Configuration 96: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 163 (no improvement in 50 epochs)\n",
      "Training time:  0:05:52.707925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 97: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 161 (no improvement in 50 epochs)\n",
      "Training time:  0:05:49.771632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 98: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 409 (no improvement in 50 epochs)\n",
      "Training time:  0:14:49.273781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 99: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 356 (no improvement in 50 epochs)\n",
      "Training time:  0:12:53.278735\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.109001.\n",
      "\n",
      "Testing Configuration 100: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 145 (no improvement in 50 epochs)\n",
      "Training time:  0:05:15.287441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 101: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 139 (no improvement in 50 epochs)\n",
      "Training time:  0:05:02.377177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 102: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 222 (no improvement in 50 epochs)\n",
      "Training time:  0:08:04.024068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 103: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 349 (no improvement in 50 epochs)\n",
      "Training time:  0:12:39.894739\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.108508.\n",
      "\n",
      "Testing Configuration 104: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 144 (no improvement in 50 epochs)\n",
      "Training time:  0:05:13.827832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 105: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 162 (no improvement in 50 epochs)\n",
      "Training time:  0:05:52.543459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 106: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 321 (no improvement in 50 epochs)\n",
      "Training time:  0:11:39.902171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 107: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 203 (no improvement in 50 epochs)\n",
      "Training time:  0:07:21.597245\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110843.\n",
      "\n",
      "Testing Configuration 108: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 101 (no improvement in 50 epochs)\n",
      "Training time:  0:03:39.279098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 109: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 124 (no improvement in 50 epochs)\n",
      "Training time:  0:04:29.386648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 110: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 188 (no improvement in 50 epochs)\n",
      "Training time:  0:06:50.634672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 111: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 276 (no improvement in 50 epochs)\n",
      "Training time:  0:10:01.272417\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.107527.\n",
      "\n",
      "Testing Configuration 112: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 181 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.827581\n",
      "\n",
      "Testing Configuration 113: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 254 (no improvement in 50 epochs)\n",
      "Training time:  0:00:25.210264\n",
      "\n",
      "Testing Configuration 114: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 235 (no improvement in 50 epochs)\n",
      "Training time:  0:00:24.369026\n",
      "\n",
      "Testing Configuration 115: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 217 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.960077\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110001.\n",
      "\n",
      "Testing Configuration 116: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 174 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.636750\n",
      "\n",
      "Testing Configuration 117: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 182 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.483926\n",
      "\n",
      "Testing Configuration 118: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 239 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.621303\n",
      "\n",
      "Testing Configuration 119: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 276 (no improvement in 50 epochs)\n",
      "Training time:  0:00:26.143270\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.105210.\n",
      "\n",
      "Testing Configuration 120: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 184 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.494972\n",
      "\n",
      "Testing Configuration 121: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 124 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.974622\n",
      "\n",
      "Testing Configuration 122: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 379 (no improvement in 50 epochs)\n",
      "Training time:  0:00:49.369176\n",
      "\n",
      "Testing Configuration 123: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 353 (no improvement in 50 epochs)\n",
      "Training time:  0:00:46.247091\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.108144.\n",
      "\n",
      "Testing Configuration 124: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.931522\n",
      "\n",
      "Testing Configuration 125: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 209 (no improvement in 50 epochs)\n",
      "Training time:  0:00:25.180478\n",
      "\n",
      "Testing Configuration 126: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 268 (no improvement in 50 epochs)\n",
      "Training time:  0:00:33.305041\n",
      "\n",
      "Testing Configuration 127: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 265 (no improvement in 50 epochs)\n",
      "Training time:  0:00:32.512820\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.109298.\n",
      "\n",
      "Testing Configuration 128: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:00:33.007758\n",
      "\n",
      "Testing Configuration 129: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:00:33.148760\n",
      "\n",
      "Testing Configuration 130: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 372 (no improvement in 50 epochs)\n",
      "Training time:  0:01:10.888843\n",
      "\n",
      "Testing Configuration 131: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 132 (no improvement in 50 epochs)\n",
      "Training time:  0:00:25.396729\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.109503.\n",
      "\n",
      "Testing Configuration 132: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.860307\n",
      "\n",
      "Testing Configuration 133: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 127 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.912285\n",
      "\n",
      "Testing Configuration 134: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 201 (no improvement in 50 epochs)\n",
      "Training time:  0:00:35.203790\n",
      "\n",
      "Testing Configuration 135: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 244 (no improvement in 50 epochs)\n",
      "Training time:  0:00:43.032315\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.109242.\n",
      "\n",
      "Testing Configuration 136: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 120 (no improvement in 50 epochs)\n",
      "Training time:  0:00:36.406590\n",
      "\n",
      "Testing Configuration 137: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 170 (no improvement in 50 epochs)\n",
      "Training time:  0:00:51.885911\n",
      "\n",
      "Testing Configuration 138: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 213 (no improvement in 50 epochs)\n",
      "Training time:  0:01:05.539602\n",
      "\n",
      "Testing Configuration 139: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 193 (no improvement in 50 epochs)\n",
      "Training time:  0:00:59.909363\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110581.\n",
      "\n",
      "Testing Configuration 140: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 149 (no improvement in 50 epochs)\n",
      "Training time:  0:00:43.077767\n",
      "\n",
      "Testing Configuration 141: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 121 (no improvement in 50 epochs)\n",
      "Training time:  0:00:35.063011\n",
      "\n",
      "Testing Configuration 142: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 236 (no improvement in 50 epochs)\n",
      "Training time:  0:01:08.866205\n",
      "\n",
      "Testing Configuration 143: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:00:51.539347\n",
      "\n",
      "Processing Model: OptResNet1D_JARILWWF with params {'input_channels': 52, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110228.\n",
      "\n",
      "Testing Configuration 144: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 117 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.630706\n",
      "\n",
      "Testing Configuration 145: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 168 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.976188\n",
      "\n",
      "Testing Configuration 146: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 344 (no improvement in 50 epochs)\n",
      "Training time:  0:00:49.669154\n",
      "\n",
      "Testing Configuration 147: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 187 (no improvement in 50 epochs)\n",
      "Training time:  0:00:27.386351\n",
      "\n",
      "Processing Model: OptECAResNet1D_JARILWWF with params {'input_channels': 52, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.110001.\n",
      "\n",
      "Testing Configuration 148: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 115 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.584574\n",
      "\n",
      "Testing Configuration 149: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 139 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.643230\n",
      "\n",
      "Testing Configuration 150: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 132 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.081505\n",
      "\n",
      "Testing Configuration 151: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 204 (no improvement in 50 epochs)\n",
      "Training time:  0:00:26.794430\n",
      "\n",
      "Processing Model: CustomResNet1D with params {'input_channels': 52}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.108056.\n",
      "\n",
      "Testing Configuration 152: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 155 (no improvement in 50 epochs)\n",
      "Training time:  0:00:30.480100\n",
      "\n",
      "Testing Configuration 153: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 107 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.368308\n",
      "\n",
      "Testing Configuration 154: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 202 (no improvement in 50 epochs)\n",
      "Training time:  0:00:40.372338\n",
      "\n",
      "Testing Configuration 155: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 129 (no improvement in 50 epochs)\n",
      "Training time:  0:00:26.122028\n",
      "\n",
      "Processing Model: CustomECAResNet1D with params {'input_channels': 52}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.111001.\n",
      "\n",
      "Testing Configuration 156: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.915583\n",
      "\n",
      "Testing Configuration 157: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 143 (no improvement in 50 epochs)\n",
      "Training time:  0:00:31.024573\n",
      "\n",
      "Testing Configuration 158: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 171 (no improvement in 50 epochs)\n",
      "Training time:  0:00:37.326010\n",
      "\n",
      "Testing Configuration 159: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 96 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.217854\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T05:26:38.135002Z",
     "start_time": "2025-05-30T14:33:30.105236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gridsearch_results_df_bgsub = run_gridsearch_with_seeds(\n",
    "    gridsearch_func=grid_search,\n",
    "    gridsearch_args={\n",
    "        'param_grid': param_grid,\n",
    "        'folder_path': folder_path_5ghz_200hz_collected,\n",
    "        'background_subtraction': True,\n",
    "        'seconds_per_sample': 5,\n",
    "        'rows_per_second': 200,\n",
    "    },\n",
    "    n_seeds=3,\n",
    "    output_dir=output_path,\n",
    "    filename=\"gridsearch_5ghz_coll_200hz\"\n",
    ")"
   ],
   "id": "992a7599f0397e42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running seed 42 ---\n",
      "Running 160 configurations...\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.317447.\n",
      "\n",
      "Testing Configuration 0: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 143 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.733373\n",
      "\n",
      "Testing Configuration 1: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 184 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.019694\n",
      "\n",
      "Testing Configuration 2: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 165 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.399941\n",
      "\n",
      "Testing Configuration 3: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 186 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.566169\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.178202.\n",
      "\n",
      "Testing Configuration 4: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 152 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.499516\n",
      "\n",
      "Testing Configuration 5: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 155 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.109216\n",
      "\n",
      "Testing Configuration 6: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 168 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.102245\n",
      "\n",
      "Testing Configuration 7: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 250 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.581890\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.180000.\n",
      "\n",
      "Testing Configuration 8: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 132 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.206158\n",
      "\n",
      "Testing Configuration 9: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 93 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.030140\n",
      "\n",
      "Testing Configuration 10: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.884079\n",
      "\n",
      "Testing Configuration 11: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 112 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.988158\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.184372.\n",
      "\n",
      "Testing Configuration 12: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 144 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.550060\n",
      "\n",
      "Testing Configuration 13: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 153 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.524791\n",
      "\n",
      "Testing Configuration 14: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 392 (no improvement in 50 epochs)\n",
      "Training time:  0:00:35.567534\n",
      "\n",
      "Testing Configuration 15: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 254 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.349882\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.170023.\n",
      "\n",
      "Testing Configuration 16: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 85 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.864960\n",
      "\n",
      "Testing Configuration 17: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.214380\n",
      "\n",
      "Testing Configuration 18: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.980841\n",
      "\n",
      "Testing Configuration 19: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 147 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.860723\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.183500.\n",
      "\n",
      "Testing Configuration 20: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 153 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.066568\n",
      "\n",
      "Testing Configuration 21: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.248934\n",
      "\n",
      "Testing Configuration 22: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 119 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.349633\n",
      "\n",
      "Testing Configuration 23: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 132 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.783144\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.178262.\n",
      "\n",
      "Testing Configuration 24: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 72 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.158952\n",
      "\n",
      "Testing Configuration 25: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 82 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.585982\n",
      "\n",
      "Testing Configuration 26: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 130 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.196052\n",
      "\n",
      "Testing Configuration 27: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 148 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.475371\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.180000.\n",
      "\n",
      "Testing Configuration 28: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 80 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.326431\n",
      "\n",
      "Testing Configuration 29: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 74 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.543993\n",
      "\n",
      "Testing Configuration 30: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 124 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.586442\n",
      "\n",
      "Testing Configuration 31: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 313 (no improvement in 50 epochs)\n",
      "Training time:  0:00:37.777016\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.175561.\n",
      "\n",
      "Testing Configuration 32: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 84 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.090834\n",
      "\n",
      "Testing Configuration 33: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 157 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.446255\n",
      "\n",
      "Testing Configuration 34: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 187 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.142833\n",
      "\n",
      "Testing Configuration 35: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 163 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.270123\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.174001.\n",
      "\n",
      "Testing Configuration 36: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 123 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.212467\n",
      "\n",
      "Testing Configuration 37: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.549566\n",
      "\n",
      "Testing Configuration 38: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 206 (no improvement in 50 epochs)\n",
      "Training time:  0:00:24.225060\n",
      "\n",
      "Testing Configuration 39: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 198 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.515709\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.285500.\n",
      "\n",
      "Testing Configuration 40: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.353646\n",
      "\n",
      "Testing Configuration 41: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 131 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.042551\n",
      "\n",
      "Testing Configuration 42: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 84 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.034909\n",
      "\n",
      "Testing Configuration 43: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 101 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.073639\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.174001.\n",
      "\n",
      "Testing Configuration 44: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.408471\n",
      "\n",
      "Testing Configuration 45: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:00:24.748063\n",
      "\n",
      "Testing Configuration 46: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 168 (no improvement in 50 epochs)\n",
      "Training time:  0:00:27.834386\n",
      "\n",
      "Testing Configuration 47: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 196 (no improvement in 50 epochs)\n",
      "Training time:  0:00:32.758331\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.177928.\n",
      "\n",
      "Testing Configuration 48: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 69 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.784342\n",
      "\n",
      "Testing Configuration 49: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.833062\n",
      "\n",
      "Testing Configuration 50: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.820014\n",
      "\n",
      "Testing Configuration 51: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 142 (no improvement in 50 epochs)\n",
      "Training time:  0:00:24.882759\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.171999.\n",
      "\n",
      "Testing Configuration 52: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 102 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.460532\n",
      "\n",
      "Testing Configuration 53: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 82 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.154931\n",
      "\n",
      "Testing Configuration 54: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 168 (no improvement in 50 epochs)\n",
      "Training time:  0:00:29.227302\n",
      "\n",
      "Testing Configuration 55: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 134 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.439453\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.174085.\n",
      "\n",
      "Testing Configuration 56: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:00:27.531523\n",
      "\n",
      "Testing Configuration 57: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 64 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.251122\n",
      "\n",
      "Testing Configuration 58: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 95 (no improvement in 50 epochs)\n",
      "Training time:  0:00:27.099036\n",
      "\n",
      "Testing Configuration 59: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 60 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.229258\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.177001.\n",
      "\n",
      "Testing Configuration 60: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 63 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.813831\n",
      "\n",
      "Testing Configuration 61: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 74 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.030851\n",
      "\n",
      "Testing Configuration 62: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.046170\n",
      "\n",
      "Testing Configuration 63: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 69 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.891809\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.171172.\n",
      "\n",
      "Testing Configuration 64: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 134 (no improvement in 50 epochs)\n",
      "Training time:  0:01:48.678766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 65: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 169 (no improvement in 50 epochs)\n",
      "Training time:  0:02:15.596532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 66: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 293 (no improvement in 50 epochs)\n",
      "Training time:  0:03:56.817090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 67: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 173 (no improvement in 50 epochs)\n",
      "Training time:  0:02:19.724722\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.179074.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 68: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 201 (no improvement in 50 epochs)\n",
      "Training time:  0:02:42.538988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 69: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 184 (no improvement in 50 epochs)\n",
      "Training time:  0:02:28.302620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 70: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 289 (no improvement in 50 epochs)\n",
      "Training time:  0:03:53.812856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 71: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 284 (no improvement in 50 epochs)\n",
      "Training time:  0:03:49.875284\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.188001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 72: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 143 (no improvement in 50 epochs)\n",
      "Training time:  0:01:56.776465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 73: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 190 (no improvement in 50 epochs)\n",
      "Training time:  0:02:34.381047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 74: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 178 (no improvement in 50 epochs)\n",
      "Training time:  0:02:26.039304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 75: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 188 (no improvement in 50 epochs)\n",
      "Training time:  0:02:33.356858\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.190494.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 76: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:01:42.843748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 77: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:01:30.311788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 78: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 333 (no improvement in 50 epochs)\n",
      "Training time:  0:04:31.804919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 79: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 285 (no improvement in 50 epochs)\n",
      "Training time:  0:03:52.637271\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.181000.\n",
      "\n",
      "Testing Configuration 80: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 161 (no improvement in 50 epochs)\n",
      "Training time:  0:04:07.138121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 81: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 187 (no improvement in 50 epochs)\n",
      "Training time:  0:04:46.147273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 82: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 295 (no improvement in 50 epochs)\n",
      "Training time:  0:07:33.330188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 83: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 265 (no improvement in 50 epochs)\n",
      "Training time:  0:06:49.144947\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.180500.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 84: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 121 (no improvement in 50 epochs)\n",
      "Training time:  0:03:07.534901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 85: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:02:58.615010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 86: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 249 (no improvement in 50 epochs)\n",
      "Training time:  0:06:24.429043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 87: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 197 (no improvement in 50 epochs)\n",
      "Training time:  0:05:03.792853\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.178001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 88: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:02:56.707102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 89: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 124 (no improvement in 50 epochs)\n",
      "Training time:  0:03:10.163012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 90: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 194 (no improvement in 50 epochs)\n",
      "Training time:  0:04:59.083880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 91: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 202 (no improvement in 50 epochs)\n",
      "Training time:  0:05:15.740028\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.181504.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 92: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 170 (no improvement in 50 epochs)\n",
      "Training time:  0:04:22.522906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 93: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 134 (no improvement in 50 epochs)\n",
      "Training time:  0:03:26.793644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 94: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 225 (no improvement in 50 epochs)\n",
      "Training time:  0:05:44.891160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 95: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 322 (no improvement in 50 epochs)\n",
      "Training time:  0:08:16.898053\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.173509.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 96: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 208 (no improvement in 50 epochs)\n",
      "Training time:  0:07:42.667384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 97: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 139 (no improvement in 50 epochs)\n",
      "Training time:  0:05:08.287848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 98: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 209 (no improvement in 50 epochs)\n",
      "Training time:  0:07:44.477762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 99: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 365 (no improvement in 50 epochs)\n",
      "Training time:  0:13:31.078573\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.180210.\n",
      "\n",
      "Testing Configuration 100: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 193 (no improvement in 50 epochs)\n",
      "Training time:  0:07:07.569355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 101: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 182 (no improvement in 50 epochs)\n",
      "Training time:  0:06:41.828470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 102: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 273 (no improvement in 50 epochs)\n",
      "Training time:  0:10:07.885317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 103: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 354 (no improvement in 50 epochs)\n",
      "Training time:  0:13:06.147228\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.172499.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 104: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 138 (no improvement in 50 epochs)\n",
      "Training time:  0:05:06.951278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 105: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 166 (no improvement in 50 epochs)\n",
      "Training time:  0:06:11.136942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 106: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 217 (no improvement in 50 epochs)\n",
      "Training time:  0:08:04.616942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 107: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 219 (no improvement in 50 epochs)\n",
      "Training time:  0:08:09.816281\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.172836.\n",
      "\n",
      "Testing Configuration 108: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 164 (no improvement in 50 epochs)\n",
      "Training time:  0:06:03.942721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 109: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 130 (no improvement in 50 epochs)\n",
      "Training time:  0:04:49.269260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 110: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 365 (no improvement in 50 epochs)\n",
      "Training time:  0:13:32.228417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 111: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 178 (no improvement in 50 epochs)\n",
      "Training time:  0:06:36.184263\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.173063.\n",
      "\n",
      "Testing Configuration 112: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 148 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.473806\n",
      "\n",
      "Testing Configuration 113: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 159 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.612450\n",
      "\n",
      "Testing Configuration 114: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 298 (no improvement in 50 epochs)\n",
      "Training time:  0:00:29.757768\n",
      "\n",
      "Testing Configuration 115: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 166 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.850653\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.176171.\n",
      "\n",
      "Testing Configuration 116: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 99 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.948910\n",
      "\n",
      "Testing Configuration 117: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.703961\n",
      "\n",
      "Testing Configuration 118: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 184 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.066216\n",
      "\n",
      "Testing Configuration 119: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 214 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.936612\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.174989.\n",
      "\n",
      "Testing Configuration 120: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 220 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.008153\n",
      "\n",
      "Testing Configuration 121: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 131 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.860567\n",
      "\n",
      "Testing Configuration 122: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 182 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.667162\n",
      "\n",
      "Testing Configuration 123: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 257 (no improvement in 50 epochs)\n",
      "Training time:  0:00:33.998500\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.178498.\n",
      "\n",
      "Testing Configuration 124: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.935571\n",
      "\n",
      "Testing Configuration 125: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 162 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.404390\n",
      "\n",
      "Testing Configuration 126: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 195 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.532854\n",
      "\n",
      "Testing Configuration 127: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 175 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.166123\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.177233.\n",
      "\n",
      "Testing Configuration 128: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 246 (no improvement in 50 epochs)\n",
      "Training time:  0:00:45.742407\n",
      "\n",
      "Testing Configuration 129: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 160 (no improvement in 50 epochs)\n",
      "Training time:  0:00:30.047827\n",
      "\n",
      "Testing Configuration 130: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 293 (no improvement in 50 epochs)\n",
      "Training time:  0:00:55.396564\n",
      "\n",
      "Testing Configuration 131: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 252 (no improvement in 50 epochs)\n",
      "Training time:  0:00:47.875101\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.174227.\n",
      "\n",
      "Testing Configuration 132: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 134 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.840978\n",
      "\n",
      "Testing Configuration 133: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 109 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.691149\n",
      "\n",
      "Testing Configuration 134: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 180 (no improvement in 50 epochs)\n",
      "Training time:  0:00:31.231992\n",
      "\n",
      "Testing Configuration 135: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 163 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.408326\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.181005.\n",
      "\n",
      "Testing Configuration 136: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:00:33.744721\n",
      "\n",
      "Testing Configuration 137: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 121 (no improvement in 50 epochs)\n",
      "Training time:  0:00:36.952655\n",
      "\n",
      "Testing Configuration 138: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 145 (no improvement in 50 epochs)\n",
      "Training time:  0:00:44.784485\n",
      "\n",
      "Testing Configuration 139: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 213 (no improvement in 50 epochs)\n",
      "Training time:  0:01:05.654774\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.183146.\n",
      "\n",
      "Testing Configuration 140: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 130 (no improvement in 50 epochs)\n",
      "Training time:  0:00:37.168360\n",
      "\n",
      "Testing Configuration 141: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 99 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.454326\n",
      "\n",
      "Testing Configuration 142: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 173 (no improvement in 50 epochs)\n",
      "Training time:  0:00:50.299844\n",
      "\n",
      "Testing Configuration 143: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 149 (no improvement in 50 epochs)\n",
      "Training time:  0:00:43.950412\n",
      "\n",
      "Processing Model: OptResNet1D_JARILWWF with params {'input_channels': 52, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.255250.\n",
      "\n",
      "Testing Configuration 144: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 123 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.463568\n",
      "\n",
      "Testing Configuration 145: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 106 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.149476\n",
      "\n",
      "Testing Configuration 146: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 129 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.594952\n",
      "\n",
      "Testing Configuration 147: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 156 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.617759\n",
      "\n",
      "Processing Model: OptECAResNet1D_JARILWWF with params {'input_channels': 52, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.167338.\n",
      "\n",
      "Testing Configuration 148: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.969378\n",
      "\n",
      "Testing Configuration 149: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 110 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.983629\n",
      "\n",
      "Testing Configuration 150: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 127 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.296831\n",
      "\n",
      "Testing Configuration 151: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.783210\n",
      "\n",
      "Processing Model: CustomResNet1D with params {'input_channels': 52}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.179555.\n",
      "\n",
      "Testing Configuration 152: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 135 (no improvement in 50 epochs)\n",
      "Training time:  0:00:26.637864\n",
      "\n",
      "Testing Configuration 153: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 144 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.617643\n",
      "\n",
      "Testing Configuration 154: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 141 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.252602\n",
      "\n",
      "Testing Configuration 155: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 140 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.249336\n",
      "\n",
      "Processing Model: CustomECAResNet1D with params {'input_channels': 52}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.173230.\n",
      "\n",
      "Testing Configuration 156: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.075545\n",
      "\n",
      "Testing Configuration 157: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 159 (no improvement in 50 epochs)\n",
      "Training time:  0:00:34.392461\n",
      "\n",
      "Testing Configuration 158: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 154 (no improvement in 50 epochs)\n",
      "Training time:  0:00:33.399044\n",
      "\n",
      "Testing Configuration 159: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 164 (no improvement in 50 epochs)\n",
      "Training time:  0:00:35.938201\n",
      "\n",
      "--- Running seed 420 ---\n",
      "Running 160 configurations...\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.166985.\n",
      "\n",
      "Testing Configuration 0: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 183 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.492744\n",
      "\n",
      "Testing Configuration 1: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 137 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.772694\n",
      "\n",
      "Testing Configuration 2: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 205 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.646542\n",
      "\n",
      "Testing Configuration 3: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 195 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.889303\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.165219.\n",
      "\n",
      "Testing Configuration 4: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 218 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.339083\n",
      "\n",
      "Testing Configuration 5: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 179 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.452911\n",
      "\n",
      "Testing Configuration 6: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 186 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.924104\n",
      "\n",
      "Testing Configuration 7: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 247 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.845870\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.173025.\n",
      "\n",
      "Testing Configuration 8: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 121 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.233439\n",
      "\n",
      "Testing Configuration 9: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 140 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.076339\n",
      "\n",
      "Testing Configuration 10: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 142 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.535596\n",
      "\n",
      "Testing Configuration 11: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 149 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.197727\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.164506.\n",
      "\n",
      "Testing Configuration 12: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 169 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.230049\n",
      "\n",
      "Testing Configuration 13: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 121 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.417275\n",
      "\n",
      "Testing Configuration 14: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 132 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.386003\n",
      "\n",
      "Testing Configuration 15: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 170 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.286779\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.164860.\n",
      "\n",
      "Testing Configuration 16: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 96 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.869029\n",
      "\n",
      "Testing Configuration 17: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 107 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.987936\n",
      "\n",
      "Testing Configuration 18: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 159 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.340288\n",
      "\n",
      "Testing Configuration 19: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 218 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.000937\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.163499.\n",
      "\n",
      "Testing Configuration 20: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 137 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.556075\n",
      "\n",
      "Testing Configuration 21: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.420359\n",
      "\n",
      "Testing Configuration 22: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 226 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.470140\n",
      "\n",
      "Testing Configuration 23: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 137 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.116403\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.162025.\n",
      "\n",
      "Testing Configuration 24: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.604765\n",
      "\n",
      "Testing Configuration 25: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 79 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.141421\n",
      "\n",
      "Testing Configuration 26: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 149 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.688814\n",
      "\n",
      "Testing Configuration 27: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 112 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.257012\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.164151.\n",
      "\n",
      "Testing Configuration 28: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 97 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.115157\n",
      "\n",
      "Testing Configuration 29: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 80 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.252525\n",
      "\n",
      "Testing Configuration 30: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 113 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.139208\n",
      "\n",
      "Testing Configuration 31: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 123 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.568673\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.160057.\n",
      "\n",
      "Testing Configuration 32: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.479827\n",
      "\n",
      "Testing Configuration 33: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.385486\n",
      "\n",
      "Testing Configuration 34: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.357023\n",
      "\n",
      "Testing Configuration 35: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 167 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.684332\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.160241.\n",
      "\n",
      "Testing Configuration 36: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.458143\n",
      "\n",
      "Testing Configuration 37: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 149 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.429988\n",
      "\n",
      "Testing Configuration 38: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 229 (no improvement in 50 epochs)\n",
      "Training time:  0:00:25.543069\n",
      "\n",
      "Testing Configuration 39: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 234 (no improvement in 50 epochs)\n",
      "Training time:  0:00:26.421528\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.164498.\n",
      "\n",
      "Testing Configuration 40: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 89 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.491628\n",
      "\n",
      "Testing Configuration 41: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 99 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.198346\n",
      "\n",
      "Testing Configuration 42: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.552298\n",
      "\n",
      "Testing Configuration 43: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 147 (no improvement in 50 epochs)\n",
      "Training time:  0:00:24.463734\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.164498.\n",
      "\n",
      "Testing Configuration 44: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.135596\n",
      "\n",
      "Testing Configuration 45: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.163869\n",
      "\n",
      "Testing Configuration 46: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 162 (no improvement in 50 epochs)\n",
      "Training time:  0:00:26.864891\n",
      "\n",
      "Testing Configuration 47: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 140 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.586046\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.162955.\n",
      "\n",
      "Testing Configuration 48: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 67 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.429218\n",
      "\n",
      "Testing Configuration 49: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 80 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.810029\n",
      "\n",
      "Testing Configuration 50: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.381691\n",
      "\n",
      "Testing Configuration 51: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 165 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.927880\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.180998.\n",
      "\n",
      "Testing Configuration 52: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.026815\n",
      "\n",
      "Testing Configuration 53: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 125 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.530988\n",
      "\n",
      "Testing Configuration 54: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 220 (no improvement in 50 epochs)\n",
      "Training time:  0:00:38.171469\n",
      "\n",
      "Testing Configuration 55: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 148 (no improvement in 50 epochs)\n",
      "Training time:  0:00:25.913583\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.162178.\n",
      "\n",
      "Testing Configuration 56: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 67 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.812874\n",
      "\n",
      "Testing Configuration 57: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 79 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.341032\n",
      "\n",
      "Testing Configuration 58: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 71 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.299271\n",
      "\n",
      "Testing Configuration 59: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 104 (no improvement in 50 epochs)\n",
      "Training time:  0:00:29.878704\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.164000.\n",
      "\n",
      "Testing Configuration 60: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.330316\n",
      "\n",
      "Testing Configuration 61: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 82 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.351047\n",
      "\n",
      "Testing Configuration 62: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:00:32.410918\n",
      "\n",
      "Testing Configuration 63: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:00:26.978922\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.167410.\n",
      "\n",
      "Testing Configuration 64: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 244 (no improvement in 50 epochs)\n",
      "Training time:  0:03:15.854781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 65: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 171 (no improvement in 50 epochs)\n",
      "Training time:  0:02:17.191902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 66: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 470 (no improvement in 50 epochs)\n",
      "Training time:  0:06:18.682699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 67: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 301 (no improvement in 50 epochs)\n",
      "Training time:  0:04:01.372806\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.173258.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 68: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 130 (no improvement in 50 epochs)\n",
      "Training time:  0:01:43.870005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 69: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 201 (no improvement in 50 epochs)\n",
      "Training time:  0:02:41.231453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 70: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 244 (no improvement in 50 epochs)\n",
      "Training time:  0:03:16.667771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 71: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 198 (no improvement in 50 epochs)\n",
      "Training time:  0:02:39.479475\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.162138.\n",
      "\n",
      "Testing Configuration 72: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 155 (no improvement in 50 epochs)\n",
      "Training time:  0:02:04.765303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 73: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:01:38.843055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 74: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 276 (no improvement in 50 epochs)\n",
      "Training time:  0:03:43.716259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 75: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 235 (no improvement in 50 epochs)\n",
      "Training time:  0:03:09.422866\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.161500.\n",
      "\n",
      "Testing Configuration 76: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 171 (no improvement in 50 epochs)\n",
      "Training time:  0:02:17.753636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 77: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 200 (no improvement in 50 epochs)\n",
      "Training time:  0:02:41.988678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 78: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 221 (no improvement in 50 epochs)\n",
      "Training time:  0:02:58.693963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 79: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 344 (no improvement in 50 epochs)\n",
      "Training time:  0:04:40.051102\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.159504.\n",
      "\n",
      "Testing Configuration 80: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 180 (no improvement in 50 epochs)\n",
      "Training time:  0:04:34.673769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 81: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 179 (no improvement in 50 epochs)\n",
      "Training time:  0:04:33.332493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 82: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 263 (no improvement in 50 epochs)\n",
      "Training time:  0:06:44.008248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 83: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 195 (no improvement in 50 epochs)\n",
      "Training time:  0:05:01.064527\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.166581.\n",
      "\n",
      "Testing Configuration 84: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 179 (no improvement in 50 epochs)\n",
      "Training time:  0:04:32.861154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 85: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 162 (no improvement in 50 epochs)\n",
      "Training time:  0:04:07.453078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 86: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 327 (no improvement in 50 epochs)\n",
      "Training time:  0:08:21.606504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 87: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 265 (no improvement in 50 epochs)\n",
      "Training time:  0:06:45.893081\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.160500.\n",
      "\n",
      "Testing Configuration 88: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 127 (no improvement in 50 epochs)\n",
      "Training time:  0:03:13.665543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 89: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 207 (no improvement in 50 epochs)\n",
      "Training time:  0:05:17.492336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 90: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 186 (no improvement in 50 epochs)\n",
      "Training time:  0:04:46.072457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 91: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 219 (no improvement in 50 epochs)\n",
      "Training time:  0:05:38.628296\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.156000.\n",
      "\n",
      "Testing Configuration 92: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 177 (no improvement in 50 epochs)\n",
      "Training time:  0:04:31.690631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 93: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 105 (no improvement in 50 epochs)\n",
      "Training time:  0:02:41.808307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 94: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 230 (no improvement in 50 epochs)\n",
      "Training time:  0:05:55.654572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 95: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 229 (no improvement in 50 epochs)\n",
      "Training time:  0:05:54.336053\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.150500.\n",
      "\n",
      "Testing Configuration 96: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 130 (no improvement in 50 epochs)\n",
      "Training time:  0:04:47.480035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 97: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 218 (no improvement in 50 epochs)\n",
      "Training time:  0:08:04.952651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 98: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 219 (no improvement in 50 epochs)\n",
      "Training time:  0:08:03.560150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 99: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 292 (no improvement in 50 epochs)\n",
      "Training time:  0:10:45.593054\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.159500.\n",
      "\n",
      "Testing Configuration 100: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 149 (no improvement in 50 epochs)\n",
      "Training time:  0:05:30.226384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 101: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 105 (no improvement in 50 epochs)\n",
      "Training time:  0:03:52.249698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 102: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 361 (no improvement in 50 epochs)\n",
      "Training time:  0:13:20.575512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 103: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 274 (no improvement in 50 epochs)\n",
      "Training time:  0:10:06.902321\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.151597.\n",
      "\n",
      "Testing Configuration 104: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 160 (no improvement in 50 epochs)\n",
      "Training time:  0:05:55.129135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 105: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 132 (no improvement in 50 epochs)\n",
      "Training time:  0:04:52.972122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 106: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 257 (no improvement in 50 epochs)\n",
      "Training time:  0:09:31.132899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 107: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 314 (no improvement in 50 epochs)\n",
      "Training time:  0:11:38.216787\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.160222.\n",
      "\n",
      "Testing Configuration 108: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 91 (no improvement in 50 epochs)\n",
      "Training time:  0:03:23.380734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 109: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:03:49.049865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 110: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 283 (no improvement in 50 epochs)\n",
      "Training time:  0:10:32.293612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 111: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 293 (no improvement in 50 epochs)\n",
      "Training time:  0:10:52.312279\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.157269.\n",
      "\n",
      "Testing Configuration 112: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.942993\n",
      "\n",
      "Testing Configuration 113: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 120 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.970880\n",
      "\n",
      "Testing Configuration 114: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 209 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.024848\n",
      "\n",
      "Testing Configuration 115: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.436932\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.174232.\n",
      "\n",
      "Testing Configuration 116: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.137575\n",
      "\n",
      "Testing Configuration 117: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 87 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.900688\n",
      "\n",
      "Testing Configuration 118: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 168 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.730249\n",
      "\n",
      "Testing Configuration 119: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 110 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.434829\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.147580.\n",
      "\n",
      "Testing Configuration 120: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 129 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.412182\n",
      "\n",
      "Testing Configuration 121: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 118 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.142095\n",
      "\n",
      "Testing Configuration 122: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 167 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.768491\n",
      "\n",
      "Testing Configuration 123: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 235 (no improvement in 50 epochs)\n",
      "Training time:  0:00:30.999549\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.145000.\n",
      "\n",
      "Testing Configuration 124: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 115 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.633166\n",
      "\n",
      "Testing Configuration 125: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.223339\n",
      "\n",
      "Testing Configuration 126: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 235 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.278888\n",
      "\n",
      "Testing Configuration 127: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 320 (no improvement in 50 epochs)\n",
      "Training time:  0:00:39.265921\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.154367.\n",
      "\n",
      "Testing Configuration 128: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 145 (no improvement in 50 epochs)\n",
      "Training time:  0:00:26.988691\n",
      "\n",
      "Testing Configuration 129: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 193 (no improvement in 50 epochs)\n",
      "Training time:  0:00:36.040668\n",
      "\n",
      "Testing Configuration 130: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 213 (no improvement in 50 epochs)\n",
      "Training time:  0:00:40.299903\n",
      "\n",
      "Testing Configuration 131: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 297 (no improvement in 50 epochs)\n",
      "Training time:  0:00:56.492359\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.169025.\n",
      "\n",
      "Testing Configuration 132: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 82 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.079363\n",
      "\n",
      "Testing Configuration 133: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 108 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.589199\n",
      "\n",
      "Testing Configuration 134: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 180 (no improvement in 50 epochs)\n",
      "Training time:  0:00:31.225624\n",
      "\n",
      "Testing Configuration 135: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 180 (no improvement in 50 epochs)\n",
      "Training time:  0:00:31.466328\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.148172.\n",
      "\n",
      "Testing Configuration 136: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 125 (no improvement in 50 epochs)\n",
      "Training time:  0:00:37.711906\n",
      "\n",
      "Testing Configuration 137: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 188 (no improvement in 50 epochs)\n",
      "Training time:  0:00:57.049768\n",
      "\n",
      "Testing Configuration 138: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 173 (no improvement in 50 epochs)\n",
      "Training time:  0:00:52.944559\n",
      "\n",
      "Testing Configuration 139: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 201 (no improvement in 50 epochs)\n",
      "Training time:  0:01:01.591441\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.145216.\n",
      "\n",
      "Testing Configuration 140: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 95 (no improvement in 50 epochs)\n",
      "Training time:  0:00:27.163173\n",
      "\n",
      "Testing Configuration 141: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 138 (no improvement in 50 epochs)\n",
      "Training time:  0:00:40.048022\n",
      "\n",
      "Testing Configuration 142: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 175 (no improvement in 50 epochs)\n",
      "Training time:  0:00:50.887224\n",
      "\n",
      "Testing Configuration 143: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 218 (no improvement in 50 epochs)\n",
      "Training time:  0:01:03.395820\n",
      "\n",
      "Processing Model: OptResNet1D_JARILWWF with params {'input_channels': 52, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.149282.\n",
      "\n",
      "Testing Configuration 144: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 144 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.360407\n",
      "\n",
      "Testing Configuration 145: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 109 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.578502\n",
      "\n",
      "Testing Configuration 146: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 82 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.836316\n",
      "\n",
      "Testing Configuration 147: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.878698\n",
      "\n",
      "Processing Model: OptECAResNet1D_JARILWWF with params {'input_channels': 52, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.143451.\n",
      "\n",
      "Testing Configuration 148: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 132 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.623416\n",
      "\n",
      "Testing Configuration 149: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 272 (no improvement in 50 epochs)\n",
      "Training time:  0:00:34.482914\n",
      "\n",
      "Testing Configuration 150: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.357114\n",
      "\n",
      "Testing Configuration 151: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 128 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.729329\n",
      "\n",
      "Processing Model: CustomResNet1D with params {'input_channels': 52}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.147570.\n",
      "\n",
      "Testing Configuration 152: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 134 (no improvement in 50 epochs)\n",
      "Training time:  0:00:26.419065\n",
      "\n",
      "Testing Configuration 153: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 137 (no improvement in 50 epochs)\n",
      "Training time:  0:00:27.190825\n",
      "\n",
      "Testing Configuration 154: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 177 (no improvement in 50 epochs)\n",
      "Training time:  0:00:35.444811\n",
      "\n",
      "Testing Configuration 155: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 160 (no improvement in 50 epochs)\n",
      "Training time:  0:00:32.475469\n",
      "\n",
      "Processing Model: CustomECAResNet1D with params {'input_channels': 52}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.145500.\n",
      "\n",
      "Testing Configuration 156: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.932935\n",
      "\n",
      "Testing Configuration 157: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:00:27.279035\n",
      "\n",
      "Testing Configuration 158: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 171 (no improvement in 50 epochs)\n",
      "Training time:  0:00:37.201982\n",
      "\n",
      "Testing Configuration 159: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.601842\n",
      "\n",
      "--- Running seed 101010 ---\n",
      "Running 160 configurations...\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.149497.\n",
      "\n",
      "Testing Configuration 0: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.732967\n",
      "\n",
      "Testing Configuration 1: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 169 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.116361\n",
      "\n",
      "Testing Configuration 2: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 376 (no improvement in 50 epochs)\n",
      "Training time:  0:00:31.905397\n",
      "\n",
      "Testing Configuration 3: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 153 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.127723\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.148620.\n",
      "\n",
      "Testing Configuration 4: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 187 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.180294\n",
      "\n",
      "Testing Configuration 5: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 180 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.919031\n",
      "\n",
      "Testing Configuration 6: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 188 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.105413\n",
      "\n",
      "Testing Configuration 7: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 337 (no improvement in 50 epochs)\n",
      "Training time:  0:00:29.495322\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.152124.\n",
      "\n",
      "Testing Configuration 8: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.782987\n",
      "\n",
      "Testing Configuration 9: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 143 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.479579\n",
      "\n",
      "Testing Configuration 10: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 128 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.563385\n",
      "\n",
      "Testing Configuration 11: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.509559\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.149658.\n",
      "\n",
      "Testing Configuration 12: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 209 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.124452\n",
      "\n",
      "Testing Configuration 13: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 130 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.279608\n",
      "\n",
      "Testing Configuration 14: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 158 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.887115\n",
      "\n",
      "Testing Configuration 15: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 257 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.784479\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.154501.\n",
      "\n",
      "Testing Configuration 16: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 112 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.262107\n",
      "\n",
      "Testing Configuration 17: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 104 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.755755\n",
      "\n",
      "Testing Configuration 18: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 149 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.183628\n",
      "\n",
      "Testing Configuration 19: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 172 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.543060\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.152017.\n",
      "\n",
      "Testing Configuration 20: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.267421\n",
      "\n",
      "Testing Configuration 21: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.763465\n",
      "\n",
      "Testing Configuration 22: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 172 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.343918\n",
      "\n",
      "Testing Configuration 23: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 188 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.041633\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.149500.\n",
      "\n",
      "Testing Configuration 24: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 70 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.958750\n",
      "\n",
      "Testing Configuration 25: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 81 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.481481\n",
      "\n",
      "Testing Configuration 26: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.056365\n",
      "\n",
      "Testing Configuration 27: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.211729\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.148546.\n",
      "\n",
      "Testing Configuration 28: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 119 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.682022\n",
      "\n",
      "Testing Configuration 29: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 80 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.360271\n",
      "\n",
      "Testing Configuration 30: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 211 (no improvement in 50 epochs)\n",
      "Training time:  0:00:24.744349\n",
      "\n",
      "Testing Configuration 31: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 161 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.117932\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.149690.\n",
      "\n",
      "Testing Configuration 32: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.295192\n",
      "\n",
      "Testing Configuration 33: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 159 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.551029\n",
      "\n",
      "Testing Configuration 34: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 277 (no improvement in 50 epochs)\n",
      "Training time:  0:00:30.693740\n",
      "\n",
      "Testing Configuration 35: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.429910\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.144846.\n",
      "\n",
      "Testing Configuration 36: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 120 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.005869\n",
      "\n",
      "Testing Configuration 37: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 192 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.137969\n",
      "\n",
      "Testing Configuration 38: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 196 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.065283\n",
      "\n",
      "Testing Configuration 39: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 450 (no improvement in 50 epochs)\n",
      "Training time:  0:00:51.432812\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.149590.\n",
      "\n",
      "Testing Configuration 40: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.638958\n",
      "\n",
      "Testing Configuration 41: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 117 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.151126\n",
      "\n",
      "Testing Configuration 42: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 90 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.939092\n",
      "\n",
      "Testing Configuration 43: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.722130\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.151026.\n",
      "\n",
      "Testing Configuration 44: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 142 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.178290\n",
      "\n",
      "Testing Configuration 45: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 112 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.374417\n",
      "\n",
      "Testing Configuration 46: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 173 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.791215\n",
      "\n",
      "Testing Configuration 47: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 143 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.898583\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.150500.\n",
      "\n",
      "Testing Configuration 48: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 75 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.762336\n",
      "\n",
      "Testing Configuration 49: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.289151\n",
      "\n",
      "Testing Configuration 50: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 153 (no improvement in 50 epochs)\n",
      "Training time:  0:00:26.551347\n",
      "\n",
      "Testing Configuration 51: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.157516\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.153726.\n",
      "\n",
      "Testing Configuration 52: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 87 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.840763\n",
      "\n",
      "Testing Configuration 53: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.145118\n",
      "\n",
      "Testing Configuration 54: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 146 (no improvement in 50 epochs)\n",
      "Training time:  0:00:25.219222\n",
      "\n",
      "Testing Configuration 55: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 106 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.513601\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.149408.\n",
      "\n",
      "Testing Configuration 56: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 68 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.184881\n",
      "\n",
      "Testing Configuration 57: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 80 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.661705\n",
      "\n",
      "Testing Configuration 58: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 79 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.517602\n",
      "\n",
      "Testing Configuration 59: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 66 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.909311\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 52, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.151189.\n",
      "\n",
      "Testing Configuration 60: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 71 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.985433\n",
      "\n",
      "Testing Configuration 61: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 70 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.864707\n",
      "\n",
      "Testing Configuration 62: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 171 (no improvement in 50 epochs)\n",
      "Training time:  0:00:48.574180\n",
      "\n",
      "Testing Configuration 63: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 160 (no improvement in 50 epochs)\n",
      "Training time:  0:00:45.770176\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.152001.\n",
      "\n",
      "Testing Configuration 64: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 217 (no improvement in 50 epochs)\n",
      "Training time:  0:02:53.764376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 65: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 207 (no improvement in 50 epochs)\n",
      "Training time:  0:02:46.072963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 66: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 348 (no improvement in 50 epochs)\n",
      "Training time:  0:04:38.776641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 67: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 257 (no improvement in 50 epochs)\n",
      "Training time:  0:03:27.202814\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.150500.\n",
      "\n",
      "Testing Configuration 68: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 160 (no improvement in 50 epochs)\n",
      "Training time:  0:02:08.423772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 69: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 288 (no improvement in 50 epochs)\n",
      "Training time:  0:03:51.594847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 70: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 384 (no improvement in 50 epochs)\n",
      "Training time:  0:05:08.677087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 71: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 214 (no improvement in 50 epochs)\n",
      "Training time:  0:02:51.931392\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.153500.\n",
      "\n",
      "Testing Configuration 72: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 165 (no improvement in 50 epochs)\n",
      "Training time:  0:02:13.807816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 73: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 119 (no improvement in 50 epochs)\n",
      "Training time:  0:01:36.543689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 74: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 204 (no improvement in 50 epochs)\n",
      "Training time:  0:02:45.350066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 75: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 284 (no improvement in 50 epochs)\n",
      "Training time:  0:03:50.829762\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.149200.\n",
      "\n",
      "Testing Configuration 76: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 159 (no improvement in 50 epochs)\n",
      "Training time:  0:02:08.462652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 77: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:02:22.797674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 78: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 216 (no improvement in 50 epochs)\n",
      "Training time:  0:02:55.177274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 79: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 249 (no improvement in 50 epochs)\n",
      "Training time:  0:03:22.744731\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.154630.\n",
      "\n",
      "Testing Configuration 80: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 108 (no improvement in 50 epochs)\n",
      "Training time:  0:02:46.286917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 81: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 162 (no improvement in 50 epochs)\n",
      "Training time:  0:04:09.475104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 82: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 318 (no improvement in 50 epochs)\n",
      "Training time:  0:08:09.777117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 83: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 247 (no improvement in 50 epochs)\n",
      "Training time:  0:06:18.818067\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.153507.\n",
      "\n",
      "Testing Configuration 84: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 174 (no improvement in 50 epochs)\n",
      "Training time:  0:04:27.551481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 85: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 187 (no improvement in 50 epochs)\n",
      "Training time:  0:04:48.474456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 86: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 241 (no improvement in 50 epochs)\n",
      "Training time:  0:06:09.593715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 87: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 208 (no improvement in 50 epochs)\n",
      "Training time:  0:05:20.306024\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.148135.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 88: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 130 (no improvement in 50 epochs)\n",
      "Training time:  0:03:20.471400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 89: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 121 (no improvement in 50 epochs)\n",
      "Training time:  0:03:07.810408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 90: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 236 (no improvement in 50 epochs)\n",
      "Training time:  0:06:05.874739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 91: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 188 (no improvement in 50 epochs)\n",
      "Training time:  0:04:51.669441\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.148500.\n",
      "\n",
      "Testing Configuration 92: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 118 (no improvement in 50 epochs)\n",
      "Training time:  0:03:02.221407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 93: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 218 (no improvement in 50 epochs)\n",
      "Training time:  0:05:38.844553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 94: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 274 (no improvement in 50 epochs)\n",
      "Training time:  0:07:03.987292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 95: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 223 (no improvement in 50 epochs)\n",
      "Training time:  0:05:44.973954\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.147356.\n",
      "\n",
      "Testing Configuration 96: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 202 (no improvement in 50 epochs)\n",
      "Training time:  0:07:28.576653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 97: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 143 (no improvement in 50 epochs)\n",
      "Training time:  0:05:16.244763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 98: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 216 (no improvement in 50 epochs)\n",
      "Training time:  0:08:02.091251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 99: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 294 (no improvement in 50 epochs)\n",
      "Training time:  0:10:52.505125\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.146459.\n",
      "\n",
      "Testing Configuration 100: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 193 (no improvement in 50 epochs)\n",
      "Training time:  0:07:07.803653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 101: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 145 (no improvement in 50 epochs)\n",
      "Training time:  0:05:36.322105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 102: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 311 (no improvement in 50 epochs)\n",
      "Training time:  0:11:47.399321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 103: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 282 (no improvement in 50 epochs)\n",
      "Training time:  0:10:25.429586\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.161059.\n",
      "\n",
      "Testing Configuration 104: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:05:35.070206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 105: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 137 (no improvement in 50 epochs)\n",
      "Training time:  0:05:05.861037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 106: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 320 (no improvement in 50 epochs)\n",
      "Training time:  0:11:52.737827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 107: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 228 (no improvement in 50 epochs)\n",
      "Training time:  0:08:27.971875\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 52, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.146864.\n",
      "\n",
      "Testing Configuration 108: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 144 (no improvement in 50 epochs)\n",
      "Training time:  0:05:19.784136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 109: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 161 (no improvement in 50 epochs)\n",
      "Training time:  0:05:59.498289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 110: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 313 (no improvement in 50 epochs)\n",
      "Training time:  0:11:38.084630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 111: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 251 (no improvement in 50 epochs)\n",
      "Training time:  0:09:20.670120\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.151506.\n",
      "\n",
      "Testing Configuration 112: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 154 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.424651\n",
      "\n",
      "Testing Configuration 113: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 197 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.816639\n",
      "\n",
      "Testing Configuration 114: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 138 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.005641\n",
      "\n",
      "Testing Configuration 115: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 305 (no improvement in 50 epochs)\n",
      "Training time:  0:00:31.716507\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.147504.\n",
      "\n",
      "Testing Configuration 116: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 140 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.594264\n",
      "\n",
      "Testing Configuration 117: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 91 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.680084\n",
      "\n",
      "Testing Configuration 118: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 372 (no improvement in 50 epochs)\n",
      "Training time:  0:00:34.973035\n",
      "\n",
      "Testing Configuration 119: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 149 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.421578\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.147111.\n",
      "\n",
      "Testing Configuration 120: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 140 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.184944\n",
      "\n",
      "Testing Configuration 121: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.038295\n",
      "\n",
      "Testing Configuration 122: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 161 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.169015\n",
      "\n",
      "Testing Configuration 123: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.879964\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.146037.\n",
      "\n",
      "Testing Configuration 124: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.089086\n",
      "\n",
      "Testing Configuration 125: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 181 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.746980\n",
      "\n",
      "Testing Configuration 126: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 189 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.936762\n",
      "\n",
      "Testing Configuration 127: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 195 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.890301\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.144300.\n",
      "\n",
      "Testing Configuration 128: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 214 (no improvement in 50 epochs)\n",
      "Training time:  0:00:39.920186\n",
      "\n",
      "Testing Configuration 129: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.564842\n",
      "\n",
      "Testing Configuration 130: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 286 (no improvement in 50 epochs)\n",
      "Training time:  0:00:54.439276\n",
      "\n",
      "Testing Configuration 131: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 148 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.316027\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.137109.\n",
      "\n",
      "Testing Configuration 132: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 97 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.822526\n",
      "\n",
      "Testing Configuration 133: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 163 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.026093\n",
      "\n",
      "Testing Configuration 134: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 161 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.209949\n",
      "\n",
      "Testing Configuration 135: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 193 (no improvement in 50 epochs)\n",
      "Training time:  0:00:34.000305\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 52, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.146499.\n",
      "\n",
      "Testing Configuration 136: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:41.155561\n",
      "\n",
      "Testing Configuration 137: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 112 (no improvement in 50 epochs)\n",
      "Training time:  0:00:34.148119\n",
      "\n",
      "Testing Configuration 138: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 154 (no improvement in 50 epochs)\n",
      "Training time:  0:00:47.591524\n",
      "\n",
      "Testing Configuration 139: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 206 (no improvement in 50 epochs)\n",
      "Training time:  0:01:03.530794\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 52, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.144505.\n",
      "\n",
      "Testing Configuration 140: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 72 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.721034\n",
      "\n",
      "Testing Configuration 141: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 89 (no improvement in 50 epochs)\n",
      "Training time:  0:00:25.766395\n",
      "\n",
      "Testing Configuration 142: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 107 (no improvement in 50 epochs)\n",
      "Training time:  0:00:31.176363\n",
      "\n",
      "Testing Configuration 143: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 132 (no improvement in 50 epochs)\n",
      "Training time:  0:00:38.545022\n",
      "\n",
      "Processing Model: OptResNet1D_JARILWWF with params {'input_channels': 52, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.144079.\n",
      "\n",
      "Testing Configuration 144: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.334990\n",
      "\n",
      "Testing Configuration 145: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 213 (no improvement in 50 epochs)\n",
      "Training time:  0:00:30.429208\n",
      "\n",
      "Testing Configuration 146: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 117 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.054351\n",
      "\n",
      "Testing Configuration 147: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.157768\n",
      "\n",
      "Processing Model: OptECAResNet1D_JARILWWF with params {'input_channels': 52, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.143209.\n",
      "\n",
      "Testing Configuration 148: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 131 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.563294\n",
      "\n",
      "Testing Configuration 149: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.550895\n",
      "\n",
      "Testing Configuration 150: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.311026\n",
      "\n",
      "Testing Configuration 151: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 160 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.017267\n",
      "\n",
      "Processing Model: CustomResNet1D with params {'input_channels': 52}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.144506.\n",
      "\n",
      "Testing Configuration 152: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 117 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.208485\n",
      "\n",
      "Testing Configuration 153: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 105 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.009689\n",
      "\n",
      "Testing Configuration 154: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.814006\n",
      "\n",
      "Testing Configuration 155: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:00:24.733364\n",
      "\n",
      "Processing Model: CustomECAResNet1D with params {'input_channels': 52}\n",
      "Started loading data...\n",
      "Train size: 345, Validation size: 62, Test size: 62\n",
      "Train labels distribution: Counter({8: 21, 0: 20, 1: 20, 2: 20, 13: 20, 14: 20, 15: 20, 16: 20, 3: 19, 4: 19, 6: 19, 10: 19, 11: 19, 12: 19, 5: 18, 7: 18, 9: 17, 17: 17})\n",
      "Validation labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Test labels distribution: Counter({0: 4, 3: 4, 4: 4, 6: 4, 8: 4, 10: 4, 11: 4, 12: 4, 1: 3, 2: 3, 5: 3, 7: 3, 9: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3})\n",
      "Done loading data (batch size: 32) in 0:00:00.142014.\n",
      "\n",
      "Testing Configuration 156: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 204 (no improvement in 50 epochs)\n",
      "Training time:  0:00:43.891936\n",
      "\n",
      "Testing Configuration 157: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 91 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.906306\n",
      "\n",
      "Testing Configuration 158: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 109 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.884933\n",
      "\n",
      "Testing Configuration 159: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 130 (no improvement in 50 epochs)\n",
      "Training time:  0:00:28.602364\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
