{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T06:31:40.558055Z",
     "start_time": "2025-05-31T06:31:33.006399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "import models\n",
    "import helper_funtions\n",
    "import preprocess\n",
    "import settings\n",
    "\n",
    "importlib.reload(settings)\n",
    "# importlib.reload(models)\n",
    "importlib.reload(helper_funtions)\n",
    "importlib.reload(preprocess)\n",
    "\n",
    "from models.LSTM import LSTM_HumanFi, CNN_LSTM, CNN_BiLSTM_TemporalAttention, CNN_BiLSTM_ChannelAttention, CNN_BiLSTM_DualAttention, CNN_BiLSTM_Attention\n",
    "from models.RadioNet import RadioNet_NeuralWave\n",
    "from models.ResNet import ECAResNet1D, ECABasicBlock1D, ResNet1D_JARILWWF, OptResNet1D_JARILWWF, OptECAResNet1D_JARILWWF, CustomResNet1D, CustomECAResNet1D\n",
    "from models.TemporalConvNet import TemporalConvNet\n",
    "\n",
    "from helper_funtions import grid_search, run_gridsearch_with_seeds\n",
    "from preprocess import DataPreprocessor\n",
    "from settings import folder_path_5ghz_10hz_collected, folder_path_60ghz_collected, folder_path_5ghz_200hz_collected, folder_path_60ghz_external, output_path"
   ],
   "id": "eaf05c930fb61034",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-31T06:31:40.570489Z",
     "start_time": "2025-05-31T06:31:40.565059Z"
    }
   },
   "source": [
    "input_dim = 30\n",
    "num_epochs = 500\n",
    "\n",
    "param_grid = {\n",
    "    'batch_size': [32],\n",
    "    'learning_rate': [0.0007], # 0.001,\n",
    "    'optimizer': ['adam'],\n",
    "    'mixup_alpha': [0.0, 0.4],\n",
    "    'smoothing_prob': [0.0, 0.5],\n",
    "    'model': [],\n",
    "}\n",
    "\n",
    "hidden_dims = [64, 128] # 32, 256\n",
    "num_layers_list = [1, 2]\n",
    "bidirectional_flags = [False, True]\n",
    "dropouts = [0.2, 0.5]\n",
    "\n",
    "for hidden_dim in hidden_dims:\n",
    "    for num_layers in num_layers_list:\n",
    "        for bidirectional in bidirectional_flags:\n",
    "            for dropout in dropouts:\n",
    "                param_grid['model'].append({\n",
    "                    'model_class': LSTM_HumanFi,\n",
    "                    'model_args': {\n",
    "                        'input_dim': input_dim,\n",
    "                        'hidden_dim': hidden_dim,\n",
    "                        'num_layers': num_layers,\n",
    "                        'bidirectional': bidirectional,\n",
    "                        'dropout': dropout\n",
    "                    },\n",
    "                    'num_epochs': num_epochs\n",
    "                })\n",
    "\n",
    "tcn_channel_options = [\n",
    "    # [32, 64],\n",
    "    [64, 128],\n",
    "    [64, 128, 128],\n",
    "    [64, 128, 256],\n",
    "    # [128, 256, 512],\n",
    "]\n",
    "tcn_kernel_sizes = [2, 3] # 5\n",
    "tcn_dropout_rates = [0.2, 0.5]\n",
    "\n",
    "for channels in tcn_channel_options:\n",
    "    for kernel_size in tcn_kernel_sizes:\n",
    "        for dropout in tcn_dropout_rates:\n",
    "            param_grid['model'].append({\n",
    "                'model_class': TemporalConvNet,\n",
    "                'model_args': {\n",
    "                    'num_inputs': input_dim,\n",
    "                    'num_channels': channels,\n",
    "                    'kernel_size': kernel_size,\n",
    "                    'dropout': dropout\n",
    "                },\n",
    "                'num_epochs': num_epochs\n",
    "            })\n",
    "\n",
    "cnn_lstm_hidden_dims = [64, 128] # 256\n",
    "cnn_lstm_layers = [1, 2]\n",
    "\n",
    "for hidden_dim in cnn_lstm_hidden_dims:\n",
    "    for num_layers in cnn_lstm_layers:\n",
    "        param_grid['model'].append({\n",
    "            'model_class': CNN_BiLSTM_Attention,\n",
    "            'model_args': {\n",
    "                'input_dim': input_dim,\n",
    "                'cnn_filters': 64,\n",
    "                'lstm_units': hidden_dim,\n",
    "                'num_layers': num_layers,\n",
    "            },\n",
    "            'num_epochs': num_epochs\n",
    "        })\n",
    "\n",
    "        param_grid['model'].append({\n",
    "            'model_class': CNN_BiLSTM_TemporalAttention,\n",
    "            'model_args': {\n",
    "                'input_dim': input_dim,\n",
    "                'cnn_channels': 64,\n",
    "                'lstm_hidden_dim': hidden_dim,\n",
    "                'lstm_layers': num_layers,\n",
    "            },\n",
    "            'num_epochs': num_epochs\n",
    "        })\n",
    "\n",
    "        # param_grid['model'].append({\n",
    "        #     'model_class': CNN_BiLSTM_ChannelAttention,\n",
    "        #     'model_args': {\n",
    "        #         'input_dim': input_dim,\n",
    "        #         'cnn_channels': 64,\n",
    "        #         'lstm_hidden_dim': hidden_dim,\n",
    "        #         'lstm_layers': num_layers,\n",
    "        #     },\n",
    "        #     'num_epochs': num_epochs\n",
    "        # })\n",
    "        #\n",
    "        # param_grid['model'].append({\n",
    "        #     'model_class': CNN_BiLSTM_DualAttention,\n",
    "        #     'model_args': {\n",
    "        #         'input_dim': input_dim,\n",
    "        #         'cnn_channels': 64,\n",
    "        #         'lstm_hidden_dim': hidden_dim,\n",
    "        #         'lstm_layers': num_layers,\n",
    "        #     },\n",
    "        #     'num_epochs': num_epochs\n",
    "        # })\n",
    "\n",
    "\n",
    "param_grid['model'] += [\n",
    "    # {\n",
    "    #     'model_class': RadioNet_NeuralWave,\n",
    "    #     'model_args': {'input_dim': input_dim},\n",
    "    #     'num_epochs': num_epochs,\n",
    "    #     'data_preprocessor': DataPreprocessor(target_dim=354)\n",
    "    # },\n",
    "    \n",
    "    # the ones commented below are not optimal (proven by early tests)\n",
    "    # {\n",
    "    #     'model_class': ECAResNet1D,\n",
    "    #     'model_args': {'input_channels': input_dim, 'block': ECABasicBlock1D, 'layers': (1, 1, 1, 1)},\n",
    "    #     'num_epochs': num_epochs\n",
    "    # },\n",
    "    # {\n",
    "    #     'model_class': ECAResNet1D,\n",
    "    #     'model_args': {'input_channels': input_dim, 'block': ECABasicBlock1D, 'layers': (2, 2, 2, 2)},\n",
    "    #     'num_epochs': num_epochs\n",
    "    # },\n",
    "    # {\n",
    "    #     'model_class': ResNet1D_JARILWWF,\n",
    "    #     'model_args': {'input_channels': input_dim, 'layers': [1,1,1,1]},\n",
    "    #     'num_epochs': num_epochs,\n",
    "    # },\n",
    "    # {\n",
    "    #     'model_class': ResNet1D_JARILWWF,\n",
    "    #     'model_args': {'input_channels': input_dim, 'layers': [2,2,2,2]},\n",
    "    #     'num_epochs': num_epochs,\n",
    "    # },\n",
    "    \n",
    "    \n",
    "    {\n",
    "        'model_class': OptResNet1D_JARILWWF,\n",
    "        'model_args': {'input_channels': input_dim, 'layers': [1,1,1,1]},\n",
    "        'num_epochs': num_epochs,\n",
    "    },\n",
    "    # {\n",
    "    #     'model_class': OptResNet1D_JARILWWF,\n",
    "    #     'model_args': {'input_channels': input_dim, 'layers': [2,2,2,2]},\n",
    "    #     'num_epochs': num_epochs,\n",
    "    # },\n",
    "    {\n",
    "        'model_class': OptECAResNet1D_JARILWWF,\n",
    "        'model_args': {'input_channels': input_dim, 'layers': [1,1,1,1]},\n",
    "        'num_epochs': num_epochs,\n",
    "    },\n",
    "    # {\n",
    "    #     'model_class': OptECAResNet1D_JARILWWF,\n",
    "    #     'model_args': {'input_channels': input_dim, 'layers': [2,2,2,2]},\n",
    "    #     'num_epochs': num_epochs,\n",
    "    # },\n",
    "    {\n",
    "        'model_class': CustomResNet1D,\n",
    "        'model_args': {'input_channels': input_dim},\n",
    "        'num_epochs': num_epochs,\n",
    "    },\n",
    "    {\n",
    "        'model_class': CustomECAResNet1D,\n",
    "        'model_args': {'input_channels': input_dim},\n",
    "        'num_epochs': num_epochs,\n",
    "    },\n",
    "] "
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T20:15:49.398031Z",
     "start_time": "2025-05-31T06:31:40.572494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gridsearch_results_df = run_gridsearch_with_seeds(\n",
    "    gridsearch_func=grid_search,\n",
    "    gridsearch_args={\n",
    "        'param_grid': param_grid,\n",
    "        'folder_path': folder_path_60ghz_external,\n",
    "        'background_subtraction': False,\n",
    "        'seconds_per_sample': 5,\n",
    "        'rows_per_second': 22,\n",
    "    },\n",
    "    n_seeds=3,\n",
    "    output_dir=output_path,\n",
    "    filename=\"gridsearch_60ghz_ext\"\n",
    ")"
   ],
   "id": "5a5de08b7ecfded3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running seed 42 ---\n",
      "Running 160 configurations...\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.046050.\n",
      "\n",
      "Testing Configuration 0: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 210 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.338176\n",
      "\n",
      "Testing Configuration 1: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 206 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.908514\n",
      "\n",
      "Testing Configuration 2: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 239 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.523510\n",
      "\n",
      "Testing Configuration 3: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 206 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.614536\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.019500.\n",
      "\n",
      "Testing Configuration 4: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 182 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.822185\n",
      "\n",
      "Testing Configuration 5: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 282 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.259340\n",
      "\n",
      "Testing Configuration 6: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:21.358337\n",
      "\n",
      "Testing Configuration 7: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 482 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.164685\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.017498.\n",
      "\n",
      "Testing Configuration 8: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 217 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.904124\n",
      "\n",
      "Testing Configuration 9: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 147 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.611190\n",
      "\n",
      "Testing Configuration 10: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 264 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.909108\n",
      "\n",
      "Testing Configuration 11: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 334 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.140383\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 12: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 396 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.900605\n",
      "\n",
      "Testing Configuration 13: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 228 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.810146\n",
      "\n",
      "Testing Configuration 14: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 264 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.751576\n",
      "\n",
      "Testing Configuration 15: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 388 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.417008\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.016001.\n",
      "\n",
      "Testing Configuration 16: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 166 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.988670\n",
      "\n",
      "Testing Configuration 17: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 96 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.237298\n",
      "\n",
      "Testing Configuration 18: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 124 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.129271\n",
      "\n",
      "Testing Configuration 19: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.899188\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.018000.\n",
      "\n",
      "Testing Configuration 20: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 93 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.433760\n",
      "\n",
      "Testing Configuration 21: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 170 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.607855\n",
      "\n",
      "Testing Configuration 22: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 138 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.825102\n",
      "\n",
      "Testing Configuration 23: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.703359\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 24: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 129 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.973694\n",
      "\n",
      "Testing Configuration 25: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 70 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.318409\n",
      "\n",
      "Testing Configuration 26: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 220 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.931304\n",
      "\n",
      "Testing Configuration 27: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 220 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.394245\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.017999.\n",
      "\n",
      "Testing Configuration 28: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 65 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.057072\n",
      "\n",
      "Testing Configuration 29: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.150318\n",
      "\n",
      "Testing Configuration 30: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.398139\n",
      "\n",
      "Testing Configuration 31: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.679465\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014500.\n",
      "\n",
      "Testing Configuration 32: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 235 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.846610\n",
      "\n",
      "Testing Configuration 33: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 192 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.487785\n",
      "\n",
      "Testing Configuration 34: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 220 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.091029\n",
      "\n",
      "Testing Configuration 35: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 318 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.475721\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.017000.\n",
      "\n",
      "Testing Configuration 36: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 257 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.439789\n",
      "\n",
      "Testing Configuration 37: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 167 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.675768\n",
      "\n",
      "Testing Configuration 38: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 365 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.615447\n",
      "\n",
      "Testing Configuration 39: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 276 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.896021\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014501.\n",
      "\n",
      "Testing Configuration 40: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 206 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.593462\n",
      "\n",
      "Testing Configuration 41: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 119 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.247973\n",
      "\n",
      "Testing Configuration 42: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 205 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.344626\n",
      "\n",
      "Testing Configuration 43: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 204 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.343480\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.018501.\n",
      "\n",
      "Testing Configuration 44: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 182 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.430139\n",
      "\n",
      "Testing Configuration 45: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 164 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.899545\n",
      "\n",
      "Testing Configuration 46: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 343 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.709677\n",
      "\n",
      "Testing Configuration 47: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 286 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.619974\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.017500.\n",
      "\n",
      "Testing Configuration 48: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 87 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.826094\n",
      "\n",
      "Testing Configuration 49: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 104 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.706598\n",
      "\n",
      "Testing Configuration 50: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 69 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.587717\n",
      "\n",
      "Testing Configuration 51: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.140131\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 52: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 102 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.741129\n",
      "\n",
      "Testing Configuration 53: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 96 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.654806\n",
      "\n",
      "Testing Configuration 54: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 65 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.380459\n",
      "\n",
      "Testing Configuration 55: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 139 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.365890\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.013500.\n",
      "\n",
      "Testing Configuration 56: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 67 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.391498\n",
      "\n",
      "Testing Configuration 57: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.481956\n",
      "\n",
      "Testing Configuration 58: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 155 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.679000\n",
      "\n",
      "Testing Configuration 59: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 67 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.665117\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.053502.\n",
      "\n",
      "Testing Configuration 60: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 87 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.589315\n",
      "\n",
      "Testing Configuration 61: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 145 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.533689\n",
      "\n",
      "Testing Configuration 62: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 134 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.072598\n",
      "\n",
      "Testing Configuration 63: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.570997\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.013999.\n",
      "\n",
      "Testing Configuration 64: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 144 (no improvement in 50 epochs)\n",
      "Training time:  0:02:34.531745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 65: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 85 (no improvement in 50 epochs)\n",
      "Training time:  0:01:33.592077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 66: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 142 (no improvement in 50 epochs)\n",
      "Training time:  0:02:31.455479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 67: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 172 (no improvement in 50 epochs)\n",
      "Training time:  0:03:05.462160\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 68: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:01:23.661953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 69: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 137 (no improvement in 50 epochs)\n",
      "Training time:  0:02:25.986522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 70: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 79 (no improvement in 50 epochs)\n",
      "Training time:  0:01:25.131382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 71: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 105 (no improvement in 50 epochs)\n",
      "Training time:  0:01:51.916483\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 72: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 204 (no improvement in 50 epochs)\n",
      "Training time:  0:03:38.522663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 73: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 130 (no improvement in 50 epochs)\n",
      "Training time:  0:02:19.395494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 74: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 218 (no improvement in 50 epochs)\n",
      "Training time:  0:03:53.820346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 75: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 247 (no improvement in 50 epochs)\n",
      "Training time:  0:04:25.352033\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 76: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 117 (no improvement in 50 epochs)\n",
      "Training time:  0:02:05.040870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 77: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 148 (no improvement in 50 epochs)\n",
      "Training time:  0:02:38.529022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 78: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 84 (no improvement in 50 epochs)\n",
      "Training time:  0:01:30.961749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 79: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 79 (no improvement in 50 epochs)\n",
      "Training time:  0:01:24.813732\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014501.\n",
      "\n",
      "Testing Configuration 80: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:04:02.417006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 81: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 87 (no improvement in 50 epochs)\n",
      "Training time:  0:03:00.518406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 82: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 68 (no improvement in 50 epochs)\n",
      "Training time:  0:02:21.629605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 83: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 145 (no improvement in 50 epochs)\n",
      "Training time:  0:05:03.963742\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.013500.\n",
      "\n",
      "Testing Configuration 84: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:04:21.793994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 85: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 141 (no improvement in 50 epochs)\n",
      "Training time:  0:04:54.888594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 86: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 82 (no improvement in 50 epochs)\n",
      "Training time:  0:02:51.365921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 87: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 89 (no improvement in 50 epochs)\n",
      "Training time:  0:03:06.611208\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 88: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 169 (no improvement in 50 epochs)\n",
      "Training time:  0:05:53.848627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 89: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 191 (no improvement in 50 epochs)\n",
      "Training time:  0:06:39.139889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 90: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 123 (no improvement in 50 epochs)\n",
      "Training time:  0:04:17.471776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 91: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 280 (no improvement in 50 epochs)\n",
      "Training time:  0:09:47.889126\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014500.\n",
      "\n",
      "Testing Configuration 92: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:04:00.743220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 93: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 145 (no improvement in 50 epochs)\n",
      "Training time:  0:05:02.388041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 94: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 101 (no improvement in 50 epochs)\n",
      "Training time:  0:03:31.064992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 95: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 186 (no improvement in 50 epochs)\n",
      "Training time:  0:06:31.680408\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.040500.\n",
      "\n",
      "Testing Configuration 96: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 149 (no improvement in 50 epochs)\n",
      "Training time:  0:07:29.788478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 97: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 236 (no improvement in 50 epochs)\n",
      "Training time:  0:11:52.982251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 98: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 182 (no improvement in 50 epochs)\n",
      "Training time:  0:09:14.507511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 99: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 160 (no improvement in 50 epochs)\n",
      "Training time:  0:08:01.813650\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 100: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 187 (no improvement in 50 epochs)\n",
      "Training time:  0:09:26.663800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 101: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:06:44.332093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 102: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 84 (no improvement in 50 epochs)\n",
      "Training time:  0:04:15.716453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 103: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 76 (no improvement in 50 epochs)\n",
      "Training time:  0:03:51.335222\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 104: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 143 (no improvement in 50 epochs)\n",
      "Training time:  0:07:10.487720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 105: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 146 (no improvement in 50 epochs)\n",
      "Training time:  0:07:21.298224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 106: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 215 (no improvement in 50 epochs)\n",
      "Training time:  0:10:50.748309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 107: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 182 (no improvement in 50 epochs)\n",
      "Training time:  0:09:10.160511\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 108: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:04:25.164759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 109: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 240 (no improvement in 50 epochs)\n",
      "Training time:  0:12:03.576155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 110: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 123 (no improvement in 50 epochs)\n",
      "Training time:  0:06:10.957155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 111: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:05:10.448247\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.013954.\n",
      "\n",
      "Testing Configuration 112: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 112 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.626848\n",
      "\n",
      "Testing Configuration 113: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 412 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.669351\n",
      "\n",
      "Testing Configuration 114: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 237 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.419261\n",
      "\n",
      "Testing Configuration 115: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 319 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.258224\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 116: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 178 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.447972\n",
      "\n",
      "Testing Configuration 117: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 121 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.701361\n",
      "\n",
      "Testing Configuration 118: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 154 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.892372\n",
      "\n",
      "Testing Configuration 119: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.969342\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 120: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 210 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.066594\n",
      "\n",
      "Testing Configuration 121: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 278 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.777334\n",
      "\n",
      "Testing Configuration 122: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 163 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.771344\n",
      "\n",
      "Testing Configuration 123: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 206 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.398084\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014499.\n",
      "\n",
      "Testing Configuration 124: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 73 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.681391\n",
      "\n",
      "Testing Configuration 125: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.346189\n",
      "\n",
      "Testing Configuration 126: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 106 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.393570\n",
      "\n",
      "Testing Configuration 127: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 169 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.559140\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 128: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 170 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.535883\n",
      "\n",
      "Testing Configuration 129: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 173 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.785042\n",
      "\n",
      "Testing Configuration 130: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 202 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.913479\n",
      "\n",
      "Testing Configuration 131: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 243 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.791498\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 132: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 143 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.188343\n",
      "\n",
      "Testing Configuration 133: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 108 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.277464\n",
      "\n",
      "Testing Configuration 134: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.641013\n",
      "\n",
      "Testing Configuration 135: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.831336\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.013500.\n",
      "\n",
      "Testing Configuration 136: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 160 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.133771\n",
      "\n",
      "Testing Configuration 137: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 155 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.093555\n",
      "\n",
      "Testing Configuration 138: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 175 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.947898\n",
      "\n",
      "Testing Configuration 139: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 352 (no improvement in 50 epochs)\n",
      "Training time:  0:00:26.498419\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.013500.\n",
      "\n",
      "Testing Configuration 140: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 174 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.990910\n",
      "\n",
      "Testing Configuration 141: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 154 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.692845\n",
      "\n",
      "Testing Configuration 142: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 162 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.614988\n",
      "\n",
      "Testing Configuration 143: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 115 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.263917\n",
      "\n",
      "Processing Model: OptResNet1D_JARILWWF with params {'input_channels': 30, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014498.\n",
      "\n",
      "Testing Configuration 144: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 174 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.357446\n",
      "\n",
      "Testing Configuration 145: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 110 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.796428\n",
      "\n",
      "Testing Configuration 146: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 96 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.088586\n",
      "\n",
      "Testing Configuration 147: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.863466\n",
      "\n",
      "Processing Model: OptECAResNet1D_JARILWWF with params {'input_channels': 30, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014500.\n",
      "\n",
      "Testing Configuration 148: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 226 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.729743\n",
      "\n",
      "Testing Configuration 149: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.699768\n",
      "\n",
      "Testing Configuration 150: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 135 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.900786\n",
      "\n",
      "Testing Configuration 151: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 97 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.324112\n",
      "\n",
      "Processing Model: CustomResNet1D with params {'input_channels': 30}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.017999.\n",
      "\n",
      "Testing Configuration 152: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 105 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.303110\n",
      "\n",
      "Testing Configuration 153: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 175 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.215786\n",
      "\n",
      "Testing Configuration 154: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 79 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.601148\n",
      "\n",
      "Testing Configuration 155: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 102 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.485206\n",
      "\n",
      "Processing Model: CustomECAResNet1D with params {'input_channels': 30}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.015500.\n",
      "\n",
      "Testing Configuration 156: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 124 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.724837\n",
      "\n",
      "Testing Configuration 157: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 160 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.756608\n",
      "\n",
      "Testing Configuration 158: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 155 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.367606\n",
      "\n",
      "Testing Configuration 159: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.483734\n",
      "\n",
      "--- Running seed 420 ---\n",
      "Running 160 configurations...\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.017500.\n",
      "\n",
      "Testing Configuration 0: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.880321\n",
      "\n",
      "Testing Configuration 1: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 243 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.273553\n",
      "\n",
      "Testing Configuration 2: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 195 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.430074\n",
      "\n",
      "Testing Configuration 3: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 310 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.262644\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014001.\n",
      "\n",
      "Testing Configuration 4: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 200 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.512297\n",
      "\n",
      "Testing Configuration 5: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 267 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.736248\n",
      "\n",
      "Testing Configuration 6: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 415 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.230164\n",
      "\n",
      "Testing Configuration 7: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 220 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.759984\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014500.\n",
      "\n",
      "Testing Configuration 8: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 211 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.020909\n",
      "\n",
      "Testing Configuration 9: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 141 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.592083\n",
      "\n",
      "Testing Configuration 10: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 259 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.082572\n",
      "\n",
      "Testing Configuration 11: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 255 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.300441\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.018500.\n",
      "\n",
      "Testing Configuration 12: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 306 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.576051\n",
      "\n",
      "Testing Configuration 13: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 250 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.787896\n",
      "\n",
      "Testing Configuration 14: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 252 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.782330\n",
      "\n",
      "Testing Configuration 15: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 259 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.591830\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.017500.\n",
      "\n",
      "Testing Configuration 16: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 203 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.711180\n",
      "\n",
      "Testing Configuration 17: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 61 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.745009\n",
      "\n",
      "Testing Configuration 18: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 85 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.284074\n",
      "\n",
      "Testing Configuration 19: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 128 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.507574\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.015000.\n",
      "\n",
      "Testing Configuration 20: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.204071\n",
      "\n",
      "Testing Configuration 21: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 68 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.641444\n",
      "\n",
      "Testing Configuration 22: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 112 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.798372\n",
      "\n",
      "Testing Configuration 23: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.773372\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014502.\n",
      "\n",
      "Testing Configuration 24: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 171 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.706494\n",
      "\n",
      "Testing Configuration 25: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 69 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.258010\n",
      "\n",
      "Testing Configuration 26: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.146030\n",
      "\n",
      "Testing Configuration 27: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 135 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.433596\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014500.\n",
      "\n",
      "Testing Configuration 28: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 132 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.005569\n",
      "\n",
      "Testing Configuration 29: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 146 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.113982\n",
      "\n",
      "Testing Configuration 30: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 203 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.031445\n",
      "\n",
      "Testing Configuration 31: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 78 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.823430\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 32: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 272 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.084106\n",
      "\n",
      "Testing Configuration 33: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 158 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.359284\n",
      "\n",
      "Testing Configuration 34: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.575326\n",
      "\n",
      "Testing Configuration 35: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 309 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.685553\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014001.\n",
      "\n",
      "Testing Configuration 36: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 280 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.603423\n",
      "\n",
      "Testing Configuration 37: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 198 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.686703\n",
      "\n",
      "Testing Configuration 38: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Training time:  0:00:21.732506\n",
      "\n",
      "Testing Configuration 39: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 244 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.656548\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.018000.\n",
      "\n",
      "Testing Configuration 40: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 180 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.638742\n",
      "\n",
      "Testing Configuration 41: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 167 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.289740\n",
      "\n",
      "Testing Configuration 42: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 167 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.811641\n",
      "\n",
      "Testing Configuration 43: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 285 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.559550\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.018501.\n",
      "\n",
      "Testing Configuration 44: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 249 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.378497\n",
      "\n",
      "Testing Configuration 45: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 267 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.881184\n",
      "\n",
      "Testing Configuration 46: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 296 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.296721\n",
      "\n",
      "Testing Configuration 47: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 244 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.812588\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.015000.\n",
      "\n",
      "Testing Configuration 48: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 140 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.079704\n",
      "\n",
      "Testing Configuration 49: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.798779\n",
      "\n",
      "Testing Configuration 50: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 131 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.179112\n",
      "\n",
      "Testing Configuration 51: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 61 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.983620\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.018000.\n",
      "\n",
      "Testing Configuration 52: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 63 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.799123\n",
      "\n",
      "Testing Configuration 53: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.455725\n",
      "\n",
      "Testing Configuration 54: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 123 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.777375\n",
      "\n",
      "Testing Configuration 55: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.702741\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.015500.\n",
      "\n",
      "Testing Configuration 56: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 80 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.139564\n",
      "\n",
      "Testing Configuration 57: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.484442\n",
      "\n",
      "Testing Configuration 58: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.340762\n",
      "\n",
      "Testing Configuration 59: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.400339\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.015000.\n",
      "\n",
      "Testing Configuration 60: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.953494\n",
      "\n",
      "Testing Configuration 61: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 89 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.796135\n",
      "\n",
      "Testing Configuration 62: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 129 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.156473\n",
      "\n",
      "Testing Configuration 63: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.703426\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 64: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 119 (no improvement in 50 epochs)\n",
      "Training time:  0:02:06.716598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 65: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:02:04.162397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 66: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 129 (no improvement in 50 epochs)\n",
      "Training time:  0:02:16.570903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 67: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 89 (no improvement in 50 epochs)\n",
      "Training time:  0:01:35.629522\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.013501.\n",
      "\n",
      "Testing Configuration 68: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:02:03.443498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 69: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 134 (no improvement in 50 epochs)\n",
      "Training time:  0:02:27.457364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 70: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 101 (no improvement in 50 epochs)\n",
      "Training time:  0:01:47.702268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 71: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:02:21.998908\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014001.\n",
      "\n",
      "Testing Configuration 72: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:02:29.809695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 73: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:02:24.215533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 74: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 132 (no improvement in 50 epochs)\n",
      "Training time:  0:02:21.539339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 75: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 202 (no improvement in 50 epochs)\n",
      "Training time:  0:03:36.620665\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.018000.\n",
      "\n",
      "Testing Configuration 76: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 74 (no improvement in 50 epochs)\n",
      "Training time:  0:01:18.749837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 77: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 158 (no improvement in 50 epochs)\n",
      "Training time:  0:02:52.751406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 78: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 74 (no improvement in 50 epochs)\n",
      "Training time:  0:01:19.689608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 79: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 90 (no improvement in 50 epochs)\n",
      "Training time:  0:01:36.184297\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.018000.\n",
      "\n",
      "Testing Configuration 80: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:05:13.053729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 81: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 121 (no improvement in 50 epochs)\n",
      "Training time:  0:04:12.882019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 82: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:05:09.909018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 83: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 127 (no improvement in 50 epochs)\n",
      "Training time:  0:04:24.687089\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.018501.\n",
      "\n",
      "Testing Configuration 84: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 72 (no improvement in 50 epochs)\n",
      "Training time:  0:02:31.758157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 85: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 89 (no improvement in 50 epochs)\n",
      "Training time:  0:03:04.355962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 86: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 104 (no improvement in 50 epochs)\n",
      "Training time:  0:03:36.256285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 87: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 82 (no improvement in 50 epochs)\n",
      "Training time:  0:02:52.478940\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.017500.\n",
      "\n",
      "Testing Configuration 88: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:03:57.821205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 89: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 174 (no improvement in 50 epochs)\n",
      "Training time:  0:06:04.219286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 90: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:04:02.278206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 91: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 109 (no improvement in 50 epochs)\n",
      "Training time:  0:03:47.322880\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.018001.\n",
      "\n",
      "Testing Configuration 92: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 187 (no improvement in 50 epochs)\n",
      "Training time:  0:06:43.104006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 93: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:02:59.207886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 94: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:02:54.459378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 95: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:03:05.618634\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.018500.\n",
      "\n",
      "Testing Configuration 96: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 157 (no improvement in 50 epochs)\n",
      "Training time:  0:07:55.359783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 97: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:04:12.463790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 98: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 160 (no improvement in 50 epochs)\n",
      "Training time:  0:08:04.965924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 99: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 233 (no improvement in 50 epochs)\n",
      "Training time:  0:11:44.662850\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.017500.\n",
      "\n",
      "Testing Configuration 100: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 112 (no improvement in 50 epochs)\n",
      "Training time:  0:05:37.906727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 101: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 132 (no improvement in 50 epochs)\n",
      "Training time:  0:06:39.699913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 102: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 110 (no improvement in 50 epochs)\n",
      "Training time:  0:05:33.211328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 103: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 84 (no improvement in 50 epochs)\n",
      "Training time:  0:04:14.762964\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 104: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 219 (no improvement in 50 epochs)\n",
      "Training time:  0:11:00.068433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 105: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 91 (no improvement in 50 epochs)\n",
      "Training time:  0:04:35.828465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 106: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 142 (no improvement in 50 epochs)\n",
      "Training time:  0:07:09.932379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 107: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 190 (no improvement in 50 epochs)\n",
      "Training time:  0:09:36.040380\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 108: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 167 (no improvement in 50 epochs)\n",
      "Training time:  0:08:25.487109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 109: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 118 (no improvement in 50 epochs)\n",
      "Training time:  0:05:56.162449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 110: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 242 (no improvement in 50 epochs)\n",
      "Training time:  0:12:10.098589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 111: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 385 (no improvement in 50 epochs)\n",
      "Training time:  0:19:26.843734\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014500.\n",
      "\n",
      "Testing Configuration 112: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 123 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.287786\n",
      "\n",
      "Testing Configuration 113: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 149 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.235563\n",
      "\n",
      "Testing Configuration 114: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 158 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.227045\n",
      "\n",
      "Testing Configuration 115: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 142 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.462361\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 116: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.254125\n",
      "\n",
      "Testing Configuration 117: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 112 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.344420\n",
      "\n",
      "Testing Configuration 118: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 203 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.353743\n",
      "\n",
      "Testing Configuration 119: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 135 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.044079\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014500.\n",
      "\n",
      "Testing Configuration 120: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 306 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.353930\n",
      "\n",
      "Testing Configuration 121: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 237 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.722591\n",
      "\n",
      "Testing Configuration 122: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 146 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.639609\n",
      "\n",
      "Testing Configuration 123: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 312 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.971936\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014500.\n",
      "\n",
      "Testing Configuration 124: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 154 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.888051\n",
      "\n",
      "Testing Configuration 125: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 209 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.969257\n",
      "\n",
      "Testing Configuration 126: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 179 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.683430\n",
      "\n",
      "Testing Configuration 127: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 212 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.820614\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014228.\n",
      "\n",
      "Testing Configuration 128: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 159 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.010317\n",
      "\n",
      "Testing Configuration 129: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 113 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.561360\n",
      "\n",
      "Testing Configuration 130: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 132 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.007133\n",
      "\n",
      "Testing Configuration 131: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 168 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.201307\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.018500.\n",
      "\n",
      "Testing Configuration 132: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 128 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.338312\n",
      "\n",
      "Testing Configuration 133: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.737620\n",
      "\n",
      "Testing Configuration 134: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 259 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.071519\n",
      "\n",
      "Testing Configuration 135: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 192 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.090665\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.018500.\n",
      "\n",
      "Testing Configuration 136: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 101 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.502379\n",
      "\n",
      "Testing Configuration 137: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 169 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.777392\n",
      "\n",
      "Testing Configuration 138: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 196 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.559480\n",
      "\n",
      "Testing Configuration 139: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 152 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.674117\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.018001.\n",
      "\n",
      "Testing Configuration 140: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 174 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.043671\n",
      "\n",
      "Testing Configuration 141: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 131 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.220367\n",
      "\n",
      "Testing Configuration 142: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 110 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.017106\n",
      "\n",
      "Testing Configuration 143: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 87 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.444434\n",
      "\n",
      "Processing Model: OptResNet1D_JARILWWF with params {'input_channels': 30, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.016500.\n",
      "\n",
      "Testing Configuration 144: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 183 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.943532\n",
      "\n",
      "Testing Configuration 145: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 76 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.470669\n",
      "\n",
      "Testing Configuration 146: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 187 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.192469\n",
      "\n",
      "Testing Configuration 147: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 87 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.707157\n",
      "\n",
      "Processing Model: OptECAResNet1D_JARILWWF with params {'input_channels': 30, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.015500.\n",
      "\n",
      "Testing Configuration 148: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 156 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.239049\n",
      "\n",
      "Testing Configuration 149: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.834402\n",
      "\n",
      "Testing Configuration 150: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 124 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.103884\n",
      "\n",
      "Testing Configuration 151: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.687029\n",
      "\n",
      "Processing Model: CustomResNet1D with params {'input_channels': 30}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.015500.\n",
      "\n",
      "Testing Configuration 152: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.005778\n",
      "\n",
      "Testing Configuration 153: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 128 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.865590\n",
      "\n",
      "Testing Configuration 154: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.841805\n",
      "\n",
      "Testing Configuration 155: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 97 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.702009\n",
      "\n",
      "Processing Model: CustomECAResNet1D with params {'input_channels': 30}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.018501.\n",
      "\n",
      "Testing Configuration 156: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.589903\n",
      "\n",
      "Testing Configuration 157: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 131 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.173020\n",
      "\n",
      "Testing Configuration 158: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 113 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.540302\n",
      "\n",
      "Testing Configuration 159: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.890613\n",
      "\n",
      "--- Running seed 101010 ---\n",
      "Running 160 configurations...\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.015499.\n",
      "\n",
      "Testing Configuration 0: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 311 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.684504\n",
      "\n",
      "Testing Configuration 1: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 206 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.514313\n",
      "\n",
      "Testing Configuration 2: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 286 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.464875\n",
      "\n",
      "Testing Configuration 3: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 260 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.047401\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 4: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 318 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.587769\n",
      "\n",
      "Testing Configuration 5: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 394 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.433589\n",
      "\n",
      "Testing Configuration 6: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 452 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.517321\n",
      "\n",
      "Testing Configuration 7: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 441 (no improvement in 50 epochs)\n",
      "Training time:  0:00:20.268085\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.013502.\n",
      "\n",
      "Testing Configuration 8: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 220 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.801517\n",
      "\n",
      "Testing Configuration 9: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 149 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.804018\n",
      "\n",
      "Testing Configuration 10: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 309 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.862265\n",
      "\n",
      "Testing Configuration 11: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 247 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.959705\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.015000.\n",
      "\n",
      "Testing Configuration 12: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 232 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.907579\n",
      "\n",
      "Testing Configuration 13: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 405 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.044753\n",
      "\n",
      "Testing Configuration 14: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 255 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.828570\n",
      "\n",
      "Testing Configuration 15: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 288 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.035714\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014500.\n",
      "\n",
      "Testing Configuration 16: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.688441\n",
      "\n",
      "Testing Configuration 17: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 125 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.465439\n",
      "\n",
      "Testing Configuration 18: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 71 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.382314\n",
      "\n",
      "Testing Configuration 19: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.383376\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.013500.\n",
      "\n",
      "Testing Configuration 20: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 188 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.697769\n",
      "\n",
      "Testing Configuration 21: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 110 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.301357\n",
      "\n",
      "Testing Configuration 22: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.332416\n",
      "\n",
      "Testing Configuration 23: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 84 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.698243\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.013502.\n",
      "\n",
      "Testing Configuration 24: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 93 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.255657\n",
      "\n",
      "Testing Configuration 25: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.100176\n",
      "\n",
      "Testing Configuration 26: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 219 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.917742\n",
      "\n",
      "Testing Configuration 27: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 302 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.947043\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.018499.\n",
      "\n",
      "Testing Configuration 28: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.201740\n",
      "\n",
      "Testing Configuration 29: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 90 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.227568\n",
      "\n",
      "Testing Configuration 30: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 113 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.548943\n",
      "\n",
      "Testing Configuration 31: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 213 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.221544\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.018502.\n",
      "\n",
      "Testing Configuration 32: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 189 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.157714\n",
      "\n",
      "Testing Configuration 33: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 205 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.211262\n",
      "\n",
      "Testing Configuration 34: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 300 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.744523\n",
      "\n",
      "Testing Configuration 35: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 303 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.806891\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014001.\n",
      "\n",
      "Testing Configuration 36: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 148 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.790984\n",
      "\n",
      "Testing Configuration 37: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 202 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.035515\n",
      "\n",
      "Testing Configuration 38: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 464 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.261687\n",
      "\n",
      "Testing Configuration 39: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 260 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.286272\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.018500.\n",
      "\n",
      "Testing Configuration 40: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 196 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.077974\n",
      "\n",
      "Testing Configuration 41: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 160 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.771702\n",
      "\n",
      "Testing Configuration 42: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 193 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.982033\n",
      "\n",
      "Testing Configuration 43: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 204 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.672058\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.018456.\n",
      "\n",
      "Testing Configuration 44: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 154 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.491254\n",
      "\n",
      "Testing Configuration 45: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 153 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.902734\n",
      "\n",
      "Testing Configuration 46: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 284 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.624649\n",
      "\n",
      "Testing Configuration 47: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 298 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.849001\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.013500.\n",
      "\n",
      "Testing Configuration 48: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 89 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.201856\n",
      "\n",
      "Testing Configuration 49: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 66 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.177339\n",
      "\n",
      "Testing Configuration 50: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 73 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.760680\n",
      "\n",
      "Testing Configuration 51: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.097243\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.015499.\n",
      "\n",
      "Testing Configuration 52: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.987118\n",
      "\n",
      "Testing Configuration 53: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 203 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.243087\n",
      "\n",
      "Testing Configuration 54: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 82 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.967228\n",
      "\n",
      "Testing Configuration 55: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 96 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.706615\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.013502.\n",
      "\n",
      "Testing Configuration 56: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 148 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.651474\n",
      "\n",
      "Testing Configuration 57: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.117260\n",
      "\n",
      "Testing Configuration 58: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 179 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.235766\n",
      "\n",
      "Testing Configuration 59: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 79 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.474564\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.019000.\n",
      "\n",
      "Testing Configuration 60: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 54 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.504097\n",
      "\n",
      "Testing Configuration 61: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 53 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.471888\n",
      "\n",
      "Testing Configuration 62: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 182 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.071243\n",
      "\n",
      "Testing Configuration 63: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 210 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.668097\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.013999.\n",
      "\n",
      "Testing Configuration 64: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 67 (no improvement in 50 epochs)\n",
      "Training time:  0:01:11.227874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 65: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:01:42.000942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 66: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 315 (no improvement in 50 epochs)\n",
      "Training time:  0:05:37.624257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 67: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 164 (no improvement in 50 epochs)\n",
      "Training time:  0:02:54.647750\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.017000.\n",
      "\n",
      "Testing Configuration 68: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:02:22.329761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 69: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:01:58.912337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 70: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:01:34.060388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 71: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:01:31.523173\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.017500.\n",
      "\n",
      "Testing Configuration 72: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:02:27.275051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 73: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 129 (no improvement in 50 epochs)\n",
      "Training time:  0:02:18.150442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 74: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 117 (no improvement in 50 epochs)\n",
      "Training time:  0:02:07.122572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 75: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 142 (no improvement in 50 epochs)\n",
      "Training time:  0:02:32.222368\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.016500.\n",
      "\n",
      "Testing Configuration 76: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 91 (no improvement in 50 epochs)\n",
      "Training time:  0:01:38.248282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 77: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:02:09.603414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 78: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:01:50.224571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 79: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 82 (no improvement in 50 epochs)\n",
      "Training time:  0:01:28.470706\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.017500.\n",
      "\n",
      "Testing Configuration 80: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:04:23.812442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 81: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:02:41.604077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 82: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:06:08.207509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 83: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 123 (no improvement in 50 epochs)\n",
      "Training time:  0:04:17.980962\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.017498.\n",
      "\n",
      "Testing Configuration 84: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 138 (no improvement in 50 epochs)\n",
      "Training time:  0:04:48.525136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 85: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 85 (no improvement in 50 epochs)\n",
      "Training time:  0:02:56.468546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 86: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 69 (no improvement in 50 epochs)\n",
      "Training time:  0:02:23.485943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 87: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 158 (no improvement in 50 epochs)\n",
      "Training time:  0:05:32.631072\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.017961.\n",
      "\n",
      "Testing Configuration 88: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 117 (no improvement in 50 epochs)\n",
      "Training time:  0:04:03.524188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 89: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:02:53.288272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 90: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 198 (no improvement in 50 epochs)\n",
      "Training time:  0:07:32.624465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 91: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 198 (no improvement in 50 epochs)\n",
      "Training time:  0:07:37.380289\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.017501.\n",
      "\n",
      "Testing Configuration 92: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:03:59.834784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 93: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 335 (no improvement in 50 epochs)\n",
      "Training time:  0:12:37.504289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 94: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 107 (no improvement in 50 epochs)\n",
      "Training time:  0:03:40.911584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 95: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 66 (no improvement in 50 epochs)\n",
      "Training time:  0:02:16.995354\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014000.\n",
      "\n",
      "Testing Configuration 96: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 121 (no improvement in 50 epochs)\n",
      "Training time:  0:06:07.256738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 97: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:07:40.144917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 98: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 312 (no improvement in 50 epochs)\n",
      "Training time:  0:17:32.391488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 99: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 183 (no improvement in 50 epochs)\n",
      "Training time:  0:09:57.309676\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014012.\n",
      "\n",
      "Testing Configuration 100: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 259 (no improvement in 50 epochs)\n",
      "Training time:  0:13:34.974304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 101: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 96 (no improvement in 50 epochs)\n",
      "Training time:  0:05:02.739622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 102: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 398 (no improvement in 50 epochs)\n",
      "Training time:  0:20:58.630870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 103: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:07:00.961365\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.016553.\n",
      "\n",
      "Testing Configuration 104: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 219 (no improvement in 50 epochs)\n",
      "Training time:  0:11:33.962965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 105: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:07:48.981375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 106: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 143 (no improvement in 50 epochs)\n",
      "Training time:  0:08:26.916020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 107: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 252 (no improvement in 50 epochs)\n",
      "Training time:  0:14:49.294825\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014514.\n",
      "\n",
      "Testing Configuration 108: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 250 (no improvement in 50 epochs)\n",
      "Training time:  0:14:42.795313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 109: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 177 (no improvement in 50 epochs)\n",
      "Training time:  0:10:24.177365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 110: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 308 (no improvement in 50 epochs)\n",
      "Training time:  0:18:06.480295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 111: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 233 (no improvement in 50 epochs)\n",
      "Training time:  0:12:29.722668\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.016004.\n",
      "\n",
      "Testing Configuration 112: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 108 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.981529\n",
      "\n",
      "Testing Configuration 113: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.603872\n",
      "\n",
      "Testing Configuration 114: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 295 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.254914\n",
      "\n",
      "Testing Configuration 115: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 211 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.164329\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014014.\n",
      "\n",
      "Testing Configuration 116: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.272515\n",
      "\n",
      "Testing Configuration 117: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 187 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.059025\n",
      "\n",
      "Testing Configuration 118: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 209 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.790632\n",
      "\n",
      "Testing Configuration 119: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 239 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.040936\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014009.\n",
      "\n",
      "Testing Configuration 120: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 71 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.979334\n",
      "\n",
      "Testing Configuration 121: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 238 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.297864\n",
      "\n",
      "Testing Configuration 122: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 212 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.900049\n",
      "\n",
      "Testing Configuration 123: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 186 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.036634\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014509.\n",
      "\n",
      "Testing Configuration 124: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 177 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.450115\n",
      "\n",
      "Testing Configuration 125: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 186 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.311834\n",
      "\n",
      "Testing Configuration 126: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 181 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.446468\n",
      "\n",
      "Testing Configuration 127: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.251633\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014509.\n",
      "\n",
      "Testing Configuration 128: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 165 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.897534\n",
      "\n",
      "Testing Configuration 129: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 110 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.391943\n",
      "\n",
      "Testing Configuration 130: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 188 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.097397\n",
      "\n",
      "Testing Configuration 131: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 254 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.099242\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014005.\n",
      "\n",
      "Testing Configuration 132: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 91 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.801104\n",
      "\n",
      "Testing Configuration 133: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 104 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.147500\n",
      "\n",
      "Testing Configuration 134: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 87 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.069631\n",
      "\n",
      "Testing Configuration 135: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 140 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.720203\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014513.\n",
      "\n",
      "Testing Configuration 136: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 153 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.480705\n",
      "\n",
      "Testing Configuration 137: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 132 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.369042\n",
      "\n",
      "Testing Configuration 138: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 201 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.234103\n",
      "\n",
      "Testing Configuration 139: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 231 (no improvement in 50 epochs)\n",
      "Training time:  0:00:18.810280\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014508.\n",
      "\n",
      "Testing Configuration 140: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 118 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.957918\n",
      "\n",
      "Testing Configuration 141: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 134 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.301165\n",
      "\n",
      "Testing Configuration 142: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 99 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.713484\n",
      "\n",
      "Testing Configuration 143: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 161 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.164998\n",
      "\n",
      "Processing Model: OptResNet1D_JARILWWF with params {'input_channels': 30, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014519.\n",
      "\n",
      "Testing Configuration 144: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 275 (no improvement in 50 epochs)\n",
      "Training time:  0:00:23.533401\n",
      "\n",
      "Testing Configuration 145: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 107 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.940813\n",
      "\n",
      "Testing Configuration 146: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 147 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.953594\n",
      "\n",
      "Testing Configuration 147: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 157 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.349959\n",
      "\n",
      "Processing Model: OptECAResNet1D_JARILWWF with params {'input_channels': 30, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014511.\n",
      "\n",
      "Testing Configuration 148: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.072632\n",
      "\n",
      "Testing Configuration 149: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 165 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.010278\n",
      "\n",
      "Testing Configuration 150: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 85 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.648497\n",
      "\n",
      "Testing Configuration 151: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 189 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.297884\n",
      "\n",
      "Processing Model: CustomResNet1D with params {'input_channels': 30}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014507.\n",
      "\n",
      "Testing Configuration 152: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 65 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.422916\n",
      "\n",
      "Testing Configuration 153: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 79 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.915232\n",
      "\n",
      "Testing Configuration 154: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 65 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.695876\n",
      "\n",
      "Testing Configuration 155: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 246 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.454925\n",
      "\n",
      "Processing Model: CustomECAResNet1D with params {'input_channels': 30}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.014008.\n",
      "\n",
      "Testing Configuration 156: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 182 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.443027\n",
      "\n",
      "Testing Configuration 157: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 95 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.885154\n",
      "\n",
      "Testing Configuration 158: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.759218\n",
      "\n",
      "Testing Configuration 159: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 171 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.703415\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T06:22:45.420388Z",
     "start_time": "2025-05-31T20:15:49.763850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gridsearch_results_df_bgsub = run_gridsearch_with_seeds(\n",
    "    gridsearch_func=grid_search,\n",
    "    gridsearch_args={\n",
    "        'param_grid': param_grid,\n",
    "        'folder_path': folder_path_60ghz_external,\n",
    "        'background_subtraction': True,\n",
    "        'seconds_per_sample': 5,\n",
    "        'rows_per_second': 22,\n",
    "    },\n",
    "    n_seeds=3,\n",
    "    output_dir=output_path,\n",
    "    filename=\"gridsearch_60ghz_ext\"\n",
    ")"
   ],
   "id": "9472764990254241",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running seed 42 ---\n",
      "Running 160 configurations...\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.047396.\n",
      "\n",
      "Testing Configuration 0: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 175 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.714951\n",
      "\n",
      "Testing Configuration 1: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.357946\n",
      "\n",
      "Testing Configuration 2: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 188 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.423763\n",
      "\n",
      "Testing Configuration 3: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 180 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.493434\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.027080.\n",
      "\n",
      "Testing Configuration 4: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 208 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.298379\n",
      "\n",
      "Testing Configuration 5: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 127 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.788466\n",
      "\n",
      "Testing Configuration 6: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 255 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.529641\n",
      "\n",
      "Testing Configuration 7: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 205 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.685436\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.028016.\n",
      "\n",
      "Testing Configuration 8: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 89 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.618844\n",
      "\n",
      "Testing Configuration 9: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 120 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.043065\n",
      "\n",
      "Testing Configuration 10: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 124 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.660110\n",
      "\n",
      "Testing Configuration 11: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 105 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.894101\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.025048.\n",
      "\n",
      "Testing Configuration 12: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 164 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.620099\n",
      "\n",
      "Testing Configuration 13: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 105 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.397338\n",
      "\n",
      "Testing Configuration 14: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 223 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.170020\n",
      "\n",
      "Testing Configuration 15: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 159 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.358238\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.027016.\n",
      "\n",
      "Testing Configuration 16: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 113 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.366315\n",
      "\n",
      "Testing Configuration 17: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 93 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.686330\n",
      "\n",
      "Testing Configuration 18: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 142 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.121265\n",
      "\n",
      "Testing Configuration 19: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 270 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.320532\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024518.\n",
      "\n",
      "Testing Configuration 20: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 142 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.439292\n",
      "\n",
      "Testing Configuration 21: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.851511\n",
      "\n",
      "Testing Configuration 22: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 102 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.480010\n",
      "\n",
      "Testing Configuration 23: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.771511\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024512.\n",
      "\n",
      "Testing Configuration 24: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 78 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.711663\n",
      "\n",
      "Testing Configuration 25: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.615828\n",
      "\n",
      "Testing Configuration 26: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 91 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.764722\n",
      "\n",
      "Testing Configuration 27: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 148 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.032156\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024526.\n",
      "\n",
      "Testing Configuration 28: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.451774\n",
      "\n",
      "Testing Configuration 29: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.390987\n",
      "\n",
      "Testing Configuration 30: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 130 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.743080\n",
      "\n",
      "Testing Configuration 31: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 106 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.749034\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023511.\n",
      "\n",
      "Testing Configuration 32: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 101 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.492738\n",
      "\n",
      "Testing Configuration 33: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 96 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.452623\n",
      "\n",
      "Testing Configuration 34: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 163 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.411460\n",
      "\n",
      "Testing Configuration 35: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 143 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.870346\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023014.\n",
      "\n",
      "Testing Configuration 36: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 130 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.482559\n",
      "\n",
      "Testing Configuration 37: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 153 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.493752\n",
      "\n",
      "Testing Configuration 38: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 123 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.721613\n",
      "\n",
      "Testing Configuration 39: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 148 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.967859\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024017.\n",
      "\n",
      "Testing Configuration 40: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 71 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.143381\n",
      "\n",
      "Testing Configuration 41: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 68 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.121836\n",
      "\n",
      "Testing Configuration 42: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 156 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.590696\n",
      "\n",
      "Testing Configuration 43: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 93 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.726199\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023515.\n",
      "\n",
      "Testing Configuration 44: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 81 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.588707\n",
      "\n",
      "Testing Configuration 45: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 95 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.436259\n",
      "\n",
      "Testing Configuration 46: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 302 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.296100\n",
      "\n",
      "Testing Configuration 47: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 226 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.662255\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024016.\n",
      "\n",
      "Testing Configuration 48: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 59 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.674974\n",
      "\n",
      "Testing Configuration 49: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 76 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.571968\n",
      "\n",
      "Testing Configuration 50: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 139 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.952980\n",
      "\n",
      "Testing Configuration 51: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 140 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.236787\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024013.\n",
      "\n",
      "Testing Configuration 52: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 69 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.152773\n",
      "\n",
      "Testing Configuration 53: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.314610\n",
      "\n",
      "Testing Configuration 54: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.106437\n",
      "\n",
      "Testing Configuration 55: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 192 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.757855\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024040.\n",
      "\n",
      "Testing Configuration 56: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 59 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.121107\n",
      "\n",
      "Testing Configuration 57: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 69 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.996786\n",
      "\n",
      "Testing Configuration 58: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 81 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.937891\n",
      "\n",
      "Testing Configuration 59: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 59 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.338742\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024019.\n",
      "\n",
      "Testing Configuration 60: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 69 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.663850\n",
      "\n",
      "Testing Configuration 61: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.890152\n",
      "\n",
      "Testing Configuration 62: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 191 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.776218\n",
      "\n",
      "Testing Configuration 63: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 189 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.172291\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024010.\n",
      "\n",
      "Testing Configuration 64: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 70 (no improvement in 50 epochs)\n",
      "Training time:  0:01:27.000312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 65: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 81 (no improvement in 50 epochs)\n",
      "Training time:  0:01:40.306314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 66: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 101 (no improvement in 50 epochs)\n",
      "Training time:  0:02:05.748776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 67: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:01:56.286750\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.056135.\n",
      "\n",
      "Testing Configuration 68: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 106 (no improvement in 50 epochs)\n",
      "Training time:  0:02:11.329757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 69: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 115 (no improvement in 50 epochs)\n",
      "Training time:  0:02:22.399768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 70: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:02:31.832236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 71: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 112 (no improvement in 50 epochs)\n",
      "Training time:  0:02:19.689319\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024015.\n",
      "\n",
      "Testing Configuration 72: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 91 (no improvement in 50 epochs)\n",
      "Training time:  0:01:53.051353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 73: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 115 (no improvement in 50 epochs)\n",
      "Training time:  0:02:21.970606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 74: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:01:58.737898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 75: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 91 (no improvement in 50 epochs)\n",
      "Training time:  0:01:55.138173\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024017.\n",
      "\n",
      "Testing Configuration 76: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 74 (no improvement in 50 epochs)\n",
      "Training time:  0:01:32.917450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 77: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 67 (no improvement in 50 epochs)\n",
      "Training time:  0:01:24.044436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 78: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:02:33.750128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 79: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 68 (no improvement in 50 epochs)\n",
      "Training time:  0:01:25.918972\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024015.\n",
      "\n",
      "Testing Configuration 80: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 93 (no improvement in 50 epochs)\n",
      "Training time:  0:03:48.786491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 81: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 87 (no improvement in 50 epochs)\n",
      "Training time:  0:03:33.237214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 82: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:05:01.837607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 83: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:03:52.287455\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024515.\n",
      "\n",
      "Testing Configuration 84: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 78 (no improvement in 50 epochs)\n",
      "Training time:  0:03:11.623071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 85: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:04:13.767805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 86: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 60 (no improvement in 50 epochs)\n",
      "Training time:  0:02:26.901878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 87: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:05:10.113354\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.026511.\n",
      "\n",
      "Testing Configuration 88: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 104 (no improvement in 50 epochs)\n",
      "Training time:  0:04:14.672320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 89: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:03:50.554892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 90: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 107 (no improvement in 50 epochs)\n",
      "Training time:  0:04:22.595415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 91: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 69 (no improvement in 50 epochs)\n",
      "Training time:  0:02:48.910364\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024030.\n",
      "\n",
      "Testing Configuration 92: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:03:21.839401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 93: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:03:36.540256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 94: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 79 (no improvement in 50 epochs)\n",
      "Training time:  0:03:12.888836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 95: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:04:40.098176\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024519.\n",
      "\n",
      "Testing Configuration 96: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 71 (no improvement in 50 epochs)\n",
      "Training time:  0:04:11.986894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 97: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:06:05.579247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 98: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 160 (no improvement in 50 epochs)\n",
      "Training time:  0:09:27.776074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 99: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 124 (no improvement in 50 epochs)\n",
      "Training time:  0:07:22.662543\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.025030.\n",
      "\n",
      "Testing Configuration 100: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 59 (no improvement in 50 epochs)\n",
      "Training time:  0:03:30.451009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 101: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 78 (no improvement in 50 epochs)\n",
      "Training time:  0:04:39.532175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 102: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 72 (no improvement in 50 epochs)\n",
      "Training time:  0:04:20.210991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 103: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 194 (no improvement in 50 epochs)\n",
      "Training time:  0:11:30.524776\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.025015.\n",
      "\n",
      "Testing Configuration 104: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 64 (no improvement in 50 epochs)\n",
      "Training time:  0:03:47.734606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 105: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 106 (no improvement in 50 epochs)\n",
      "Training time:  0:06:17.988896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 106: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 161 (no improvement in 50 epochs)\n",
      "Training time:  0:09:35.928584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 107: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 142 (no improvement in 50 epochs)\n",
      "Training time:  0:08:20.575082\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.025029.\n",
      "\n",
      "Testing Configuration 108: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 69 (no improvement in 50 epochs)\n",
      "Training time:  0:04:06.616520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 109: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 61 (no improvement in 50 epochs)\n",
      "Training time:  0:03:37.358943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 110: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 153 (no improvement in 50 epochs)\n",
      "Training time:  0:09:02.779833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 111: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:08:57.331700\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024509.\n",
      "\n",
      "Testing Configuration 112: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 107 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.187019\n",
      "\n",
      "Testing Configuration 113: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 115 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.893643\n",
      "\n",
      "Testing Configuration 114: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 190 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.497657\n",
      "\n",
      "Testing Configuration 115: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.677959\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.026022.\n",
      "\n",
      "Testing Configuration 116: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.171942\n",
      "\n",
      "Testing Configuration 117: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 80 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.240250\n",
      "\n",
      "Testing Configuration 118: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 229 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.425049\n",
      "\n",
      "Testing Configuration 119: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 163 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.284715\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023010.\n",
      "\n",
      "Testing Configuration 120: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.130080\n",
      "\n",
      "Testing Configuration 121: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 112 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.431975\n",
      "\n",
      "Testing Configuration 122: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 283 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.246168\n",
      "\n",
      "Testing Configuration 123: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.518400\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024517.\n",
      "\n",
      "Testing Configuration 124: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 105 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.098556\n",
      "\n",
      "Testing Configuration 125: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 91 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.373963\n",
      "\n",
      "Testing Configuration 126: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 209 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.008500\n",
      "\n",
      "Testing Configuration 127: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 212 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.431015\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024526.\n",
      "\n",
      "Testing Configuration 128: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 75 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.052720\n",
      "\n",
      "Testing Configuration 129: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 99 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.414249\n",
      "\n",
      "Testing Configuration 130: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.909354\n",
      "\n",
      "Testing Configuration 131: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 177 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.541429\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024031.\n",
      "\n",
      "Testing Configuration 132: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 107 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.691132\n",
      "\n",
      "Testing Configuration 133: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 70 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.897629\n",
      "\n",
      "Testing Configuration 134: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 82 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.703729\n",
      "\n",
      "Testing Configuration 135: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 160 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.462885\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023519.\n",
      "\n",
      "Testing Configuration 136: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 78 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.800681\n",
      "\n",
      "Testing Configuration 137: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 72 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.725793\n",
      "\n",
      "Testing Configuration 138: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 121 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.174735\n",
      "\n",
      "Testing Configuration 139: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 214 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.715161\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024032.\n",
      "\n",
      "Testing Configuration 140: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 166 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.081678\n",
      "\n",
      "Testing Configuration 141: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 117 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.807832\n",
      "\n",
      "Testing Configuration 142: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 168 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.619922\n",
      "\n",
      "Testing Configuration 143: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 85 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.081530\n",
      "\n",
      "Processing Model: OptResNet1D_JARILWWF with params {'input_channels': 30, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024011.\n",
      "\n",
      "Testing Configuration 144: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 97 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.523981\n",
      "\n",
      "Testing Configuration 145: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 90 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.453261\n",
      "\n",
      "Testing Configuration 146: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 120 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.446567\n",
      "\n",
      "Testing Configuration 147: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 120 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.518826\n",
      "\n",
      "Processing Model: OptECAResNet1D_JARILWWF with params {'input_channels': 30, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024516.\n",
      "\n",
      "Testing Configuration 148: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 72 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.171040\n",
      "\n",
      "Testing Configuration 149: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 151 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.163415\n",
      "\n",
      "Testing Configuration 150: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 97 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.025476\n",
      "\n",
      "Testing Configuration 151: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 150 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.938797\n",
      "\n",
      "Processing Model: CustomResNet1D with params {'input_channels': 30}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023549.\n",
      "\n",
      "Testing Configuration 152: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.273501\n",
      "\n",
      "Testing Configuration 153: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 84 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.446197\n",
      "\n",
      "Testing Configuration 154: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 147 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.678061\n",
      "\n",
      "Testing Configuration 155: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 95 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.908094\n",
      "\n",
      "Processing Model: CustomECAResNet1D with params {'input_channels': 30}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024519.\n",
      "\n",
      "Testing Configuration 156: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.171883\n",
      "\n",
      "Testing Configuration 157: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 81 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.793655\n",
      "\n",
      "Testing Configuration 158: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.686180\n",
      "\n",
      "Testing Configuration 159: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 102 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.418060\n",
      "\n",
      "--- Running seed 420 ---\n",
      "Running 160 configurations...\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024018.\n",
      "\n",
      "Testing Configuration 0: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 138 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.431937\n",
      "\n",
      "Testing Configuration 1: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 172 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.360937\n",
      "\n",
      "Testing Configuration 2: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 227 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.351635\n",
      "\n",
      "Testing Configuration 3: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 265 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.315540\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024019.\n",
      "\n",
      "Testing Configuration 4: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 191 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.713967\n",
      "\n",
      "Testing Configuration 5: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.514388\n",
      "\n",
      "Testing Configuration 6: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 215 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.588916\n",
      "\n",
      "Testing Configuration 7: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 113 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.863374\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024017.\n",
      "\n",
      "Testing Configuration 8: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 84 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.377351\n",
      "\n",
      "Testing Configuration 9: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 132 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.447206\n",
      "\n",
      "Testing Configuration 10: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 104 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.713415\n",
      "\n",
      "Testing Configuration 11: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.926341\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024016.\n",
      "\n",
      "Testing Configuration 12: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.952297\n",
      "\n",
      "Testing Configuration 13: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.777633\n",
      "\n",
      "Testing Configuration 14: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 163 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.447653\n",
      "\n",
      "Testing Configuration 15: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 326 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.666988\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.025023.\n",
      "\n",
      "Testing Configuration 16: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 87 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.429645\n",
      "\n",
      "Testing Configuration 17: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 85 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.544770\n",
      "\n",
      "Testing Configuration 18: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 200 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.700414\n",
      "\n",
      "Testing Configuration 19: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 76 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.108217\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023511.\n",
      "\n",
      "Testing Configuration 20: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 131 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.966821\n",
      "\n",
      "Testing Configuration 21: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 139 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.482432\n",
      "\n",
      "Testing Configuration 22: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 250 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.933232\n",
      "\n",
      "Testing Configuration 23: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 306 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.830945\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024019.\n",
      "\n",
      "Testing Configuration 24: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 108 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.303336\n",
      "\n",
      "Testing Configuration 25: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 66 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.196140\n",
      "\n",
      "Testing Configuration 26: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 129 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.506749\n",
      "\n",
      "Testing Configuration 27: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 194 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.600452\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024015.\n",
      "\n",
      "Testing Configuration 28: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 74 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.862743\n",
      "\n",
      "Testing Configuration 29: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 96 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.217034\n",
      "\n",
      "Testing Configuration 30: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 194 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.309185\n",
      "\n",
      "Testing Configuration 31: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.710301\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024010.\n",
      "\n",
      "Testing Configuration 32: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 93 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.270753\n",
      "\n",
      "Testing Configuration 33: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 113 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.039798\n",
      "\n",
      "Testing Configuration 34: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 141 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.572133\n",
      "\n",
      "Testing Configuration 35: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 228 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.478243\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024518.\n",
      "\n",
      "Testing Configuration 36: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 177 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.311444\n",
      "\n",
      "Testing Configuration 37: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 127 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.815317\n",
      "\n",
      "Testing Configuration 38: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 127 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.074622\n",
      "\n",
      "Testing Configuration 39: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 147 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.178289\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024123.\n",
      "\n",
      "Testing Configuration 40: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 67 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.041589\n",
      "\n",
      "Testing Configuration 41: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 76 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.545501\n",
      "\n",
      "Testing Configuration 42: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 182 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.032632\n",
      "\n",
      "Testing Configuration 43: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 223 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.448115\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023526.\n",
      "\n",
      "Testing Configuration 44: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.529902\n",
      "\n",
      "Testing Configuration 45: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 87 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.039277\n",
      "\n",
      "Testing Configuration 46: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 185 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.129744\n",
      "\n",
      "Testing Configuration 47: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 156 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.069207\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.028514.\n",
      "\n",
      "Testing Configuration 48: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 74 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.368352\n",
      "\n",
      "Testing Configuration 49: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.634448\n",
      "\n",
      "Testing Configuration 50: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 293 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.847874\n",
      "\n",
      "Testing Configuration 51: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 89 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.603389\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023513.\n",
      "\n",
      "Testing Configuration 52: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 107 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.895904\n",
      "\n",
      "Testing Configuration 53: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 138 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.590986\n",
      "\n",
      "Testing Configuration 54: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 95 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.851383\n",
      "\n",
      "Testing Configuration 55: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 99 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.211458\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024508.\n",
      "\n",
      "Testing Configuration 56: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 58 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.969098\n",
      "\n",
      "Testing Configuration 57: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 70 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.055056\n",
      "\n",
      "Testing Configuration 58: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 102 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.430917\n",
      "\n",
      "Testing Configuration 59: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.840236\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.048123.\n",
      "\n",
      "Testing Configuration 60: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 65 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.403543\n",
      "\n",
      "Testing Configuration 61: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 74 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.148108\n",
      "\n",
      "Testing Configuration 62: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.927370\n",
      "\n",
      "Testing Configuration 63: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 124 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.213414\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.026530.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 64: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:01:46.951790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 65: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 66 (no improvement in 50 epochs)\n",
      "Training time:  0:01:22.463226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 66: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 138 (no improvement in 50 epochs)\n",
      "Training time:  0:02:51.513277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 67: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:02:50.493133\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.027033.\n",
      "\n",
      "Testing Configuration 68: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 65 (no improvement in 50 epochs)\n",
      "Training time:  0:01:21.415453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 69: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 83 (no improvement in 50 epochs)\n",
      "Training time:  0:01:43.868920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 70: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 95 (no improvement in 50 epochs)\n",
      "Training time:  0:01:59.028682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 71: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 71 (no improvement in 50 epochs)\n",
      "Training time:  0:01:29.603418\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.028018.\n",
      "\n",
      "Testing Configuration 72: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:01:36.904356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 73: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 55 (no improvement in 50 epochs)\n",
      "Training time:  0:01:08.783818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 74: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:02:22.994665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 75: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 124 (no improvement in 50 epochs)\n",
      "Training time:  0:02:36.560321\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.026037.\n",
      "\n",
      "Testing Configuration 76: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 58 (no improvement in 50 epochs)\n",
      "Training time:  0:01:12.049793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 77: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 102 (no improvement in 50 epochs)\n",
      "Training time:  0:02:07.959616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 78: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 253 (no improvement in 50 epochs)\n",
      "Training time:  0:05:02.020003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 79: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 104 (no improvement in 50 epochs)\n",
      "Training time:  0:01:56.509654\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024500.\n",
      "\n",
      "Testing Configuration 80: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 59 (no improvement in 50 epochs)\n",
      "Training time:  0:02:08.485059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 81: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 104 (no improvement in 50 epochs)\n",
      "Training time:  0:03:55.490683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 82: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 147 (no improvement in 50 epochs)\n",
      "Training time:  0:05:35.111049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 83: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 124 (no improvement in 50 epochs)\n",
      "Training time:  0:04:41.619227\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.027572.\n",
      "\n",
      "Testing Configuration 84: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 100 (no improvement in 50 epochs)\n",
      "Training time:  0:03:47.150593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 85: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 64 (no improvement in 50 epochs)\n",
      "Training time:  0:02:25.512080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 86: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 73 (no improvement in 50 epochs)\n",
      "Training time:  0:02:39.570486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 87: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 84 (no improvement in 50 epochs)\n",
      "Training time:  0:03:04.605922\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024000.\n",
      "\n",
      "Testing Configuration 88: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 56 (no improvement in 50 epochs)\n",
      "Training time:  0:02:01.766076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 89: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 72 (no improvement in 50 epochs)\n",
      "Training time:  0:02:36.396387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 90: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:03:21.846538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 91: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:03:25.249492\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023000.\n",
      "\n",
      "Testing Configuration 92: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 69 (no improvement in 50 epochs)\n",
      "Training time:  0:02:31.010720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 93: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 59 (no improvement in 50 epochs)\n",
      "Training time:  0:02:08.360585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 94: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 164 (no improvement in 50 epochs)\n",
      "Training time:  0:05:58.488510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 95: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:02:51.844322\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.027500.\n",
      "\n",
      "Testing Configuration 96: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 60 (no improvement in 50 epochs)\n",
      "Training time:  0:03:09.807017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 97: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 79 (no improvement in 50 epochs)\n",
      "Training time:  0:04:13.545531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 98: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 194 (no improvement in 50 epochs)\n",
      "Training time:  0:10:14.336174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 99: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 296 (no improvement in 50 epochs)\n",
      "Training time:  0:15:39.166200\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.029996.\n",
      "\n",
      "Testing Configuration 100: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 65 (no improvement in 50 epochs)\n",
      "Training time:  0:03:26.335929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 101: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 66 (no improvement in 50 epochs)\n",
      "Training time:  0:03:31.920125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 102: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 172 (no improvement in 50 epochs)\n",
      "Training time:  0:09:06.131592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 103: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 101 (no improvement in 50 epochs)\n",
      "Training time:  0:05:17.734356\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.028500.\n",
      "\n",
      "Testing Configuration 104: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 60 (no improvement in 50 epochs)\n",
      "Training time:  0:03:10.878333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 105: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:04:31.882286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 106: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 124 (no improvement in 50 epochs)\n",
      "Training time:  0:06:31.680772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 107: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 76 (no improvement in 50 epochs)\n",
      "Training time:  0:03:59.766427\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.026501.\n",
      "\n",
      "Testing Configuration 108: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 74 (no improvement in 50 epochs)\n",
      "Training time:  0:03:57.855314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 109: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 90 (no improvement in 50 epochs)\n",
      "Training time:  0:04:45.677175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 110: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 130 (no improvement in 50 epochs)\n",
      "Training time:  0:06:54.687266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 111: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 173 (no improvement in 50 epochs)\n",
      "Training time:  0:09:10.501302\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.026500.\n",
      "\n",
      "Testing Configuration 112: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 101 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.533863\n",
      "\n",
      "Testing Configuration 113: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 121 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.793427\n",
      "\n",
      "Testing Configuration 114: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 269 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.536610\n",
      "\n",
      "Testing Configuration 115: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 185 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.498684\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024424.\n",
      "\n",
      "Testing Configuration 116: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.037651\n",
      "\n",
      "Testing Configuration 117: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 167 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.007452\n",
      "\n",
      "Testing Configuration 118: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.909065\n",
      "\n",
      "Testing Configuration 119: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 101 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.302460\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.025498.\n",
      "\n",
      "Testing Configuration 120: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 131 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.158273\n",
      "\n",
      "Testing Configuration 121: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.801505\n",
      "\n",
      "Testing Configuration 122: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 325 (no improvement in 50 epochs)\n",
      "Training time:  0:00:19.957778\n",
      "\n",
      "Testing Configuration 123: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 154 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.672731\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.025500.\n",
      "\n",
      "Testing Configuration 124: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 69 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.667907\n",
      "\n",
      "Testing Configuration 125: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.262603\n",
      "\n",
      "Testing Configuration 126: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 230 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.359583\n",
      "\n",
      "Testing Configuration 127: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.539569\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024000.\n",
      "\n",
      "Testing Configuration 128: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 69 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.976195\n",
      "\n",
      "Testing Configuration 129: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.377606\n",
      "\n",
      "Testing Configuration 130: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 156 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.378967\n",
      "\n",
      "Testing Configuration 131: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 124 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.798192\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.025000.\n",
      "\n",
      "Testing Configuration 132: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 75 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.794341\n",
      "\n",
      "Testing Configuration 133: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 93 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.722675\n",
      "\n",
      "Testing Configuration 134: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 93 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.968582\n",
      "\n",
      "Testing Configuration 135: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.862534\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023500.\n",
      "\n",
      "Testing Configuration 136: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 109 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.955135\n",
      "\n",
      "Testing Configuration 137: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.962170\n",
      "\n",
      "Testing Configuration 138: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 306 (no improvement in 50 epochs)\n",
      "Training time:  0:00:24.431298\n",
      "\n",
      "Testing Configuration 139: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 128 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.325035\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.025002.\n",
      "\n",
      "Testing Configuration 140: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 80 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.933611\n",
      "\n",
      "Testing Configuration 141: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.181261\n",
      "\n",
      "Testing Configuration 142: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 154 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.123631\n",
      "\n",
      "Testing Configuration 143: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 123 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.917258\n",
      "\n",
      "Processing Model: OptResNet1D_JARILWWF with params {'input_channels': 30, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023500.\n",
      "\n",
      "Testing Configuration 144: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 68 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.869124\n",
      "\n",
      "Testing Configuration 145: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 106 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.138490\n",
      "\n",
      "Testing Configuration 146: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 112 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.228054\n",
      "\n",
      "Testing Configuration 147: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 240 (no improvement in 50 epochs)\n",
      "Training time:  0:00:21.107447\n",
      "\n",
      "Processing Model: OptECAResNet1D_JARILWWF with params {'input_channels': 30, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024501.\n",
      "\n",
      "Testing Configuration 148: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 119 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.503944\n",
      "\n",
      "Testing Configuration 149: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 166 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.192989\n",
      "\n",
      "Testing Configuration 150: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.448049\n",
      "\n",
      "Testing Configuration 151: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 119 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.358954\n",
      "\n",
      "Processing Model: CustomResNet1D with params {'input_channels': 30}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024502.\n",
      "\n",
      "Testing Configuration 152: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 61 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.632723\n",
      "\n",
      "Testing Configuration 153: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 80 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.113843\n",
      "\n",
      "Testing Configuration 154: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 204 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.679452\n",
      "\n",
      "Testing Configuration 155: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 106 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.766389\n",
      "\n",
      "Processing Model: CustomECAResNet1D with params {'input_channels': 30}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024500.\n",
      "\n",
      "Testing Configuration 156: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 62 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.804616\n",
      "\n",
      "Testing Configuration 157: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.205294\n",
      "\n",
      "Testing Configuration 158: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 156 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.678210\n",
      "\n",
      "Testing Configuration 159: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 91 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.549052\n",
      "\n",
      "--- Running seed 101010 ---\n",
      "Running 160 configurations...\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024999.\n",
      "\n",
      "Testing Configuration 0: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 138 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.251276\n",
      "\n",
      "Testing Configuration 1: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.097764\n",
      "\n",
      "Testing Configuration 2: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 223 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.427155\n",
      "\n",
      "Testing Configuration 3: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 276 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.959102\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.025000.\n",
      "\n",
      "Testing Configuration 4: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 119 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.915572\n",
      "\n",
      "Testing Configuration 5: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 244 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.543395\n",
      "\n",
      "Testing Configuration 6: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 202 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.226506\n",
      "\n",
      "Testing Configuration 7: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 199 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.439257\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023000.\n",
      "\n",
      "Testing Configuration 8: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 139 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.220482\n",
      "\n",
      "Testing Configuration 9: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 147 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.328425\n",
      "\n",
      "Testing Configuration 10: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 153 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.550160\n",
      "\n",
      "Testing Configuration 11: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 118 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.317233\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024000.\n",
      "\n",
      "Testing Configuration 12: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 111 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.232439\n",
      "\n",
      "Testing Configuration 13: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 186 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.418019\n",
      "\n",
      "Testing Configuration 14: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 113 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.174353\n",
      "\n",
      "Testing Configuration 15: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 130 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.743752\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024499.\n",
      "\n",
      "Testing Configuration 16: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 121 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.152749\n",
      "\n",
      "Testing Configuration 17: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 121 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.404818\n",
      "\n",
      "Testing Configuration 18: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 131 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.314000\n",
      "\n",
      "Testing Configuration 19: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 177 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.079973\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024000.\n",
      "\n",
      "Testing Configuration 20: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 82 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.965923\n",
      "\n",
      "Testing Configuration 21: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 86 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.401690\n",
      "\n",
      "Testing Configuration 22: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 199 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.102482\n",
      "\n",
      "Testing Configuration 23: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 148 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.381473\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024500.\n",
      "\n",
      "Testing Configuration 24: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 64 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.918959\n",
      "\n",
      "Testing Configuration 25: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.682000\n",
      "\n",
      "Testing Configuration 26: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 137 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.990597\n",
      "\n",
      "Testing Configuration 27: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 90 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.594990\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.025486.\n",
      "\n",
      "Testing Configuration 28: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.315046\n",
      "\n",
      "Testing Configuration 29: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.335552\n",
      "\n",
      "Testing Configuration 30: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 73 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.380970\n",
      "\n",
      "Testing Configuration 31: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 136 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.219713\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023999.\n",
      "\n",
      "Testing Configuration 32: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 81 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.136894\n",
      "\n",
      "Testing Configuration 33: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 96 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.897722\n",
      "\n",
      "Testing Configuration 34: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 98 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.406796\n",
      "\n",
      "Testing Configuration 35: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 222 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.689715\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.025500.\n",
      "\n",
      "Testing Configuration 36: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.355181\n",
      "\n",
      "Testing Configuration 37: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 148 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.192259\n",
      "\n",
      "Testing Configuration 38: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 176 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.626559\n",
      "\n",
      "Testing Configuration 39: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 139 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.522970\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024500.\n",
      "\n",
      "Testing Configuration 40: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 79 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.371937\n",
      "\n",
      "Testing Configuration 41: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 91 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.960383\n",
      "\n",
      "Testing Configuration 42: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 163 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.565627\n",
      "\n",
      "Testing Configuration 43: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 133 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.292778\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024001.\n",
      "\n",
      "Testing Configuration 44: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 68 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.847760\n",
      "\n",
      "Testing Configuration 45: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 97 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.331117\n",
      "\n",
      "Testing Configuration 46: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 457 (no improvement in 50 epochs)\n",
      "Training time:  0:00:22.866967\n",
      "\n",
      "Testing Configuration 47: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 157 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.450581\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024500.\n",
      "\n",
      "Testing Configuration 48: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 79 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.661062\n",
      "\n",
      "Testing Configuration 49: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 113 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.038101\n",
      "\n",
      "Testing Configuration 50: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.546486\n",
      "\n",
      "Testing Configuration 51: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 116 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.661958\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.025000.\n",
      "\n",
      "Testing Configuration 52: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 108 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.661397\n",
      "\n",
      "Testing Configuration 53: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 101 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.616991\n",
      "\n",
      "Testing Configuration 54: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 209 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.874888\n",
      "\n",
      "Testing Configuration 55: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 142 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.103750\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.025501.\n",
      "\n",
      "Testing Configuration 56: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 56 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.615132\n",
      "\n",
      "Testing Configuration 57: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 72 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.765846\n",
      "\n",
      "Testing Configuration 58: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 117 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.091954\n",
      "\n",
      "Testing Configuration 59: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 182 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.703691\n",
      "\n",
      "Processing Model: LSTM_HumanFi with params {'input_dim': 30, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024002.\n",
      "\n",
      "Testing Configuration 60: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 68 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.322602\n",
      "\n",
      "Testing Configuration 61: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 80 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.227190\n",
      "\n",
      "Testing Configuration 62: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 76 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.257141\n",
      "\n",
      "Testing Configuration 63: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: LSTM_HumanFi\n",
      "Early stopping at epoch 81 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.919394\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024500.\n",
      "\n",
      "Testing Configuration 64: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 60 (no improvement in 50 epochs)\n",
      "Training time:  0:01:06.761936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 65: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 73 (no improvement in 50 epochs)\n",
      "Training time:  0:01:21.409854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 66: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 99 (no improvement in 50 epochs)\n",
      "Training time:  0:01:49.929811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 67: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:02:09.531990\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.025498.\n",
      "\n",
      "Testing Configuration 68: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 113 (no improvement in 50 epochs)\n",
      "Training time:  0:02:04.434904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 69: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 71 (no improvement in 50 epochs)\n",
      "Training time:  0:01:19.963211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 70: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:01:36.937921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 71: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 84 (no improvement in 50 epochs)\n",
      "Training time:  0:01:37.064798\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.026500.\n",
      "\n",
      "Testing Configuration 72: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 63 (no improvement in 50 epochs)\n",
      "Training time:  0:01:09.814879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 73: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 61 (no improvement in 50 epochs)\n",
      "Training time:  0:01:07.607225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 74: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 130 (no improvement in 50 epochs)\n",
      "Training time:  0:02:25.205105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 75: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 199 (no improvement in 50 epochs)\n",
      "Training time:  0:03:39.629632\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.026000.\n",
      "\n",
      "Testing Configuration 76: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:01:25.613265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 77: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 75 (no improvement in 50 epochs)\n",
      "Training time:  0:01:26.758697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 78: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 168 (no improvement in 50 epochs)\n",
      "Training time:  0:03:05.950635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 79: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 238 (no improvement in 50 epochs)\n",
      "Training time:  0:04:26.451757\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024000.\n",
      "\n",
      "Testing Configuration 80: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 79 (no improvement in 50 epochs)\n",
      "Training time:  0:02:51.379330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 81: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 56 (no improvement in 50 epochs)\n",
      "Training time:  0:02:01.688406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 82: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 230 (no improvement in 50 epochs)\n",
      "Training time:  0:08:21.077605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 83: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 183 (no improvement in 50 epochs)\n",
      "Training time:  0:06:39.072617\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.028000.\n",
      "\n",
      "Testing Configuration 84: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 57 (no improvement in 50 epochs)\n",
      "Training time:  0:02:04.941491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 85: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 76 (no improvement in 50 epochs)\n",
      "Training time:  0:02:45.256141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 86: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 61 (no improvement in 50 epochs)\n",
      "Training time:  0:02:12.491742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 87: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 106 (no improvement in 50 epochs)\n",
      "Training time:  0:03:50.126189\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023500.\n",
      "\n",
      "Testing Configuration 88: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 66 (no improvement in 50 epochs)\n",
      "Training time:  0:02:23.294439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 89: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:03:25.599025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 90: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 197 (no improvement in 50 epochs)\n",
      "Training time:  0:07:12.391665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 91: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 72 (no improvement in 50 epochs)\n",
      "Training time:  0:02:48.796604\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 128], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023002.\n",
      "\n",
      "Testing Configuration 92: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 81 (no improvement in 50 epochs)\n",
      "Training time:  0:02:55.788464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 93: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 89 (no improvement in 50 epochs)\n",
      "Training time:  0:03:15.334183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 94: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 114 (no improvement in 50 epochs)\n",
      "Training time:  0:04:08.931404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 95: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 69 (no improvement in 50 epochs)\n",
      "Training time:  0:02:33.704278\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024000.\n",
      "\n",
      "Testing Configuration 96: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 84 (no improvement in 50 epochs)\n",
      "Training time:  0:04:27.617499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 97: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 93 (no improvement in 50 epochs)\n",
      "Training time:  0:04:55.911134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 98: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 89 (no improvement in 50 epochs)\n",
      "Training time:  0:04:46.937764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 99: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 243 (no improvement in 50 epochs)\n",
      "Training time:  0:12:51.729633\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 2, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023500.\n",
      "\n",
      "Testing Configuration 100: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 73 (no improvement in 50 epochs)\n",
      "Training time:  0:03:55.319111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 101: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 89 (no improvement in 50 epochs)\n",
      "Training time:  0:04:42.366611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 102: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 184 (no improvement in 50 epochs)\n",
      "Training time:  0:09:47.601712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 103: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 63 (no improvement in 50 epochs)\n",
      "Training time:  0:03:22.215605\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023501.\n",
      "\n",
      "Testing Configuration 104: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 72 (no improvement in 50 epochs)\n",
      "Training time:  0:03:50.355178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 105: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 73 (no improvement in 50 epochs)\n",
      "Training time:  0:03:51.457566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 106: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 69 (no improvement in 50 epochs)\n",
      "Training time:  0:03:40.144426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 107: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:05:28.931045\n",
      "\n",
      "Processing Model: TemporalConvNet with params {'num_inputs': 30, 'num_channels': [64, 128, 256], 'kernel_size': 3, 'dropout': 0.5}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023000.\n",
      "\n",
      "Testing Configuration 108: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 70 (no improvement in 50 epochs)\n",
      "Training time:  0:03:41.928581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 109: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 70 (no improvement in 50 epochs)\n",
      "Training time:  0:03:41.098852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 110: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 147 (no improvement in 50 epochs)\n",
      "Training time:  0:07:46.265589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\centu\\PycharmProjects\\MasterThesis_mmWavePI\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 111: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: TemporalConvNet\n",
      "Early stopping at epoch 149 (no improvement in 50 epochs)\n",
      "Training time:  0:07:58.447423\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023500.\n",
      "\n",
      "Testing Configuration 112: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 126 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.700654\n",
      "\n",
      "Testing Configuration 113: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 142 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.684932\n",
      "\n",
      "Testing Configuration 114: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.152514\n",
      "\n",
      "Testing Configuration 115: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 189 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.090365\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023999.\n",
      "\n",
      "Testing Configuration 116: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 103 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.524050\n",
      "\n",
      "Testing Configuration 117: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 78 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.330430\n",
      "\n",
      "Testing Configuration 118: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 184 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.279531\n",
      "\n",
      "Testing Configuration 119: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 119 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.494999\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 64, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023000.\n",
      "\n",
      "Testing Configuration 120: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 130 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.991740\n",
      "\n",
      "Testing Configuration 121: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 221 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.681017\n",
      "\n",
      "Testing Configuration 122: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 372 (no improvement in 50 epochs)\n",
      "Training time:  0:00:25.017643\n",
      "\n",
      "Testing Configuration 123: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 160 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.807914\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 64, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023500.\n",
      "\n",
      "Testing Configuration 124: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 95 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.049454\n",
      "\n",
      "Testing Configuration 125: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 77 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.055939\n",
      "\n",
      "Testing Configuration 126: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 224 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.813223\n",
      "\n",
      "Testing Configuration 127: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 216 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.289990\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023356.\n",
      "\n",
      "Testing Configuration 128: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 82 (no improvement in 50 epochs)\n",
      "Training time:  0:00:04.102702\n",
      "\n",
      "Testing Configuration 129: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 107 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.512503\n",
      "\n",
      "Testing Configuration 130: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 188 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.057252\n",
      "\n",
      "Testing Configuration 131: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 192 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.536731\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 1}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023000.\n",
      "\n",
      "Testing Configuration 132: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 59 (no improvement in 50 epochs)\n",
      "Training time:  0:00:02.910267\n",
      "\n",
      "Testing Configuration 133: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 70 (no improvement in 50 epochs)\n",
      "Training time:  0:00:03.537103\n",
      "\n",
      "Testing Configuration 134: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 157 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.939371\n",
      "\n",
      "Testing Configuration 135: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 122 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.604262\n",
      "\n",
      "Processing Model: CNN_BiLSTM_Attention with params {'input_dim': 30, 'cnn_filters': 64, 'lstm_units': 128, 'num_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023000.\n",
      "\n",
      "Testing Configuration 136: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 99 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.306770\n",
      "\n",
      "Testing Configuration 137: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 137 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.300385\n",
      "\n",
      "Testing Configuration 138: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 166 (no improvement in 50 epochs)\n",
      "Training time:  0:00:12.292762\n",
      "\n",
      "Testing Configuration 139: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_Attention\n",
      "Early stopping at epoch 174 (no improvement in 50 epochs)\n",
      "Training time:  0:00:13.044456\n",
      "\n",
      "Processing Model: CNN_BiLSTM_TemporalAttention with params {'input_dim': 30, 'cnn_channels': 64, 'lstm_hidden_dim': 128, 'lstm_layers': 2}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023500.\n",
      "\n",
      "Testing Configuration 140: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 95 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.575660\n",
      "\n",
      "Testing Configuration 141: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 93 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.569691\n",
      "\n",
      "Testing Configuration 142: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 196 (no improvement in 50 epochs)\n",
      "Training time:  0:00:14.344875\n",
      "\n",
      "Testing Configuration 143: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CNN_BiLSTM_TemporalAttention\n",
      "Early stopping at epoch 107 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.008426\n",
      "\n",
      "Processing Model: OptResNet1D_JARILWWF with params {'input_channels': 30, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023000.\n",
      "\n",
      "Testing Configuration 144: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 80 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.605890\n",
      "\n",
      "Testing Configuration 145: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.368788\n",
      "\n",
      "Testing Configuration 146: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 128 (no improvement in 50 epochs)\n",
      "Training time:  0:00:10.731540\n",
      "\n",
      "Testing Configuration 147: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptResNet1D_JARILWWF\n",
      "Early stopping at epoch 175 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.568394\n",
      "\n",
      "Processing Model: OptECAResNet1D_JARILWWF with params {'input_channels': 30, 'layers': [1, 1, 1, 1]}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.024458.\n",
      "\n",
      "Testing Configuration 148: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 156 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.631056\n",
      "\n",
      "Testing Configuration 149: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 88 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.305599\n",
      "\n",
      "Testing Configuration 150: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 159 (no improvement in 50 epochs)\n",
      "Training time:  0:00:15.232427\n",
      "\n",
      "Testing Configuration 151: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: OptECAResNet1D_JARILWWF\n",
      "Early stopping at epoch 159 (no improvement in 50 epochs)\n",
      "Training time:  0:00:16.043985\n",
      "\n",
      "Processing Model: CustomResNet1D with params {'input_channels': 30}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023000.\n",
      "\n",
      "Testing Configuration 152: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 92 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.108680\n",
      "\n",
      "Testing Configuration 153: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 97 (no improvement in 50 epochs)\n",
      "Training time:  0:00:08.060642\n",
      "\n",
      "Testing Configuration 154: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 146 (no improvement in 50 epochs)\n",
      "Training time:  0:00:11.545529\n",
      "\n",
      "Testing Configuration 155: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomResNet1D\n",
      "Early stopping at epoch 97 (no improvement in 50 epochs)\n",
      "Training time:  0:00:07.598005\n",
      "\n",
      "Processing Model: CustomECAResNet1D with params {'input_channels': 30}\n",
      "Started loading data...\n",
      "Train size: 509, Validation size: 105, Test size: 105\n",
      "Train labels distribution: Counter({4: 123, 6: 91, 0: 75, 5: 67, 1: 51, 2: 51, 3: 51})\n",
      "Validation labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Test labels distribution: Counter({4: 26, 6: 19, 0: 16, 5: 14, 1: 10, 2: 10, 3: 10})\n",
      "Done loading data (batch size: 32) in 0:00:00.023000.\n",
      "\n",
      "Testing Configuration 156: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 71 (no improvement in 50 epochs)\n",
      "Training time:  0:00:06.385042\n",
      "\n",
      "Testing Configuration 157: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.0, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 61 (no improvement in 50 epochs)\n",
      "Training time:  0:00:05.556113\n",
      "\n",
      "Testing Configuration 158: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.0\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 180 (no improvement in 50 epochs)\n",
      "Training time:  0:00:17.905277\n",
      "\n",
      "Testing Configuration 159: batch_size=32, lr=0.0007, optimizer=adam, mixup_alpha=0.4, smoothing_prob=0.5\n",
      "\n",
      "(Device: cuda) Started training model: CustomECAResNet1D\n",
      "Early stopping at epoch 94 (no improvement in 50 epochs)\n",
      "Training time:  0:00:09.168525\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
